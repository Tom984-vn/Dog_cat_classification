{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "5_0rRqF8Py98"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as func\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLdDtjBAPy9-",
        "outputId": "74192bb3-8c2d-4481-806c-c233c08f2529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/tongpython/cat-and-dog/versions/1\n"
          ]
        }
      ],
      "source": [
        "path = kagglehub.dataset_download(\"tongpython/cat-and-dog\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtdhMwbnPy9_"
      },
      "source": [
        "## Import library\n",
        "    - Pytorch\n",
        "    - Pytorch model (pytorch.nn)\n",
        "        + Import function (ReLU, tanh, ...)\n",
        "    - Optimization\n",
        "    - DataLoader\n",
        "## Create the model\n",
        "    - Init\n",
        "        ! Super(NN) to properly initialize\n",
        "        + Create conv layer (3 channels with 3x3 kernels)\n",
        "        + Max pooling layer inbetween\n",
        "        + create hidden layer (linear)\n",
        "    - Forward\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "OdlJ0vK5Py-A"
      },
      "outputs": [],
      "source": [
        "num_classes = 2\n",
        "batch_size = 32\n",
        "learning_rate = 0.00012\n",
        "num_epochs = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "HCRZjWXoPy-B"
      },
      "outputs": [],
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(NN, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "        self.hlayer1 = nn.Linear(input_size, 128)\n",
        "        self.hlayer2 = nn.Linear(128, 128)\n",
        "        self.hlayer3 = nn.Linear(128, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = func.relu(self.hlayer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = func.relu(self.hlayer2(x))\n",
        "        return x\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, in_channel = 3, num_classes = 2):\n",
        "      super(CNN, self).__init__()\n",
        "      self.relu = nn.ReLU()\n",
        "      self.linear_dropout = nn.Dropout(p=0.25)\n",
        "      self.convd_dropout = nn.Dropout2d(p=0.2)\n",
        "      self.conv = nn.Conv2d(in_channels = in_channel, out_channels = 64, kernel_size=3,stride=1,padding=1)  #convolutional layer 64 layers with kernel 3x3\n",
        "      self.bn1 = nn.BatchNorm2d(64)\n",
        "      self.bn2 = nn.BatchNorm2d(128)\n",
        "      self.bn3 = nn.BatchNorm2d(256)\n",
        "      self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  #pooling layer help reduce size from 128x128 -> 64x64\n",
        "      self.conv2 = nn.Conv2d(in_channels= 64, out_channels= 128, kernel_size=3, stride=1,padding=1)\n",
        "      self.conv3 = nn.Conv2d(in_channels= 128, out_channels= 256, kernel_size=3, stride=1,padding=1)\n",
        "      self.fc1 = nn.Linear(256 * 16 * 16 , 128)  # Adjusted input size for Linear layer\n",
        "      self.fc2 = nn.Linear(128, num_classes)  # Added another fully connected layer\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(self.relu(self.bn1(self.conv(x))))\n",
        "    x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
        "    x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
        "    x = self.convd_dropout(x)\n",
        "    x = x.reshape(x.shape[0],-1)\n",
        "    x = func.relu(self.fc1(x))\n",
        "    x = self.linear_dropout(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "y3HaFoVd6cnc"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "aYEdbv_sPy-B"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),  # Augment data\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "# Load dataset\n",
        "train_dataset = datasets.ImageFolder(root=os.path.join(path,\"training_set\",\"training_set\"), transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(path,\"test_set\",\"test_set\"), transform=transform)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "RGWEpcZ8Py-C"
      },
      "outputs": [],
      "source": [
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr= learning_rate,betas=(0.9,0.999))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQtH3nBVPy-C",
        "outputId": "9b5d9f26-7145-4fd5-8470-274715509636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, batch: 0 with loss: 0.7017897963523865 and average loss:0.7017897963523865\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 5 with loss: 1.1800613403320312 and average loss:1.5168883800506592\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 10 with loss: 0.8124530911445618 and average loss:1.210898518562317\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 15 with loss: 0.7448480725288391 and average loss:1.0582395792007446\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 20 with loss: 0.5829296708106995 and average loss:0.9576705694198608\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 25 with loss: 0.5910103917121887 and average loss:0.9085599184036255\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 30 with loss: 0.6702612638473511 and average loss:0.8712359666824341\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 35 with loss: 0.7536793947219849 and average loss:0.8422365188598633\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 40 with loss: 0.5886200666427612 and average loss:0.8188891410827637\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 45 with loss: 0.6628890037536621 and average loss:0.7988429665565491\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 50 with loss: 0.6279915571212769 and average loss:0.78408282995224\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 55 with loss: 0.6810571551322937 and average loss:0.7733712792396545\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 60 with loss: 0.5944749712944031 and average loss:0.7651746273040771\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 65 with loss: 0.6708171367645264 and average loss:0.7576772570610046\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 70 with loss: 0.7019352316856384 and average loss:0.7505416870117188\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 75 with loss: 0.6218015551567078 and average loss:0.7420796751976013\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 80 with loss: 0.6326708793640137 and average loss:0.7378205060958862\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 85 with loss: 0.6405363082885742 and average loss:0.730453372001648\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 90 with loss: 0.6845052242279053 and average loss:0.7254011034965515\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 95 with loss: 0.6798986792564392 and average loss:0.7215100526809692\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 100 with loss: 0.5502709150314331 and average loss:0.7156685590744019\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 105 with loss: 0.7255693078041077 and average loss:0.7138479351997375\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 110 with loss: 0.6606628894805908 and average loss:0.7111653089523315\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 115 with loss: 0.6959033608436584 and average loss:0.707495927810669\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 120 with loss: 0.6226385235786438 and average loss:0.7046613693237305\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 125 with loss: 0.6661568284034729 and average loss:0.7027517557144165\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 130 with loss: 0.7428504824638367 and average loss:0.7001548409461975\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 135 with loss: 0.4935326874256134 and average loss:0.6959372758865356\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 140 with loss: 0.5615496635437012 and average loss:0.6909663081169128\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 145 with loss: 0.5419898629188538 and average loss:0.6874867081642151\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 150 with loss: 0.859634280204773 and average loss:0.686953067779541\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 155 with loss: 0.5363725423812866 and average loss:0.6845176815986633\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 160 with loss: 0.6528643369674683 and average loss:0.6828599572181702\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 165 with loss: 0.5657878518104553 and average loss:0.678314745426178\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 170 with loss: 0.5319584012031555 and average loss:0.6764522194862366\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 175 with loss: 0.7175241708755493 and average loss:0.6745039820671082\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 180 with loss: 0.5149924755096436 and average loss:0.6725916862487793\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 185 with loss: 0.6215230822563171 and average loss:0.6703949570655823\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 190 with loss: 0.5029029846191406 and average loss:0.6687847375869751\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 195 with loss: 0.6270381212234497 and average loss:0.6664502024650574\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 200 with loss: 0.6463922262191772 and average loss:0.6648736000061035\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 205 with loss: 0.47609272599220276 and average loss:0.6621557474136353\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 210 with loss: 0.5935053825378418 and average loss:0.6615381240844727\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 215 with loss: 0.6240519285202026 and average loss:0.6612233519554138\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 220 with loss: 0.718208909034729 and average loss:0.6607456207275391\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 225 with loss: 0.6060727834701538 and average loss:0.658042311668396\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 230 with loss: 0.626349151134491 and average loss:0.6572474837303162\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 235 with loss: 0.5849468111991882 and average loss:0.6562591791152954\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 240 with loss: 0.5827933549880981 and average loss:0.6562333703041077\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 245 with loss: 0.5794686079025269 and average loss:0.6546586155891418\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 0, batch: 250 with loss: 0.46772879362106323 and average loss:0.6535647511482239\n",
            "Epoch 1: LR=0.000120\n",
            "epoch: 1, batch: 0 with loss: 0.5301882028579712 and average loss:0.5301882028579712\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 5 with loss: 0.6084040999412537 and average loss:0.5832061171531677\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 10 with loss: 0.5305593609809875 and average loss:0.5921779870986938\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 15 with loss: 0.51661616563797 and average loss:0.5848943591117859\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 20 with loss: 0.5688626170158386 and average loss:0.5906598567962646\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 25 with loss: 0.6213937401771545 and average loss:0.5831910967826843\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 30 with loss: 0.7179667949676514 and average loss:0.5836822390556335\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 35 with loss: 0.7159559726715088 and average loss:0.5775465965270996\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 40 with loss: 0.5698933005332947 and average loss:0.5732024312019348\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 45 with loss: 0.7273616194725037 and average loss:0.5752648711204529\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 50 with loss: 0.5234019160270691 and average loss:0.5755447745323181\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 55 with loss: 0.5124927759170532 and average loss:0.5719079375267029\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 60 with loss: 0.6643021106719971 and average loss:0.5721600651741028\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 65 with loss: 0.6819456815719604 and average loss:0.5748077034950256\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 70 with loss: 0.6121835708618164 and average loss:0.5743408799171448\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 75 with loss: 0.5199921131134033 and average loss:0.5719603896141052\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 80 with loss: 0.5148307681083679 and average loss:0.5720654129981995\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 85 with loss: 0.5177591443061829 and average loss:0.5710511803627014\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 90 with loss: 0.5050764083862305 and average loss:0.5696003437042236\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 95 with loss: 0.6481711268424988 and average loss:0.5692706108093262\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 100 with loss: 0.46000227332115173 and average loss:0.5678715109825134\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 105 with loss: 0.5706837773323059 and average loss:0.568906843662262\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 110 with loss: 0.610896110534668 and average loss:0.5653011798858643\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 115 with loss: 0.5257297158241272 and average loss:0.5655614137649536\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 120 with loss: 0.46857550740242004 and average loss:0.5645604729652405\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 125 with loss: 0.6467693448066711 and average loss:0.5642534494400024\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 130 with loss: 0.5609813332557678 and average loss:0.5646474957466125\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 135 with loss: 0.5697449445724487 and average loss:0.5639608502388\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 140 with loss: 0.5212787389755249 and average loss:0.5630574226379395\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 145 with loss: 0.4810379445552826 and average loss:0.561896562576294\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 150 with loss: 0.6538575887680054 and average loss:0.5644094944000244\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 155 with loss: 0.6125239133834839 and average loss:0.5654534101486206\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 160 with loss: 0.5646942853927612 and average loss:0.5649169683456421\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 165 with loss: 0.7193529009819031 and average loss:0.5658868551254272\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 170 with loss: 0.5985740423202515 and average loss:0.5652479529380798\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 175 with loss: 0.6143948435783386 and average loss:0.5646576881408691\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 180 with loss: 0.7195567488670349 and average loss:0.5651757717132568\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 185 with loss: 0.6090360283851624 and average loss:0.5665713548660278\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 190 with loss: 0.5384719967842102 and average loss:0.5646714568138123\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 195 with loss: 0.5994303226470947 and average loss:0.5648351907730103\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 200 with loss: 0.463704377412796 and average loss:0.564094066619873\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 205 with loss: 0.687919557094574 and average loss:0.563863217830658\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 210 with loss: 0.6215572953224182 and average loss:0.5649200677871704\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 215 with loss: 0.6592714786529541 and average loss:0.5657205581665039\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 220 with loss: 0.4240908920764923 and average loss:0.5640963315963745\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 225 with loss: 0.5249927043914795 and average loss:0.5646921992301941\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 230 with loss: 0.7171963453292847 and average loss:0.5645231604576111\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 235 with loss: 0.6499626636505127 and average loss:0.563281774520874\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 240 with loss: 0.5483294725418091 and average loss:0.56293123960495\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 245 with loss: 0.4749602675437927 and average loss:0.5627939701080322\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 1, batch: 250 with loss: 0.6498116254806519 and average loss:0.5624313354492188\n",
            "Epoch 2: LR=0.000120\n",
            "epoch: 2, batch: 0 with loss: 0.5950992703437805 and average loss:0.5950992703437805\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 5 with loss: 0.6266629695892334 and average loss:0.5793083310127258\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 10 with loss: 0.5211281776428223 and average loss:0.5532565116882324\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 15 with loss: 0.36202505230903625 and average loss:0.5435588359832764\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 20 with loss: 0.47528743743896484 and average loss:0.5427245497703552\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 25 with loss: 0.4848349094390869 and average loss:0.5379379391670227\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 30 with loss: 0.528670608997345 and average loss:0.5399532914161682\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 35 with loss: 0.42108288407325745 and average loss:0.5380191802978516\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 40 with loss: 0.5295065641403198 and average loss:0.534363865852356\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 45 with loss: 0.5714938044548035 and average loss:0.5376051664352417\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 50 with loss: 0.479813814163208 and average loss:0.5410588383674622\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 55 with loss: 0.48920938372612 and average loss:0.5422565340995789\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 60 with loss: 0.47231215238571167 and average loss:0.5433679223060608\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 65 with loss: 0.5319466590881348 and average loss:0.5396164655685425\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 70 with loss: 0.5316500663757324 and average loss:0.5394724607467651\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 75 with loss: 0.43301957845687866 and average loss:0.5340235829353333\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 80 with loss: 0.5875462889671326 and average loss:0.5357215404510498\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 85 with loss: 0.4792686402797699 and average loss:0.5310578942298889\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 90 with loss: 0.6286258101463318 and average loss:0.5294962525367737\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 95 with loss: 0.261513352394104 and average loss:0.5259279608726501\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 100 with loss: 0.5352797508239746 and average loss:0.5302177667617798\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 105 with loss: 0.45272165536880493 and average loss:0.5277098417282104\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 110 with loss: 0.5298006534576416 and average loss:0.5271223187446594\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 115 with loss: 0.49541163444519043 and average loss:0.5270258188247681\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 120 with loss: 0.6211671829223633 and average loss:0.5268616080284119\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 125 with loss: 0.5445430874824524 and average loss:0.5288439989089966\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 130 with loss: 0.6082561612129211 and average loss:0.5283821821212769\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 135 with loss: 0.5181480050086975 and average loss:0.5279141068458557\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 140 with loss: 0.542426347732544 and average loss:0.5300799608230591\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 145 with loss: 0.46909913420677185 and average loss:0.5311604738235474\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 150 with loss: 0.5415867567062378 and average loss:0.530935525894165\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 155 with loss: 0.5547691583633423 and average loss:0.5327997803688049\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 160 with loss: 0.5033328533172607 and average loss:0.5312724113464355\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 165 with loss: 0.38530394434928894 and average loss:0.5276722311973572\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 170 with loss: 0.709365725517273 and average loss:0.5276907086372375\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 175 with loss: 0.39759552478790283 and average loss:0.5271005630493164\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 180 with loss: 0.4499475061893463 and average loss:0.5264331102371216\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 185 with loss: 0.36704105138778687 and average loss:0.5245895981788635\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 190 with loss: 0.5449433326721191 and average loss:0.5236833095550537\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 195 with loss: 0.553886353969574 and average loss:0.5258204936981201\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 200 with loss: 0.5839110016822815 and average loss:0.5261496305465698\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 205 with loss: 0.39796245098114014 and average loss:0.5252046585083008\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 210 with loss: 0.5441603064537048 and average loss:0.5239630341529846\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 215 with loss: 0.5992051362991333 and average loss:0.524027943611145\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 220 with loss: 0.46882885694503784 and average loss:0.5233330130577087\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 225 with loss: 0.4913305640220642 and average loss:0.522415280342102\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 230 with loss: 0.6178246140480042 and average loss:0.5211015343666077\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 235 with loss: 0.49695730209350586 and average loss:0.5207562446594238\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 240 with loss: 0.4116551876068115 and average loss:0.5190869569778442\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 245 with loss: 0.4944100081920624 and average loss:0.5177472829818726\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 2, batch: 250 with loss: 0.39988085627555847 and average loss:0.5167144536972046\n",
            "Epoch 3: LR=0.000120\n",
            "epoch: 3, batch: 0 with loss: 0.34392067790031433 and average loss:0.34392067790031433\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 5 with loss: 0.4286128282546997 and average loss:0.4677833914756775\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 10 with loss: 0.6315873861312866 and average loss:0.48024433851242065\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 15 with loss: 0.6440550684928894 and average loss:0.47588422894477844\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 20 with loss: 0.5712681412696838 and average loss:0.49435874819755554\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 25 with loss: 0.5675442814826965 and average loss:0.502098798751831\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 30 with loss: 0.49060696363449097 and average loss:0.49275216460227966\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 35 with loss: 0.5033311247825623 and average loss:0.4984821677207947\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 40 with loss: 0.5558925271034241 and average loss:0.49871885776519775\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 45 with loss: 0.6618982553482056 and average loss:0.49816587567329407\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 50 with loss: 0.4672152101993561 and average loss:0.49686750769615173\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 55 with loss: 0.39537832140922546 and average loss:0.4932962954044342\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 60 with loss: 0.5841872692108154 and average loss:0.4966142177581787\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 65 with loss: 0.6071922779083252 and average loss:0.4972712993621826\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 70 with loss: 0.43592530488967896 and average loss:0.4991728365421295\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 75 with loss: 0.4293312728404999 and average loss:0.49449363350868225\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 80 with loss: 0.5706320405006409 and average loss:0.49191972613334656\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 85 with loss: 0.3147066533565521 and average loss:0.4860074520111084\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 90 with loss: 0.31091731786727905 and average loss:0.4856032729148865\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 95 with loss: 0.4688231647014618 and average loss:0.485390305519104\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 100 with loss: 0.824259340763092 and average loss:0.48541364073753357\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 105 with loss: 0.511766254901886 and average loss:0.4860938787460327\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 110 with loss: 0.4353603422641754 and average loss:0.48432907462120056\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 115 with loss: 0.5927627086639404 and average loss:0.4852868616580963\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 120 with loss: 0.4281572699546814 and average loss:0.4868505597114563\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 125 with loss: 0.5232262015342712 and average loss:0.4869689345359802\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 130 with loss: 0.6156096458435059 and average loss:0.4865838885307312\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 135 with loss: 0.4196794629096985 and average loss:0.48629531264305115\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 140 with loss: 0.4856794476509094 and average loss:0.48496508598327637\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 145 with loss: 0.4287368953227997 and average loss:0.4852931499481201\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 150 with loss: 0.5123218894004822 and average loss:0.4852939248085022\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 155 with loss: 0.4062400460243225 and average loss:0.48466163873672485\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 160 with loss: 0.5649188160896301 and average loss:0.48531967401504517\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 165 with loss: 0.5094536542892456 and average loss:0.48498350381851196\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 170 with loss: 0.2969319522380829 and average loss:0.48157259821891785\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 175 with loss: 0.4854034185409546 and average loss:0.48157820105552673\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 180 with loss: 0.6681442260742188 and average loss:0.4825291633605957\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 185 with loss: 0.5000552535057068 and average loss:0.48253780603408813\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 190 with loss: 0.5484647750854492 and average loss:0.48240944743156433\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 195 with loss: 0.43824321031570435 and average loss:0.48186200857162476\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 200 with loss: 0.8500511646270752 and average loss:0.48254233598709106\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 205 with loss: 0.5549719333648682 and average loss:0.4827464520931244\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 210 with loss: 0.40788698196411133 and average loss:0.48131948709487915\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 215 with loss: 0.6225157380104065 and average loss:0.48257747292518616\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 220 with loss: 0.3108810484409332 and average loss:0.4806988835334778\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 225 with loss: 0.3619430363178253 and average loss:0.4793609380722046\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 230 with loss: 0.47437259554862976 and average loss:0.47945156693458557\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 235 with loss: 0.4314059615135193 and average loss:0.47816842794418335\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 240 with loss: 0.6539339423179626 and average loss:0.4772251546382904\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 245 with loss: 0.6459900140762329 and average loss:0.47695082426071167\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 3, batch: 250 with loss: 0.7570842504501343 and average loss:0.47819840908050537\n",
            "Epoch 4: LR=0.000120\n",
            "epoch: 4, batch: 0 with loss: 0.398974746465683 and average loss:0.398974746465683\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 5 with loss: 0.43984097242355347 and average loss:0.5073630809783936\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 10 with loss: 0.49387797713279724 and average loss:0.5136781334877014\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 15 with loss: 0.4702241122722626 and average loss:0.5019883513450623\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 20 with loss: 0.4395795166492462 and average loss:0.48408830165863037\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 25 with loss: 0.5603951811790466 and average loss:0.48566123843193054\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 30 with loss: 0.5031263828277588 and average loss:0.48172464966773987\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 35 with loss: 0.41458600759506226 and average loss:0.47408708930015564\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 40 with loss: 0.4427073001861572 and average loss:0.47294512391090393\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 45 with loss: 0.4080185890197754 and average loss:0.4682677388191223\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 50 with loss: 0.401863157749176 and average loss:0.46221494674682617\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 55 with loss: 0.39756232500076294 and average loss:0.46216607093811035\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 60 with loss: 0.2874357998371124 and average loss:0.45998552441596985\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 65 with loss: 0.7833619117736816 and average loss:0.46198567748069763\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 70 with loss: 0.5961338877677917 and average loss:0.46276581287384033\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 75 with loss: 0.5923933982849121 and average loss:0.46294793486595154\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 80 with loss: 0.3991307020187378 and average loss:0.4601729214191437\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 85 with loss: 0.4532245993614197 and average loss:0.46027621626853943\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 90 with loss: 0.4212045967578888 and average loss:0.46049413084983826\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 95 with loss: 0.3843662738800049 and average loss:0.4602828621864319\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 100 with loss: 0.43304333090782166 and average loss:0.46210089325904846\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 105 with loss: 0.37164023518562317 and average loss:0.4584518074989319\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 110 with loss: 0.39505496621131897 and average loss:0.4561357796192169\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 115 with loss: 0.3965972363948822 and average loss:0.45640334486961365\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 120 with loss: 0.6143254041671753 and average loss:0.45776522159576416\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 125 with loss: 0.43629300594329834 and average loss:0.46016448736190796\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 130 with loss: 0.45905357599258423 and average loss:0.4638434946537018\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 135 with loss: 0.43293383717536926 and average loss:0.46539178490638733\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 140 with loss: 0.55625981092453 and average loss:0.4662177860736847\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 145 with loss: 0.44405612349510193 and average loss:0.4670008718967438\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 150 with loss: 0.44884538650512695 and average loss:0.4656659662723541\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 155 with loss: 0.4080657660961151 and average loss:0.4661167860031128\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 160 with loss: 0.3391021490097046 and average loss:0.46737444400787354\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 165 with loss: 0.3204631805419922 and average loss:0.4665190279483795\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 170 with loss: 0.3596037030220032 and average loss:0.46729376912117004\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 175 with loss: 0.37278085947036743 and average loss:0.4671732783317566\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 180 with loss: 0.4767967462539673 and average loss:0.46724119782447815\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 185 with loss: 0.38595741987228394 and average loss:0.4652346074581146\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 190 with loss: 0.5275455117225647 and average loss:0.464447945356369\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 195 with loss: 0.4338786005973816 and average loss:0.4630936086177826\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 200 with loss: 0.3771563470363617 and average loss:0.46254396438598633\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 205 with loss: 0.38661932945251465 and average loss:0.46162134408950806\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 210 with loss: 0.6002534627914429 and average loss:0.4626564383506775\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 215 with loss: 0.45493584871292114 and average loss:0.463073194026947\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 220 with loss: 0.36534902453422546 and average loss:0.4627583622932434\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 225 with loss: 0.4211069345474243 and average loss:0.4619022011756897\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 230 with loss: 0.31387680768966675 and average loss:0.46120917797088623\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 235 with loss: 0.4349752962589264 and average loss:0.4617173671722412\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 240 with loss: 0.4285217225551605 and average loss:0.4612693786621094\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 245 with loss: 0.43101486563682556 and average loss:0.4594661295413971\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 4, batch: 250 with loss: 0.3138427734375 and average loss:0.4586137533187866\n",
            "Epoch 5: LR=0.000120\n",
            "epoch: 5, batch: 0 with loss: 0.3118833005428314 and average loss:0.3118833005428314\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 5 with loss: 0.6974213123321533 and average loss:0.5372052192687988\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 10 with loss: 0.8224809169769287 and average loss:0.5281301736831665\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 15 with loss: 0.43176135420799255 and average loss:0.4962104260921478\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 20 with loss: 0.49580997228622437 and average loss:0.4938535690307617\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 25 with loss: 0.28598305583000183 and average loss:0.47907689213752747\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 30 with loss: 0.4704042971134186 and average loss:0.47746407985687256\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 35 with loss: 0.29589012265205383 and average loss:0.4605991542339325\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 40 with loss: 0.27624550461769104 and average loss:0.453860342502594\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 45 with loss: 0.4782596826553345 and average loss:0.4478953778743744\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 50 with loss: 0.2841728627681732 and average loss:0.44003456830978394\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 55 with loss: 0.49274009466171265 and average loss:0.4414675831794739\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 60 with loss: 0.4122457206249237 and average loss:0.44279730319976807\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 65 with loss: 0.4999665319919586 and average loss:0.44590598344802856\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 70 with loss: 0.3781428337097168 and average loss:0.4489590525627136\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 75 with loss: 0.44607046246528625 and average loss:0.45010894536972046\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 80 with loss: 0.32957524061203003 and average loss:0.4488554298877716\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 85 with loss: 0.412060022354126 and average loss:0.45035991072654724\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 90 with loss: 0.45298346877098083 and average loss:0.4559536576271057\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 95 with loss: 0.5855646133422852 and average loss:0.45488256216049194\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 100 with loss: 0.5316887497901917 and average loss:0.4559221863746643\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 105 with loss: 0.5200974345207214 and average loss:0.45472055673599243\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 110 with loss: 0.33497557044029236 and average loss:0.45538586378097534\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 115 with loss: 0.42931756377220154 and average loss:0.45634332299232483\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 120 with loss: 0.4137890338897705 and average loss:0.45699793100357056\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 125 with loss: 0.33391451835632324 and average loss:0.4552190601825714\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 130 with loss: 0.3121936023235321 and average loss:0.452968031167984\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 135 with loss: 0.6279124021530151 and average loss:0.45475393533706665\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 140 with loss: 0.3149360418319702 and average loss:0.45287439227104187\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 145 with loss: 0.4249483048915863 and average loss:0.4534631073474884\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 150 with loss: 0.36678895354270935 and average loss:0.4546643793582916\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 155 with loss: 0.4025931656360626 and average loss:0.4543752074241638\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 160 with loss: 0.41936448216438293 and average loss:0.45392605662345886\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 165 with loss: 0.4746706783771515 and average loss:0.4536912143230438\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 170 with loss: 0.44504013657569885 and average loss:0.4536060690879822\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 175 with loss: 0.39120426774024963 and average loss:0.4533492624759674\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 180 with loss: 0.4228854775428772 and average loss:0.45243600010871887\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 185 with loss: 0.3750961720943451 and average loss:0.44962841272354126\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 190 with loss: 0.5876420140266418 and average loss:0.44930291175842285\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 195 with loss: 0.4464139938354492 and average loss:0.448883056640625\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 200 with loss: 0.2296133190393448 and average loss:0.44609716534614563\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 205 with loss: 0.5278078317642212 and average loss:0.4470604360103607\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 210 with loss: 0.3552463948726654 and average loss:0.44496992230415344\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 215 with loss: 0.4406895637512207 and average loss:0.4459631145000458\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 220 with loss: 0.3310393691062927 and average loss:0.4445141553878784\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 225 with loss: 0.3485051989555359 and average loss:0.44289448857307434\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 230 with loss: 0.2770649194717407 and average loss:0.4422588646411896\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 235 with loss: 0.5471969246864319 and average loss:0.442276269197464\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 240 with loss: 0.3738742768764496 and average loss:0.44066205620765686\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 245 with loss: 0.30833834409713745 and average loss:0.4403238594532013\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 5, batch: 250 with loss: 0.7199772596359253 and average loss:0.44114232063293457\n",
            "Epoch 6: LR=0.000120\n",
            "epoch: 6, batch: 0 with loss: 0.35011401772499084 and average loss:0.35011401772499084\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 5 with loss: 0.4567529559135437 and average loss:0.34106510877609253\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 10 with loss: 0.3962860107421875 and average loss:0.35327911376953125\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 15 with loss: 0.5120358467102051 and average loss:0.38945117592811584\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 20 with loss: 0.40345898270606995 and average loss:0.39786967635154724\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 25 with loss: 0.336809903383255 and average loss:0.40608397126197815\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 30 with loss: 0.4717414975166321 and average loss:0.4106579124927521\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 35 with loss: 0.32954341173171997 and average loss:0.413387268781662\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 40 with loss: 0.4536748230457306 and average loss:0.4146104156970978\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 45 with loss: 0.5032535791397095 and average loss:0.41977232694625854\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 50 with loss: 0.4599200189113617 and average loss:0.41665318608283997\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 55 with loss: 0.5793808698654175 and average loss:0.4216514527797699\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 60 with loss: 0.5831826329231262 and average loss:0.4249725937843323\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 65 with loss: 0.3208301365375519 and average loss:0.425792396068573\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 70 with loss: 0.2892412543296814 and average loss:0.4258161783218384\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 75 with loss: 0.4594525694847107 and average loss:0.41985562443733215\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 80 with loss: 0.5887960195541382 and average loss:0.4214809238910675\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 85 with loss: 0.6908403635025024 and average loss:0.425182968378067\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 90 with loss: 0.3342830538749695 and average loss:0.42273420095443726\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 95 with loss: 0.2899183928966522 and average loss:0.4247375726699829\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 100 with loss: 0.41419827938079834 and average loss:0.42517271637916565\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 105 with loss: 0.42610350251197815 and average loss:0.4214218854904175\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 110 with loss: 0.37876567244529724 and average loss:0.4176376461982727\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 115 with loss: 0.4970542788505554 and average loss:0.4194153845310211\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 120 with loss: 0.5448699593544006 and average loss:0.4246363639831543\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 125 with loss: 0.40577277541160583 and average loss:0.4261512756347656\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 130 with loss: 0.503680408000946 and average loss:0.4258211553096771\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 135 with loss: 0.545074999332428 and average loss:0.42470577359199524\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 140 with loss: 0.35764941573143005 and average loss:0.4225817024707794\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 145 with loss: 0.45933863520622253 and average loss:0.42216014862060547\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 150 with loss: 0.4864644706249237 and average loss:0.42305514216423035\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 155 with loss: 0.3243348002433777 and average loss:0.4219866991043091\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 160 with loss: 0.35531070828437805 and average loss:0.4225955009460449\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 165 with loss: 0.454196572303772 and average loss:0.4233386814594269\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 170 with loss: 0.4099929630756378 and average loss:0.42231711745262146\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 175 with loss: 0.6684543490409851 and average loss:0.42338380217552185\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 180 with loss: 0.48851659893989563 and average loss:0.4223254323005676\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 185 with loss: 0.42045992612838745 and average loss:0.4217824935913086\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 190 with loss: 0.3485565483570099 and average loss:0.4226347506046295\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 195 with loss: 0.637170672416687 and average loss:0.4220160245895386\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 200 with loss: 0.47222501039505005 and average loss:0.42228132486343384\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 205 with loss: 0.38719555735588074 and average loss:0.4217294156551361\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 210 with loss: 0.49151405692100525 and average loss:0.4211493134498596\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 215 with loss: 0.6755662560462952 and average loss:0.4206039011478424\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 220 with loss: 0.47320401668548584 and average loss:0.4201051592826843\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 225 with loss: 0.4670657813549042 and average loss:0.4202803671360016\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 230 with loss: 0.39793476462364197 and average loss:0.4204546809196472\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 235 with loss: 0.5532220602035522 and average loss:0.42098742723464966\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 240 with loss: 0.45318710803985596 and average loss:0.4205593168735504\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 245 with loss: 0.4130617082118988 and average loss:0.42163220047950745\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 6, batch: 250 with loss: 0.4736999571323395 and average loss:0.4221166670322418\n",
            "Epoch 7: LR=0.000120\n",
            "epoch: 7, batch: 0 with loss: 0.3141832649707794 and average loss:0.3141832649707794\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 5 with loss: 0.5393091440200806 and average loss:0.41697797179222107\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 10 with loss: 0.44676730036735535 and average loss:0.4458160400390625\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 15 with loss: 0.252139687538147 and average loss:0.41955310106277466\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 20 with loss: 0.560106635093689 and average loss:0.4161831736564636\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 25 with loss: 0.2676500678062439 and average loss:0.4125004708766937\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 30 with loss: 0.2661062180995941 and average loss:0.4065162241458893\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 35 with loss: 0.49968844652175903 and average loss:0.40845584869384766\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 40 with loss: 0.34745359420776367 and average loss:0.4106253385543823\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 45 with loss: 0.46045568585395813 and average loss:0.41185376048088074\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 50 with loss: 0.4783281981945038 and average loss:0.4160205125808716\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 55 with loss: 0.4312591850757599 and average loss:0.410767138004303\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 60 with loss: 0.5911427736282349 and average loss:0.41422560811042786\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 65 with loss: 0.31732362508773804 and average loss:0.41378411650657654\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 70 with loss: 0.3183061182498932 and average loss:0.4123239815235138\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 75 with loss: 0.5146961212158203 and average loss:0.4147617816925049\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 80 with loss: 0.4324406087398529 and average loss:0.41551199555397034\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 85 with loss: 0.4554368555545807 and average loss:0.4142374098300934\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 90 with loss: 0.47174257040023804 and average loss:0.41463491320610046\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 95 with loss: 0.3402092754840851 and average loss:0.41246312856674194\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 100 with loss: 0.35456863045692444 and average loss:0.4093993902206421\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 105 with loss: 0.4713973104953766 and average loss:0.4091953933238983\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 110 with loss: 0.3580467104911804 and average loss:0.40864673256874084\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 115 with loss: 0.3932388126850128 and average loss:0.4064134955406189\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 120 with loss: 0.46122220158576965 and average loss:0.4087396562099457\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 125 with loss: 0.35436397790908813 and average loss:0.40899384021759033\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 130 with loss: 0.48665115237236023 and average loss:0.41091829538345337\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 135 with loss: 0.5460945963859558 and average loss:0.41146329045295715\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 140 with loss: 0.5819358825683594 and average loss:0.4130818843841553\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 145 with loss: 0.3609429597854614 and average loss:0.41270706057548523\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 150 with loss: 0.380051851272583 and average loss:0.4139663875102997\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 155 with loss: 0.41563549637794495 and average loss:0.41421011090278625\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 160 with loss: 0.5555344820022583 and average loss:0.4163641631603241\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 165 with loss: 0.26390376687049866 and average loss:0.4165748953819275\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 170 with loss: 0.36109501123428345 and average loss:0.4149520993232727\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 175 with loss: 0.5724747776985168 and average loss:0.4157199561595917\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 180 with loss: 0.34949883818626404 and average loss:0.41692349314689636\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 185 with loss: 0.3892596662044525 and average loss:0.41814616322517395\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 190 with loss: 0.5192295908927917 and average loss:0.4212409555912018\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 195 with loss: 0.40478867292404175 and average loss:0.4212309420108795\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 200 with loss: 0.4460320472717285 and average loss:0.42068731784820557\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 205 with loss: 0.41650745272636414 and average loss:0.4189918637275696\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 210 with loss: 0.2967858910560608 and average loss:0.41591066122055054\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 215 with loss: 0.41433340311050415 and average loss:0.4133021831512451\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 220 with loss: 0.4168923497200012 and average loss:0.413042813539505\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 225 with loss: 0.3561960458755493 and average loss:0.4127553403377533\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 230 with loss: 0.5819003582000732 and average loss:0.4122247099876404\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 235 with loss: 0.4728851020336151 and average loss:0.4122355580329895\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 240 with loss: 0.4390278458595276 and average loss:0.4116479158401489\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 245 with loss: 0.44716504216194153 and average loss:0.41338083148002625\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 7, batch: 250 with loss: 0.3622816205024719 and average loss:0.4130009114742279\n",
            "Epoch 8: LR=0.000120\n",
            "epoch: 8, batch: 0 with loss: 0.31773266196250916 and average loss:0.31773266196250916\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 5 with loss: 0.2630867660045624 and average loss:0.36586880683898926\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 10 with loss: 0.35771235823631287 and average loss:0.37615126371383667\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 15 with loss: 0.6942499876022339 and average loss:0.39475786685943604\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 20 with loss: 0.48688438534736633 and average loss:0.3918614089488983\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 25 with loss: 0.4045177102088928 and average loss:0.3963063955307007\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 30 with loss: 0.3672253489494324 and average loss:0.40482550859451294\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 35 with loss: 0.4608137011528015 and average loss:0.40491342544555664\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 40 with loss: 0.4858548641204834 and average loss:0.4083925485610962\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 45 with loss: 0.3640882670879364 and average loss:0.4031234085559845\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 50 with loss: 0.35844916105270386 and average loss:0.403382807970047\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 55 with loss: 0.45549607276916504 and average loss:0.4033567011356354\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 60 with loss: 0.35327577590942383 and average loss:0.39724665880203247\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 65 with loss: 0.5130325555801392 and average loss:0.3973429799079895\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 70 with loss: 0.3447595536708832 and average loss:0.39531105756759644\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 75 with loss: 0.3364246189594269 and average loss:0.3913554847240448\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 80 with loss: 0.23059919476509094 and average loss:0.39374056458473206\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 85 with loss: 0.22678083181381226 and average loss:0.39071473479270935\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 90 with loss: 0.4175579249858856 and average loss:0.3851996660232544\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 95 with loss: 0.45374342799186707 and average loss:0.38898587226867676\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 100 with loss: 0.5084936022758484 and average loss:0.38966313004493713\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 105 with loss: 0.34249240159988403 and average loss:0.39266854524612427\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 110 with loss: 0.3211404085159302 and average loss:0.3923848569393158\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 115 with loss: 0.34128478169441223 and average loss:0.390571653842926\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 120 with loss: 0.3300206959247589 and average loss:0.3912983238697052\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 125 with loss: 0.3866932988166809 and average loss:0.392341285943985\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 130 with loss: 0.4213821589946747 and average loss:0.39254358410835266\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 135 with loss: 0.341734915971756 and average loss:0.3922841548919678\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 140 with loss: 0.6155859231948853 and average loss:0.395871102809906\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 145 with loss: 0.3824370503425598 and average loss:0.39672401547431946\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 150 with loss: 0.45904895663261414 and average loss:0.39779359102249146\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 155 with loss: 0.36506032943725586 and average loss:0.3971651494503021\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 160 with loss: 0.5336347222328186 and average loss:0.3983100354671478\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 165 with loss: 0.30418694019317627 and average loss:0.39651626348495483\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 170 with loss: 0.43219706416130066 and average loss:0.3958343267440796\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 175 with loss: 0.42835384607315063 and average loss:0.3980890214443207\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 180 with loss: 0.3227199912071228 and average loss:0.3990377187728882\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 185 with loss: 0.35499492287635803 and average loss:0.3985055983066559\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 190 with loss: 0.3815223276615143 and average loss:0.39804312586784363\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 195 with loss: 0.3518010079860687 and average loss:0.3973122835159302\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 200 with loss: 0.40439096093177795 and average loss:0.3964969217777252\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 205 with loss: 0.3648751974105835 and average loss:0.39619526267051697\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 210 with loss: 0.5588438510894775 and average loss:0.3971073627471924\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 215 with loss: 0.42912033200263977 and average loss:0.3962346613407135\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 220 with loss: 0.47182294726371765 and average loss:0.3984373211860657\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 225 with loss: 0.4379180669784546 and average loss:0.39824217557907104\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 230 with loss: 0.4562380909919739 and average loss:0.3982180058956146\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 235 with loss: 0.353328138589859 and average loss:0.3988187909126282\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 240 with loss: 0.304456889629364 and average loss:0.3987969756126404\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 245 with loss: 0.7465183734893799 and average loss:0.40019646286964417\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 8, batch: 250 with loss: 0.3147134482860565 and average loss:0.39864692091941833\n",
            "Epoch 9: LR=0.000120\n",
            "epoch: 9, batch: 0 with loss: 0.4055897295475006 and average loss:0.4055897295475006\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 5 with loss: 0.38778844475746155 and average loss:0.35275763273239136\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 10 with loss: 0.39908650517463684 and average loss:0.36941078305244446\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 15 with loss: 0.31002894043922424 and average loss:0.3516955077648163\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 20 with loss: 0.3410051167011261 and average loss:0.3473508656024933\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 25 with loss: 0.26271310448646545 and average loss:0.3458375334739685\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 30 with loss: 0.4232659339904785 and average loss:0.35729196667671204\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 35 with loss: 0.30993494391441345 and average loss:0.3680223226547241\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 40 with loss: 0.34570813179016113 and average loss:0.36582425236701965\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 45 with loss: 0.3221266269683838 and average loss:0.3704698383808136\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 50 with loss: 0.43626147508621216 and average loss:0.369981586933136\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 55 with loss: 0.4593597650527954 and average loss:0.3694555163383484\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 60 with loss: 0.3773459196090698 and average loss:0.3705887496471405\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 65 with loss: 0.5167578458786011 and average loss:0.3720240592956543\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 70 with loss: 0.34561020135879517 and average loss:0.3740313947200775\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 75 with loss: 0.49921077489852905 and average loss:0.3784511387348175\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 80 with loss: 0.3701203167438507 and average loss:0.38173535466194153\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 85 with loss: 0.3891274929046631 and average loss:0.38125163316726685\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 90 with loss: 0.4997912645339966 and average loss:0.3814274072647095\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 95 with loss: 0.40261274576187134 and average loss:0.38399240374565125\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 100 with loss: 0.4124765396118164 and average loss:0.3855041265487671\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 105 with loss: 0.48426809906959534 and average loss:0.3857235610485077\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 110 with loss: 0.3937649130821228 and average loss:0.38548457622528076\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 115 with loss: 0.3808657228946686 and average loss:0.3892066776752472\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 120 with loss: 0.41009023785591125 and average loss:0.3912298381328583\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 125 with loss: 0.31699469685554504 and average loss:0.38967782258987427\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 130 with loss: 0.2861348092556 and average loss:0.39008358120918274\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 135 with loss: 0.42132049798965454 and average loss:0.3887506425380707\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 140 with loss: 0.5834798812866211 and average loss:0.3893083930015564\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 145 with loss: 0.2542894184589386 and average loss:0.3916848301887512\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 150 with loss: 0.4366399645805359 and average loss:0.3919570744037628\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 155 with loss: 0.4686717391014099 and average loss:0.38962870836257935\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 160 with loss: 0.42080095410346985 and average loss:0.39082738757133484\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 165 with loss: 0.3105243444442749 and average loss:0.388801246881485\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 170 with loss: 0.400029718875885 and average loss:0.38923636078834534\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 175 with loss: 0.4711259603500366 and average loss:0.3885480761528015\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 180 with loss: 0.43623456358909607 and average loss:0.386874794960022\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 185 with loss: 0.6336047053337097 and average loss:0.3889809548854828\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 190 with loss: 0.32956767082214355 and average loss:0.3879740536212921\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 195 with loss: 0.21165044605731964 and average loss:0.3854972720146179\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 200 with loss: 0.3484654724597931 and average loss:0.38619494438171387\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 205 with loss: 0.21631884574890137 and average loss:0.3841448426246643\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 210 with loss: 0.45137763023376465 and average loss:0.3846054673194885\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 215 with loss: 0.3345663249492645 and average loss:0.38320496678352356\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 220 with loss: 0.3511474132537842 and average loss:0.3837445080280304\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 225 with loss: 0.3209993243217468 and average loss:0.38381820917129517\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 230 with loss: 0.32204604148864746 and average loss:0.3825007975101471\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 235 with loss: 0.48271846771240234 and average loss:0.38248974084854126\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 240 with loss: 0.2954736649990082 and average loss:0.3813175559043884\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 245 with loss: 0.5834924578666687 and average loss:0.3811737298965454\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 9, batch: 250 with loss: 1.0275003910064697 and average loss:0.38361456990242004\n",
            "Epoch 10: LR=0.000120\n",
            "epoch: 10, batch: 0 with loss: 0.33038824796676636 and average loss:0.33038824796676636\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 5 with loss: 0.41035956144332886 and average loss:0.35305696725845337\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 10 with loss: 0.35934633016586304 and average loss:0.34912198781967163\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 15 with loss: 0.41187772154808044 and average loss:0.3762401044368744\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 20 with loss: 0.44618821144104004 and average loss:0.37672606110572815\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 25 with loss: 0.3580280542373657 and average loss:0.37491315603256226\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 30 with loss: 0.5131821632385254 and average loss:0.3772549629211426\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 35 with loss: 0.37680357694625854 and average loss:0.3795592188835144\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 40 with loss: 0.39320021867752075 and average loss:0.3769500255584717\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 45 with loss: 0.3968493640422821 and average loss:0.3841855823993683\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 50 with loss: 0.3326684534549713 and average loss:0.38895151019096375\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 55 with loss: 0.2135206013917923 and average loss:0.3864898383617401\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 60 with loss: 0.2750336825847626 and average loss:0.38421496748924255\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 65 with loss: 0.39985015988349915 and average loss:0.38022875785827637\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 70 with loss: 0.3915700912475586 and average loss:0.37907490134239197\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 75 with loss: 0.35107916593551636 and average loss:0.3789990544319153\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 80 with loss: 0.24096694588661194 and average loss:0.3737036883831024\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 85 with loss: 0.3239912986755371 and average loss:0.3695516884326935\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 90 with loss: 0.4096065163612366 and average loss:0.368833988904953\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 95 with loss: 0.4258436858654022 and average loss:0.3692176043987274\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 100 with loss: 0.6437749266624451 and average loss:0.36983248591423035\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 105 with loss: 0.3323262929916382 and average loss:0.37318748235702515\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 110 with loss: 0.2297820895910263 and average loss:0.37056586146354675\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 115 with loss: 0.22021441161632538 and average loss:0.36966967582702637\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 120 with loss: 0.545753002166748 and average loss:0.36941275000572205\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 125 with loss: 0.31728339195251465 and average loss:0.3692568838596344\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 130 with loss: 0.35642245411872864 and average loss:0.3692089915275574\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 135 with loss: 0.5059763193130493 and average loss:0.3695996105670929\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 140 with loss: 0.32853472232818604 and average loss:0.3700876832008362\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 145 with loss: 0.4490121006965637 and average loss:0.37116217613220215\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 150 with loss: 0.43861594796180725 and average loss:0.3715076446533203\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 155 with loss: 0.3238762319087982 and average loss:0.3726179301738739\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 160 with loss: 0.34052586555480957 and average loss:0.3729515075683594\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 165 with loss: 0.5448850989341736 and average loss:0.375383198261261\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 170 with loss: 0.34705665707588196 and average loss:0.3752916157245636\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 175 with loss: 0.28913643956184387 and average loss:0.3757202625274658\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 180 with loss: 0.5385615229606628 and average loss:0.3759518265724182\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 185 with loss: 0.36967113614082336 and average loss:0.37573757767677307\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 190 with loss: 0.3288125693798065 and average loss:0.37524130940437317\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 195 with loss: 0.22171512246131897 and average loss:0.375331312417984\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 200 with loss: 0.6847551465034485 and average loss:0.3749851584434509\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 205 with loss: 0.29654431343078613 and average loss:0.37418869137763977\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 210 with loss: 0.3658562898635864 and average loss:0.37534037232398987\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 215 with loss: 0.3476536273956299 and average loss:0.3753671944141388\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 220 with loss: 0.4669526517391205 and average loss:0.37717750668525696\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 225 with loss: 0.42315027117729187 and average loss:0.3764457106590271\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 230 with loss: 0.24537339806556702 and average loss:0.3759278357028961\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 235 with loss: 0.4550717771053314 and average loss:0.37565553188323975\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 240 with loss: 0.1852256953716278 and average loss:0.37453997135162354\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 245 with loss: 0.3140729069709778 and average loss:0.3723147511482239\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 10, batch: 250 with loss: 0.36285296082496643 and average loss:0.37230032682418823\n",
            "Epoch 11: LR=0.000120\n",
            "epoch: 11, batch: 0 with loss: 0.455514520406723 and average loss:0.455514520406723\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 5 with loss: 0.3839782476425171 and average loss:0.3949702978134155\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 10 with loss: 0.28478336334228516 and average loss:0.36036762595176697\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 15 with loss: 0.2739713191986084 and average loss:0.3658984899520874\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 20 with loss: 0.4451664686203003 and average loss:0.36854293942451477\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 25 with loss: 0.5045021176338196 and average loss:0.3744593560695648\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 30 with loss: 0.35259541869163513 and average loss:0.36662861704826355\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 35 with loss: 0.4163157045841217 and average loss:0.37639328837394714\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 40 with loss: 0.42025330662727356 and average loss:0.37177085876464844\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 45 with loss: 0.33673524856567383 and average loss:0.3639140725135803\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 50 with loss: 0.38877907395362854 and average loss:0.35987603664398193\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 55 with loss: 0.34314030408859253 and average loss:0.36155471205711365\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 60 with loss: 0.22958646714687347 and average loss:0.3658146262168884\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 65 with loss: 0.36239150166511536 and average loss:0.3654467463493347\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 70 with loss: 0.24940133094787598 and average loss:0.3599235415458679\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 75 with loss: 0.2526775002479553 and average loss:0.3547522723674774\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 80 with loss: 0.3683544397354126 and average loss:0.3531409502029419\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 85 with loss: 0.33999955654144287 and average loss:0.3578245937824249\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 90 with loss: 0.5087761878967285 and average loss:0.35950902104377747\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 95 with loss: 0.35290971398353577 and average loss:0.3596387505531311\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 100 with loss: 0.408834844827652 and average loss:0.35846665501594543\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 105 with loss: 0.5686333775520325 and average loss:0.35724174976348877\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 110 with loss: 0.3341517150402069 and average loss:0.3579123914241791\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 115 with loss: 0.3687017560005188 and average loss:0.35594475269317627\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 120 with loss: 0.20983274281024933 and average loss:0.3580358326435089\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 125 with loss: 0.48440274596214294 and average loss:0.359447717666626\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 130 with loss: 0.4730641841888428 and average loss:0.3625546097755432\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 135 with loss: 0.4400901198387146 and average loss:0.363314151763916\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 140 with loss: 0.3964196443557739 and average loss:0.36180248856544495\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 145 with loss: 0.3604431450366974 and average loss:0.361036479473114\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 150 with loss: 0.3698347508907318 and average loss:0.360554963350296\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 155 with loss: 0.21167591214179993 and average loss:0.3616412281990051\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 160 with loss: 0.3839017450809479 and average loss:0.3619716167449951\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 165 with loss: 0.6635163426399231 and average loss:0.3623950183391571\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 170 with loss: 0.3672034442424774 and average loss:0.36369478702545166\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 175 with loss: 0.2613973021507263 and average loss:0.36310940980911255\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 180 with loss: 0.4003768563270569 and average loss:0.36393001675605774\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 185 with loss: 0.3724764883518219 and average loss:0.36358946561813354\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 190 with loss: 0.3379390239715576 and average loss:0.3645118176937103\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 195 with loss: 0.2767035663127899 and average loss:0.3634970188140869\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 200 with loss: 0.4399740993976593 and average loss:0.36510419845581055\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 205 with loss: 0.24832767248153687 and average loss:0.3658027946949005\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 210 with loss: 0.41692227125167847 and average loss:0.3657090663909912\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 215 with loss: 0.2514887750148773 and average loss:0.36496004462242126\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 220 with loss: 0.33602190017700195 and average loss:0.36475077271461487\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 225 with loss: 0.28307491540908813 and average loss:0.3645963668823242\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 230 with loss: 0.395534873008728 and average loss:0.3656870424747467\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 235 with loss: 0.2035369873046875 and average loss:0.36550381779670715\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 240 with loss: 0.40306898951530457 and average loss:0.36552515625953674\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 245 with loss: 0.5616431832313538 and average loss:0.367227703332901\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 11, batch: 250 with loss: 0.4133874475955963 and average loss:0.36734500527381897\n",
            "Epoch 12: LR=0.000120\n",
            "epoch: 12, batch: 0 with loss: 0.3800255358219147 and average loss:0.3800255358219147\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 5 with loss: 0.2976526618003845 and average loss:0.38467320799827576\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 10 with loss: 0.2949465811252594 and average loss:0.3707755506038666\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 15 with loss: 0.2848243713378906 and average loss:0.35015785694122314\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 20 with loss: 0.24916468560695648 and average loss:0.36519041657447815\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 25 with loss: 0.3746967911720276 and average loss:0.3655913174152374\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 30 with loss: 0.36996394395828247 and average loss:0.3659570515155792\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 35 with loss: 0.34448355436325073 and average loss:0.37172070145606995\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 40 with loss: 0.5657973885536194 and average loss:0.3757880628108978\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 45 with loss: 0.30410876870155334 and average loss:0.37150219082832336\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 50 with loss: 0.2997123599052429 and average loss:0.36859557032585144\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 55 with loss: 0.5737196207046509 and average loss:0.367732435464859\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 60 with loss: 0.2752339541912079 and average loss:0.3646434247493744\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 65 with loss: 0.3281731903553009 and average loss:0.3623183071613312\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 70 with loss: 0.38565143942832947 and average loss:0.3579626679420471\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 75 with loss: 0.39542099833488464 and average loss:0.3621838390827179\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 80 with loss: 0.418011337518692 and average loss:0.36166056990623474\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 85 with loss: 0.34066393971443176 and average loss:0.3598249554634094\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 90 with loss: 0.31834203004837036 and average loss:0.36528608202934265\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 95 with loss: 0.29227951169013977 and average loss:0.36142703890800476\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 100 with loss: 0.5821142196655273 and average loss:0.36346620321273804\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 105 with loss: 0.3592144250869751 and average loss:0.3596976697444916\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 110 with loss: 0.36693504452705383 and average loss:0.3584006130695343\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 115 with loss: 0.38292625546455383 and average loss:0.3590543568134308\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 120 with loss: 0.4078044891357422 and average loss:0.3590247333049774\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 125 with loss: 0.38035717606544495 and average loss:0.3604760766029358\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 130 with loss: 0.3885146975517273 and average loss:0.35982224345207214\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 135 with loss: 0.33625054359436035 and average loss:0.35927167534828186\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 140 with loss: 0.23264876008033752 and average loss:0.3569543659687042\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 145 with loss: 0.39238816499710083 and average loss:0.36020639538764954\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 150 with loss: 0.29281342029571533 and average loss:0.3604070544242859\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 155 with loss: 0.29611027240753174 and average loss:0.3594749867916107\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 160 with loss: 0.36288005113601685 and average loss:0.3581373989582062\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 165 with loss: 0.19581080973148346 and average loss:0.35786473751068115\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 170 with loss: 0.31406018137931824 and average loss:0.3575398325920105\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 175 with loss: 0.28119704127311707 and average loss:0.35844719409942627\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 180 with loss: 0.33297255635261536 and average loss:0.35715317726135254\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 185 with loss: 0.44001248478889465 and average loss:0.3572545647621155\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 190 with loss: 0.3647971451282501 and average loss:0.3589269816875458\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 195 with loss: 0.3236837387084961 and average loss:0.35823726654052734\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 200 with loss: 0.36780232191085815 and average loss:0.3592211902141571\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 205 with loss: 0.3413905203342438 and average loss:0.3594045341014862\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 210 with loss: 0.46049055457115173 and average loss:0.35943546891212463\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 215 with loss: 0.44975268840789795 and average loss:0.3587472140789032\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 220 with loss: 0.17845465242862701 and average loss:0.3573652505874634\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 225 with loss: 0.4028506577014923 and average loss:0.358766108751297\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 230 with loss: 0.27547454833984375 and average loss:0.3583848774433136\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 235 with loss: 0.43552812933921814 and average loss:0.3568557798862457\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 240 with loss: 0.38071680068969727 and average loss:0.35660451650619507\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 245 with loss: 0.2715405523777008 and average loss:0.355417937040329\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 12, batch: 250 with loss: 0.6725530624389648 and average loss:0.3576410412788391\n",
            "Epoch 13: LR=0.000120\n",
            "epoch: 13, batch: 0 with loss: 0.3149907886981964 and average loss:0.3149907886981964\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 5 with loss: 0.3253362476825714 and average loss:0.3367794156074524\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 10 with loss: 0.30517175793647766 and average loss:0.35000085830688477\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 15 with loss: 0.2877388894557953 and average loss:0.3426644504070282\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 20 with loss: 0.41520318388938904 and average loss:0.34979039430618286\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 25 with loss: 0.3705126643180847 and average loss:0.3466680645942688\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 30 with loss: 0.4952383041381836 and average loss:0.3499109447002411\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 35 with loss: 0.49004092812538147 and average loss:0.34965530037879944\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 40 with loss: 0.25887542963027954 and average loss:0.3418312966823578\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 45 with loss: 0.38443636894226074 and average loss:0.3382939100265503\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 50 with loss: 0.30143263936042786 and average loss:0.33725473284721375\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 55 with loss: 0.5492954850196838 and average loss:0.34332528710365295\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 60 with loss: 0.167463019490242 and average loss:0.34572896361351013\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 65 with loss: 0.3560792803764343 and average loss:0.34271737933158875\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 70 with loss: 0.41158774495124817 and average loss:0.34479981660842896\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 75 with loss: 0.47586584091186523 and average loss:0.34674468636512756\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 80 with loss: 0.39791592955589294 and average loss:0.3484330475330353\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 85 with loss: 0.3403053283691406 and average loss:0.3474659025669098\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 90 with loss: 0.2254815697669983 and average loss:0.3480769693851471\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 95 with loss: 0.4422953128814697 and average loss:0.34841036796569824\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 100 with loss: 0.295335978269577 and average loss:0.3487362861633301\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 105 with loss: 0.09698742628097534 and average loss:0.3447948694229126\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 110 with loss: 0.43789729475975037 and average loss:0.3462420403957367\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 115 with loss: 0.28592732548713684 and average loss:0.35138681530952454\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 120 with loss: 0.2992671728134155 and average loss:0.3501143753528595\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 125 with loss: 0.37167873978614807 and average loss:0.3521352708339691\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 130 with loss: 0.37312471866607666 and average loss:0.3517162501811981\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 135 with loss: 0.22574672102928162 and average loss:0.3505113124847412\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 140 with loss: 0.38362815976142883 and average loss:0.35060518980026245\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 145 with loss: 0.3226816654205322 and average loss:0.3511650860309601\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 150 with loss: 0.336820513010025 and average loss:0.3490898311138153\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 155 with loss: 0.23294126987457275 and average loss:0.3476564586162567\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 160 with loss: 0.35679203271865845 and average loss:0.3475290536880493\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 165 with loss: 0.23622369766235352 and average loss:0.34581851959228516\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 170 with loss: 0.220428004860878 and average loss:0.34735262393951416\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 175 with loss: 0.4143907427787781 and average loss:0.3467002809047699\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 180 with loss: 0.37181922793388367 and average loss:0.3476507067680359\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 185 with loss: 0.5753095746040344 and average loss:0.3517090976238251\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 190 with loss: 0.30383729934692383 and average loss:0.35205817222595215\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 195 with loss: 0.348619669675827 and average loss:0.3524155914783478\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 200 with loss: 0.4088878631591797 and average loss:0.3531150817871094\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 205 with loss: 0.3243385851383209 and average loss:0.35197755694389343\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 210 with loss: 0.4489001929759979 and average loss:0.3527553379535675\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 215 with loss: 0.3748667538166046 and average loss:0.3523998260498047\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 220 with loss: 0.3169395923614502 and average loss:0.35200706124305725\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 225 with loss: 0.3971259593963623 and average loss:0.3529715836048126\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 230 with loss: 0.3211732804775238 and average loss:0.3523760139942169\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 235 with loss: 0.3079761266708374 and average loss:0.3512340486049652\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 240 with loss: 0.3513486087322235 and average loss:0.3516002297401428\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 245 with loss: 0.3352469205856323 and average loss:0.35329198837280273\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 13, batch: 250 with loss: 0.1321462094783783 and average loss:0.35135388374328613\n",
            "Epoch 14: LR=0.000120\n",
            "epoch: 14, batch: 0 with loss: 0.2935764789581299 and average loss:0.2935764789581299\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 5 with loss: 0.4773513674736023 and average loss:0.4031260907649994\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 10 with loss: 0.26492103934288025 and average loss:0.35641321539878845\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 15 with loss: 0.4782431423664093 and average loss:0.36168453097343445\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 20 with loss: 0.30048784613609314 and average loss:0.3506922125816345\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 25 with loss: 0.29821091890335083 and average loss:0.3442731499671936\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 30 with loss: 0.24745440483093262 and average loss:0.35122767090797424\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 35 with loss: 0.2818150818347931 and average loss:0.34145650267601013\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 40 with loss: 0.3671760559082031 and average loss:0.339871346950531\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 45 with loss: 0.19846276938915253 and average loss:0.33693748712539673\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 50 with loss: 0.20847612619400024 and average loss:0.3354725241661072\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 55 with loss: 0.4094682037830353 and average loss:0.33363187313079834\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 60 with loss: 0.47379612922668457 and average loss:0.33481302857398987\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 65 with loss: 0.35249441862106323 and average loss:0.33079084753990173\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 70 with loss: 0.5047145485877991 and average loss:0.33362963795661926\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 75 with loss: 0.3079875111579895 and average loss:0.3306547701358795\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 80 with loss: 0.4335527718067169 and average loss:0.3319477438926697\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 85 with loss: 0.36196696758270264 and average loss:0.3290155231952667\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 90 with loss: 0.3706475794315338 and average loss:0.32820650935173035\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 95 with loss: 0.4761533737182617 and average loss:0.33021101355552673\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 100 with loss: 0.2920484244823456 and average loss:0.33057138323783875\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 105 with loss: 0.531159520149231 and average loss:0.333972692489624\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 110 with loss: 0.3141535520553589 and average loss:0.33295953273773193\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 115 with loss: 0.31721851229667664 and average loss:0.33492180705070496\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 120 with loss: 0.27644380927085876 and average loss:0.33452755212783813\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 125 with loss: 0.2093331664800644 and average loss:0.33469998836517334\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 130 with loss: 0.38963833451271057 and average loss:0.3349323272705078\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 135 with loss: 0.3426542580127716 and average loss:0.3365769684314728\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 140 with loss: 0.257254034280777 and average loss:0.3368283212184906\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 145 with loss: 0.30834782123565674 and average loss:0.33724403381347656\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 150 with loss: 0.20801132917404175 and average loss:0.3361152112483978\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 155 with loss: 0.2812190651893616 and average loss:0.3386659026145935\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 160 with loss: 0.44286006689071655 and average loss:0.34080156683921814\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 165 with loss: 0.44623807072639465 and average loss:0.341707706451416\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 170 with loss: 0.24484437704086304 and average loss:0.34018632769584656\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 175 with loss: 0.37855419516563416 and average loss:0.3395833969116211\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 180 with loss: 0.4654097557067871 and average loss:0.3391847610473633\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 185 with loss: 0.3837716281414032 and average loss:0.3383142054080963\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 190 with loss: 0.3154241144657135 and average loss:0.3401506841182709\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 195 with loss: 0.5045226812362671 and average loss:0.3409365713596344\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 200 with loss: 0.4526223838329315 and average loss:0.3408428430557251\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 205 with loss: 0.42600199580192566 and average loss:0.3419182300567627\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 210 with loss: 0.3798201084136963 and average loss:0.34221234917640686\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 215 with loss: 0.26750603318214417 and average loss:0.3417738974094391\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 220 with loss: 0.16978666186332703 and average loss:0.3414382338523865\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 225 with loss: 0.3384215533733368 and average loss:0.3423425555229187\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 230 with loss: 0.5598887205123901 and average loss:0.34264394640922546\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 235 with loss: 0.5156673192977905 and average loss:0.34291359782218933\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 240 with loss: 0.24931401014328003 and average loss:0.3422127068042755\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 245 with loss: 0.26040348410606384 and average loss:0.3427409827709198\n",
            "Epoch 15: LR=0.000120\n",
            "epoch: 14, batch: 250 with loss: 0.19799192249774933 and average loss:0.34197530150413513\n",
            "Epoch 15: LR=0.000120\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "  total_loss = 0\n",
        "  model.train()\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "        total_loss += loss\n",
        "        average_loss = total_loss / (batch_idx + 1)\n",
        "        if batch_idx % 5 == 0:\n",
        "          print(f'epoch: {epoch}, batch: {batch_idx} with loss: {loss} and average loss:{average_loss}')\n",
        "          print(f\"Epoch {epoch+1}: LR={optimizer.param_groups[0]['lr']:.6f}\")\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReH9xISzPy-C",
        "outputId": "e900dbd5-0d68-409c-f367-838c29360dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 7016 / 8005 with accuracy 87.65\n",
            "Got 1695 / 2023 with accuracy 83.79\n"
          ]
        }
      ],
      "source": [
        "def check_accuracy(loader,model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()   #evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)   #max(1) finds the highest value and predictions to store the indices of the value (like from 0-9 then just need the highest value to find the number, the value itself is not needed)\n",
        "            num_correct += (predictions==y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
        "        model.train()\n",
        "\n",
        "check_accuracy(train_loader, model)\n",
        "check_accuracy(test_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = 'cnn.pt'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "GbtGgqMzYrbf"
      },
      "execution_count": 196,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}