{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tom984-vn/Dog_cat_classification/blob/main/dog_cat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_0rRqF8Py98"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as func\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLdDtjBAPy9-",
        "outputId": "e8e48f8a-2c27-4b10-c4ec-3b68c7875b80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/tongpython/cat-and-dog/versions/1\n"
          ]
        }
      ],
      "source": [
        "path = kagglehub.dataset_download(\"tongpython/cat-and-dog\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtdhMwbnPy9_"
      },
      "source": [
        "## Import library\n",
        "    - Pytorch\n",
        "    - Pytorch model (pytorch.nn)\n",
        "        + Import function (ReLU, tanh, ...)\n",
        "    - Optimization\n",
        "    - DataLoader\n",
        "## Create the model\n",
        "    - Init\n",
        "        ! Super(NN) to properly initialize\n",
        "        + Create conv layer (3 channels with 3x3 kernels)\n",
        "        + Max pooling layer inbetween\n",
        "        + create hidden layer (linear)\n",
        "    - Forward\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdlJ0vK5Py-A"
      },
      "outputs": [],
      "source": [
        "num_classes = 2\n",
        "batch_size = 32\n",
        "learning_rate = 0.0002\n",
        "num_epochs = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCRZjWXoPy-B"
      },
      "outputs": [],
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(NN, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "        self.hlayer1 = nn.Linear(input_size, 128)\n",
        "        self.hlayer2 = nn.Linear(128, 128)\n",
        "        self.hlayer3 = nn.Linear(128, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = func.relu(self.hlayer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = func.relu(self.hlayer2(x))\n",
        "        return x\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, in_channel = 3, num_classes = 2):\n",
        "      super(CNN, self).__init__()\n",
        "      self.relu = nn.ReLU()\n",
        "      self.linear_dropout = nn.Dropout(p=0.3)\n",
        "      self.convd_dropout = nn.Dropout2d(p=0.2)\n",
        "      self.conv = nn.Conv2d(in_channels = in_channel, out_channels = 64, kernel_size=3,stride=1,padding=1)  #convolutional layer 64 layers with kernel 3x3\n",
        "      self.bn1 = nn.BatchNorm2d(64)\n",
        "      self.bn2 = nn.BatchNorm2d(128)\n",
        "      self.bn3 = nn.BatchNorm2d(256)\n",
        "      self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  #pooling layer help reduce size from 128x128 -> 64x64\n",
        "      self.conv2 = nn.Conv2d(in_channels= 64, out_channels= 128, kernel_size=3, stride=1,padding=1)\n",
        "      self.conv3 = nn.Conv2d(in_channels= 128, out_channels= 256, kernel_size=3, stride=1,padding=1)\n",
        "      self.fc1 = nn.Linear(256 * 16 * 16 , 256)  # Adjusted input size for Linear layer\n",
        "      self.fc2 = nn.Linear(256, num_classes)  # Added another fully connected layer\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(self.relu(self.bn1(self.conv(x))))\n",
        "    x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
        "    x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
        "    x = self.convd_dropout(x)\n",
        "    x = x.reshape(x.shape[0],-1)\n",
        "    x = func.relu(self.fc1(x))\n",
        "    x = self.linear_dropout(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "y3HaFoVd6cnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYEdbv_sPy-B"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),  # Augment data\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "transform_normal = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "# Load dataset\n",
        "train_dataset = datasets.ImageFolder(root=os.path.join(path,\"training_set\",\"training_set\"), transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(path,\"test_set\",\"test_set\"), transform=transform_normal)\n",
        "\n",
        "train_size = int(0.8 * len(train_dataset))  # 80% training\n",
        "val_size = len(train_dataset) - train_size  # 20% validation\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():  # No gradient computation\n",
        "        for data, targets in val_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            scores = model(data)\n",
        "            loss = criterion(scores, targets)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    return total_val_loss / len(val_loader)  # Average validation loss"
      ],
      "metadata": {
        "id": "x0ZMrVeRm7An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGWEpcZ8Py-C"
      },
      "outputs": [],
      "source": [
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr= learning_rate,betas=(0.9,0.999), weight_decay= 1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQtH3nBVPy-C",
        "outputId": "b32dd697-4b4d-4bb9-926e-fe72cb2c7898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, batch: 0 with loss: 0.7431042194366455 and average loss:0.7431042194366455\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 5 with loss: 1.5798603296279907 and average loss:4.117805480957031\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 10 with loss: 0.8310168385505676 and average loss:3.0296642780303955\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 15 with loss: 1.0945453643798828 and average loss:2.523366928100586\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 20 with loss: 0.8935760259628296 and average loss:2.157184362411499\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 25 with loss: 0.7769662737846375 and average loss:1.9009045362472534\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 30 with loss: 0.650777280330658 and average loss:1.7082089185714722\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 35 with loss: 0.6848944425582886 and average loss:1.569071650505066\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 40 with loss: 0.6448819637298584 and average loss:1.4579890966415405\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 45 with loss: 0.7297124862670898 and average loss:1.3724942207336426\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 50 with loss: 0.6660488247871399 and average loss:1.3025246858596802\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 55 with loss: 0.67125004529953 and average loss:1.2471568584442139\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 60 with loss: 0.7337332367897034 and average loss:1.202088713645935\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 65 with loss: 0.6593564748764038 and average loss:1.162368655204773\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 70 with loss: 0.5940572023391724 and average loss:1.1258271932601929\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 75 with loss: 0.7137799859046936 and average loss:1.0936843156814575\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 80 with loss: 0.6584619283676147 and average loss:1.065205454826355\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 85 with loss: 0.5538754463195801 and average loss:1.0380640029907227\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 90 with loss: 0.724714457988739 and average loss:1.0164111852645874\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 95 with loss: 0.6346309185028076 and average loss:0.9985172152519226\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 100 with loss: 0.6568896770477295 and average loss:0.982384443283081\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 105 with loss: 0.6994810104370117 and average loss:0.9668258428573608\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 110 with loss: 0.6503322720527649 and average loss:0.9527957439422607\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 115 with loss: 0.5995386242866516 and average loss:0.9389193058013916\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 120 with loss: 0.6823757290840149 and average loss:0.9266055822372437\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 125 with loss: 0.6702377200126648 and average loss:0.9150788187980652\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 130 with loss: 0.5582920908927917 and average loss:0.9031578302383423\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 135 with loss: 0.5804478526115417 and average loss:0.8931452631950378\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 140 with loss: 0.5648468136787415 and average loss:0.8834028840065002\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 145 with loss: 0.6016207337379456 and average loss:0.8732672929763794\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 150 with loss: 0.6093975901603699 and average loss:0.8645501732826233\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 155 with loss: 0.615119993686676 and average loss:0.8560600280761719\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 160 with loss: 0.6192315220832825 and average loss:0.8469192981719971\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 165 with loss: 0.6562793254852295 and average loss:0.839939296245575\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 170 with loss: 0.7007719874382019 and average loss:0.8335940837860107\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 175 with loss: 0.7101934552192688 and average loss:0.8284033536911011\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 180 with loss: 0.6653110980987549 and average loss:0.8228026032447815\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 185 with loss: 0.5379555225372314 and average loss:0.81662917137146\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 190 with loss: 0.6719589829444885 and average loss:0.8112437725067139\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 195 with loss: 0.6340157985687256 and average loss:0.8065382242202759\n",
            "Epoch 1: LR=0.000200\n",
            "epoch: 0, batch: 200 with loss: 0.7644474506378174 and average loss:0.8019426465034485\n",
            "Epoch 1: LR=0.000200\n",
            "Validation loss: 0.5933136700415144\n",
            "epoch: 1, batch: 0 with loss: 0.5932778716087341 and average loss:0.5932778716087341\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 5 with loss: 0.6474051475524902 and average loss:0.5880616903305054\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 10 with loss: 0.606261670589447 and average loss:0.6019883751869202\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 15 with loss: 0.5822972655296326 and average loss:0.5919163823127747\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 20 with loss: 0.6796713471412659 and average loss:0.6050389409065247\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 25 with loss: 0.7652570605278015 and average loss:0.6105130314826965\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 30 with loss: 0.5152963995933533 and average loss:0.6023016571998596\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 35 with loss: 0.6311374306678772 and average loss:0.598051905632019\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 40 with loss: 0.7004650235176086 and average loss:0.5987661480903625\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 45 with loss: 0.6729328632354736 and average loss:0.6006506085395813\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 50 with loss: 0.5911982655525208 and average loss:0.5991947054862976\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 55 with loss: 0.7522821426391602 and average loss:0.6025276184082031\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 60 with loss: 0.5468343496322632 and average loss:0.5995784997940063\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 65 with loss: 0.6764031648635864 and average loss:0.6008785367012024\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 70 with loss: 0.6084518432617188 and average loss:0.6014520525932312\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 75 with loss: 0.5020062327384949 and average loss:0.597649872303009\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 80 with loss: 0.5422562956809998 and average loss:0.5990732312202454\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 85 with loss: 0.5826444625854492 and average loss:0.599886953830719\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 90 with loss: 0.5356155037879944 and average loss:0.60230553150177\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 95 with loss: 0.7228249311447144 and average loss:0.6038401126861572\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 100 with loss: 0.5256845355033875 and average loss:0.6005156636238098\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 105 with loss: 0.6631982326507568 and average loss:0.5996132493019104\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 110 with loss: 0.638529360294342 and average loss:0.6005000472068787\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 115 with loss: 0.6434659361839294 and average loss:0.6017661094665527\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 120 with loss: 0.5469847917556763 and average loss:0.599372923374176\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 125 with loss: 0.5965232253074646 and average loss:0.6002399921417236\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 130 with loss: 0.6769066452980042 and average loss:0.6002587080001831\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 135 with loss: 0.5543820858001709 and average loss:0.5995861887931824\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 140 with loss: 0.5441332459449768 and average loss:0.5980746150016785\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 145 with loss: 0.6613080501556396 and average loss:0.5988612174987793\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 150 with loss: 0.607476532459259 and average loss:0.5985293984413147\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 155 with loss: 0.4917024075984955 and average loss:0.5960091948509216\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 160 with loss: 0.3932265639305115 and average loss:0.5941335558891296\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 165 with loss: 0.455557644367218 and average loss:0.5931524038314819\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 170 with loss: 0.6463356614112854 and average loss:0.5943471789360046\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 175 with loss: 0.567679762840271 and average loss:0.59348464012146\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 180 with loss: 0.4952210783958435 and average loss:0.5926012992858887\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 185 with loss: 0.5444878339767456 and average loss:0.5917856693267822\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 190 with loss: 0.5780017375946045 and average loss:0.5911963582038879\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 195 with loss: 0.5849351286888123 and average loss:0.5901347398757935\n",
            "Epoch 2: LR=0.000200\n",
            "epoch: 1, batch: 200 with loss: 0.9605417847633362 and average loss:0.5932473540306091\n",
            "Epoch 2: LR=0.000200\n",
            "Validation loss: 0.5415052824160632\n",
            "epoch: 2, batch: 0 with loss: 0.6394275426864624 and average loss:0.6394275426864624\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 5 with loss: 0.7669957876205444 and average loss:0.6813836097717285\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 10 with loss: 0.5708686113357544 and average loss:0.6581503748893738\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 15 with loss: 0.533169150352478 and average loss:0.6387740969657898\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 20 with loss: 0.604569137096405 and average loss:0.626349925994873\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 25 with loss: 0.5694301724433899 and average loss:0.6105290651321411\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 30 with loss: 0.6018728017807007 and average loss:0.6099799275398254\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 35 with loss: 0.4899466931819916 and average loss:0.5978493094444275\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 40 with loss: 0.47674673795700073 and average loss:0.5931381583213806\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 45 with loss: 0.6277766227722168 and average loss:0.5984904766082764\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 50 with loss: 0.6527965664863586 and average loss:0.6027376055717468\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 55 with loss: 0.5743550658226013 and average loss:0.6039592623710632\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 60 with loss: 0.6112784147262573 and average loss:0.6010643243789673\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 65 with loss: 0.5631974935531616 and average loss:0.597041130065918\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 70 with loss: 0.49030449986457825 and average loss:0.593706488609314\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 75 with loss: 0.5793558955192566 and average loss:0.5952256321907043\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 80 with loss: 0.7862157821655273 and average loss:0.5954751372337341\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 85 with loss: 0.7587497234344482 and average loss:0.5957742929458618\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 90 with loss: 0.5810456275939941 and average loss:0.5930349230766296\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 95 with loss: 0.4777487814426422 and average loss:0.5927556753158569\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 100 with loss: 0.49246466159820557 and average loss:0.5899320244789124\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 105 with loss: 0.47391578555107117 and average loss:0.589180052280426\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 110 with loss: 0.4296550452709198 and average loss:0.5840283036231995\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 115 with loss: 0.5427463054656982 and average loss:0.5859690308570862\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 120 with loss: 0.5216546654701233 and average loss:0.5823639035224915\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 125 with loss: 0.564276933670044 and average loss:0.582972526550293\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 130 with loss: 0.4955575466156006 and average loss:0.5791707634925842\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 135 with loss: 0.5773624777793884 and average loss:0.5787399411201477\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 140 with loss: 0.49408096075057983 and average loss:0.5766933560371399\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 145 with loss: 0.39531952142715454 and average loss:0.5749632120132446\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 150 with loss: 0.54885333776474 and average loss:0.5713516473770142\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 155 with loss: 0.5162668824195862 and average loss:0.5690160989761353\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 160 with loss: 0.6353453397750854 and average loss:0.572084367275238\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 165 with loss: 0.5461852550506592 and average loss:0.5706769824028015\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 170 with loss: 0.4893626868724823 and average loss:0.569871723651886\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 175 with loss: 0.5507912635803223 and average loss:0.5697818994522095\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 180 with loss: 0.7083503603935242 and average loss:0.5704671144485474\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 185 with loss: 0.5501585006713867 and average loss:0.5699405074119568\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 190 with loss: 0.5035839676856995 and average loss:0.5695061683654785\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 195 with loss: 0.4885999262332916 and average loss:0.5685622692108154\n",
            "Epoch 3: LR=0.000200\n",
            "epoch: 2, batch: 200 with loss: 0.5626584887504578 and average loss:0.5681694149971008\n",
            "Epoch 3: LR=0.000200\n",
            "Validation loss: 0.5242814030133042\n",
            "epoch: 3, batch: 0 with loss: 0.6642640233039856 and average loss:0.6642640233039856\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 5 with loss: 0.3932192027568817 and average loss:0.5692103505134583\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 10 with loss: 0.735763669013977 and average loss:0.5564972162246704\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 15 with loss: 0.5833423733711243 and average loss:0.5556204915046692\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 20 with loss: 0.6051461100578308 and average loss:0.537814199924469\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 25 with loss: 0.5458790063858032 and average loss:0.5332891345024109\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 30 with loss: 0.46500828862190247 and average loss:0.5304226875305176\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 35 with loss: 0.4815545976161957 and average loss:0.5340611338615417\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 40 with loss: 0.5290710926055908 and average loss:0.544200599193573\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 45 with loss: 0.512545108795166 and average loss:0.5355573296546936\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 50 with loss: 0.5716161131858826 and average loss:0.5352014303207397\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 55 with loss: 0.5258819460868835 and average loss:0.5343798398971558\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 60 with loss: 0.6076914668083191 and average loss:0.5323084592819214\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 65 with loss: 0.43710294365882874 and average loss:0.5322558879852295\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 70 with loss: 0.6224726438522339 and average loss:0.5347840785980225\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 75 with loss: 0.6525920033454895 and average loss:0.53477543592453\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 80 with loss: 0.595930814743042 and average loss:0.5371889472007751\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 85 with loss: 0.4778854548931122 and average loss:0.5348222255706787\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 90 with loss: 0.54458087682724 and average loss:0.5343465805053711\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 95 with loss: 0.4073803126811981 and average loss:0.535286545753479\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 100 with loss: 0.4987039864063263 and average loss:0.5365158915519714\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 105 with loss: 0.4119839668273926 and average loss:0.5370360612869263\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 110 with loss: 0.5255225896835327 and average loss:0.5365573763847351\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 115 with loss: 0.41517212986946106 and average loss:0.535322368144989\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 120 with loss: 0.4665476083755493 and average loss:0.5344493985176086\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 125 with loss: 0.491319477558136 and average loss:0.5354199409484863\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 130 with loss: 0.44362014532089233 and average loss:0.5369693636894226\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 135 with loss: 0.5337138772010803 and average loss:0.5377860069274902\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 140 with loss: 0.4493885338306427 and average loss:0.5362212061882019\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 145 with loss: 0.5030569434165955 and average loss:0.5364298820495605\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 150 with loss: 0.41091302037239075 and average loss:0.5338538289070129\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 155 with loss: 0.41191527247428894 and average loss:0.531731367111206\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 160 with loss: 0.964695930480957 and average loss:0.5319182872772217\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 165 with loss: 0.3283424973487854 and average loss:0.5317542552947998\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 170 with loss: 0.42922690510749817 and average loss:0.5298023819923401\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 175 with loss: 0.48907241225242615 and average loss:0.5293998718261719\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 180 with loss: 0.43715205788612366 and average loss:0.5299794673919678\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 185 with loss: 0.3967112898826599 and average loss:0.5282822251319885\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 190 with loss: 0.5499095916748047 and average loss:0.526089072227478\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 195 with loss: 0.43865543603897095 and average loss:0.525373101234436\n",
            "Epoch 4: LR=0.000200\n",
            "epoch: 3, batch: 200 with loss: 1.0346766710281372 and average loss:0.5282248854637146\n",
            "Epoch 4: LR=0.000200\n",
            "Validation loss: 0.5321376656784731\n",
            "epoch: 4, batch: 0 with loss: 0.5306000113487244 and average loss:0.5306000113487244\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 5 with loss: 0.5744034647941589 and average loss:0.5216134786605835\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 10 with loss: 0.4850337505340576 and average loss:0.5444644093513489\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 15 with loss: 0.4691835343837738 and average loss:0.5375847220420837\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 20 with loss: 0.5234792232513428 and average loss:0.5421478748321533\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 25 with loss: 0.519091784954071 and average loss:0.5303244590759277\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 30 with loss: 0.6255797743797302 and average loss:0.5351881384849548\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 35 with loss: 0.5334442257881165 and average loss:0.5328270792961121\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 40 with loss: 0.46257734298706055 and average loss:0.5359194874763489\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 45 with loss: 0.6548274755477905 and average loss:0.54075688123703\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 50 with loss: 0.43583178520202637 and average loss:0.5429679155349731\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 55 with loss: 0.5369997620582581 and average loss:0.540389358997345\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 60 with loss: 0.4510435461997986 and average loss:0.5416926741600037\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 65 with loss: 0.4511966109275818 and average loss:0.5372717380523682\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 70 with loss: 0.48286399245262146 and average loss:0.5329594612121582\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 75 with loss: 0.5078790187835693 and average loss:0.5284545421600342\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 80 with loss: 0.5024652481079102 and average loss:0.532050609588623\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 85 with loss: 0.4955280125141144 and average loss:0.5290518403053284\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 90 with loss: 0.5502946376800537 and average loss:0.5286345481872559\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 95 with loss: 0.4877620041370392 and average loss:0.5278738737106323\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 100 with loss: 0.4791041910648346 and average loss:0.527993381023407\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 105 with loss: 0.46610766649246216 and average loss:0.5276370644569397\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 110 with loss: 0.5118457674980164 and average loss:0.5270333886146545\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 115 with loss: 0.5596082210540771 and average loss:0.5263288617134094\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 120 with loss: 0.4218927323818207 and average loss:0.5219236612319946\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 125 with loss: 0.6833663582801819 and average loss:0.5230637192726135\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 130 with loss: 0.6199387907981873 and average loss:0.5221431255340576\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 135 with loss: 0.5462392568588257 and average loss:0.5213621854782104\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 140 with loss: 0.5468941926956177 and average loss:0.5211045742034912\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 145 with loss: 0.4530473053455353 and average loss:0.520445704460144\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 150 with loss: 0.4256313443183899 and average loss:0.5215485692024231\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 155 with loss: 0.4238319396972656 and average loss:0.5217925906181335\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 160 with loss: 0.4449211657047272 and average loss:0.5191120505332947\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 165 with loss: 0.44882458448410034 and average loss:0.5194554924964905\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 170 with loss: 0.4983862638473511 and average loss:0.5195778012275696\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 175 with loss: 0.5001614093780518 and average loss:0.5194862484931946\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 180 with loss: 0.5547955632209778 and average loss:0.5192261934280396\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 185 with loss: 0.577198326587677 and average loss:0.5189462900161743\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 190 with loss: 0.45438385009765625 and average loss:0.5185126662254333\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 195 with loss: 0.5684918761253357 and average loss:0.5173463225364685\n",
            "Epoch 5: LR=0.000200\n",
            "epoch: 4, batch: 200 with loss: 0.3033882677555084 and average loss:0.5143597722053528\n",
            "Epoch 5: LR=0.000200\n",
            "Validation loss: 0.4933606856593899\n",
            "epoch: 5, batch: 0 with loss: 0.5105167031288147 and average loss:0.5105167031288147\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 5 with loss: 0.457645446062088 and average loss:0.5546755790710449\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 10 with loss: 0.36815670132637024 and average loss:0.49550843238830566\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 15 with loss: 0.45135700702667236 and average loss:0.5042592883110046\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 20 with loss: 0.38162335753440857 and average loss:0.49801358580589294\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 25 with loss: 0.4466269016265869 and average loss:0.4837382435798645\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 30 with loss: 0.42441225051879883 and average loss:0.48508137464523315\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 35 with loss: 0.5462501645088196 and average loss:0.4902944564819336\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 40 with loss: 0.45873674750328064 and average loss:0.4864064157009125\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 45 with loss: 0.5293717980384827 and average loss:0.48611384630203247\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 50 with loss: 0.40530407428741455 and average loss:0.4857168197631836\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 55 with loss: 0.3991423547267914 and average loss:0.4878772795200348\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 60 with loss: 0.4834883213043213 and average loss:0.4874173700809479\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 65 with loss: 0.3190223276615143 and average loss:0.4833846986293793\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 70 with loss: 0.44780054688453674 and average loss:0.4827382564544678\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 75 with loss: 0.6557681560516357 and average loss:0.4852142333984375\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 80 with loss: 0.6671463251113892 and average loss:0.4871499836444855\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 85 with loss: 0.5300000309944153 and average loss:0.4907585084438324\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 90 with loss: 0.43833333253860474 and average loss:0.4888323247432709\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 95 with loss: 0.4312695860862732 and average loss:0.4860171675682068\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 100 with loss: 0.608193576335907 and average loss:0.4885525405406952\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 105 with loss: 0.5761450529098511 and average loss:0.486924946308136\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 110 with loss: 0.591688334941864 and average loss:0.488631933927536\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 115 with loss: 0.36377450823783875 and average loss:0.4877929091453552\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 120 with loss: 0.48815903067588806 and average loss:0.48913899064064026\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 125 with loss: 0.4102928042411804 and average loss:0.49193012714385986\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 130 with loss: 0.42213675379753113 and average loss:0.4896724820137024\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 135 with loss: 0.6295088529586792 and average loss:0.4900420010089874\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 140 with loss: 0.6040434241294861 and average loss:0.4920647144317627\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 145 with loss: 0.4478214681148529 and average loss:0.49118465185165405\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 150 with loss: 0.3524271547794342 and average loss:0.4904736578464508\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 155 with loss: 0.43250036239624023 and average loss:0.49049660563468933\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 160 with loss: 0.4863283634185791 and average loss:0.49020683765411377\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 165 with loss: 0.3352402448654175 and average loss:0.4889477789402008\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 170 with loss: 0.519191324710846 and average loss:0.48672208189964294\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 175 with loss: 0.25818178057670593 and average loss:0.4837440252304077\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 180 with loss: 0.46984100341796875 and average loss:0.4820396304130554\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 185 with loss: 0.7008917331695557 and average loss:0.4845044016838074\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 190 with loss: 0.3966459333896637 and average loss:0.4838155508041382\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 195 with loss: 0.324442982673645 and average loss:0.48201945424079895\n",
            "Epoch 6: LR=0.000200\n",
            "epoch: 5, batch: 200 with loss: 0.5819717645645142 and average loss:0.48273080587387085\n",
            "Epoch 6: LR=0.000200\n",
            "Validation loss: 0.4948590894540151\n",
            "epoch: 6, batch: 0 with loss: 0.3566407859325409 and average loss:0.3566407859325409\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 5 with loss: 0.6240761876106262 and average loss:0.4308569133281708\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 10 with loss: 0.629424512386322 and average loss:0.4572080373764038\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 15 with loss: 0.3483298420906067 and average loss:0.45191237330436707\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 20 with loss: 0.3520921468734741 and average loss:0.45139840245246887\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 25 with loss: 0.4361143708229065 and average loss:0.44741004705429077\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 30 with loss: 0.8668810129165649 and average loss:0.4541497528553009\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 35 with loss: 0.4459746479988098 and average loss:0.44866907596588135\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 40 with loss: 0.3438582420349121 and average loss:0.45509758591651917\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 45 with loss: 0.3987530767917633 and average loss:0.4486219584941864\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 50 with loss: 0.621182918548584 and average loss:0.4573219418525696\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 55 with loss: 0.4119723439216614 and average loss:0.4566524028778076\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 60 with loss: 0.4332674443721771 and average loss:0.45511794090270996\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 65 with loss: 0.3610987663269043 and average loss:0.45284005999565125\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 70 with loss: 0.28778743743896484 and average loss:0.4495355188846588\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 75 with loss: 0.4543892443180084 and average loss:0.44700950384140015\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 80 with loss: 0.46071726083755493 and average loss:0.448569118976593\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 85 with loss: 0.44596555829048157 and average loss:0.4465116858482361\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 90 with loss: 0.42871105670928955 and average loss:0.44622451066970825\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 95 with loss: 0.46397286653518677 and average loss:0.44812333583831787\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 100 with loss: 0.47434136271476746 and average loss:0.4490715265274048\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 105 with loss: 0.4773654639720917 and average loss:0.44772061705589294\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 110 with loss: 0.4665798544883728 and average loss:0.449754923582077\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 115 with loss: 0.5787467360496521 and average loss:0.45102736353874207\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 120 with loss: 0.3660578727722168 and average loss:0.45191091299057007\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 125 with loss: 0.39552226662635803 and average loss:0.4520530104637146\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 130 with loss: 0.4300341010093689 and average loss:0.4511570334434509\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 135 with loss: 0.40688562393188477 and average loss:0.45068997144699097\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 140 with loss: 0.46941232681274414 and average loss:0.4514802396297455\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 145 with loss: 0.48252975940704346 and average loss:0.4528787136077881\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 150 with loss: 0.5831676721572876 and average loss:0.4552158713340759\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 155 with loss: 0.36273980140686035 and average loss:0.4559384882450104\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 160 with loss: 0.5796284079551697 and average loss:0.45879629254341125\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 165 with loss: 0.4779760241508484 and average loss:0.4587375521659851\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 170 with loss: 0.5033228397369385 and average loss:0.4613538086414337\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 175 with loss: 0.34865665435791016 and average loss:0.4613714814186096\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 180 with loss: 0.44468286633491516 and average loss:0.460884153842926\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 185 with loss: 0.6035933494567871 and average loss:0.46160081028938293\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 190 with loss: 0.5061785578727722 and average loss:0.46041741967201233\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 195 with loss: 0.4655455946922302 and average loss:0.46282801032066345\n",
            "Epoch 7: LR=0.000200\n",
            "epoch: 6, batch: 200 with loss: 0.17786568403244019 and average loss:0.46207404136657715\n",
            "Epoch 7: LR=0.000200\n",
            "Validation loss: 0.46754294458557577\n",
            "epoch: 7, batch: 0 with loss: 0.4555085003376007 and average loss:0.4555085003376007\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 5 with loss: 0.4795989990234375 and average loss:0.42812833189964294\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 10 with loss: 0.5338942408561707 and average loss:0.4260006844997406\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 15 with loss: 0.49847501516342163 and average loss:0.433788001537323\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 20 with loss: 0.32898104190826416 and average loss:0.44023191928863525\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 25 with loss: 0.4961172938346863 and average loss:0.42870381474494934\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 30 with loss: 0.5758512020111084 and average loss:0.43297135829925537\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 35 with loss: 0.38488534092903137 and average loss:0.4415506422519684\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 40 with loss: 0.31919020414352417 and average loss:0.4347895383834839\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 45 with loss: 0.38302576541900635 and average loss:0.4370412528514862\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 50 with loss: 0.4333474636077881 and average loss:0.43821588158607483\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 55 with loss: 0.3369300961494446 and average loss:0.44184911251068115\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 60 with loss: 0.24314762651920319 and average loss:0.43385741114616394\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 65 with loss: 0.5963332653045654 and average loss:0.4384136199951172\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 70 with loss: 0.633365273475647 and average loss:0.43608424067497253\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 75 with loss: 0.44117599725723267 and average loss:0.43801283836364746\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 80 with loss: 0.5503843426704407 and average loss:0.43878206610679626\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 85 with loss: 0.47654345631599426 and average loss:0.4414532780647278\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 90 with loss: 0.4316234886646271 and average loss:0.44006964564323425\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 95 with loss: 0.3670308589935303 and average loss:0.44026392698287964\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 100 with loss: 0.393315851688385 and average loss:0.438300222158432\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 105 with loss: 0.38949111104011536 and average loss:0.43473896384239197\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 110 with loss: 0.6650265455245972 and average loss:0.4353145956993103\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 115 with loss: 0.3340628147125244 and average loss:0.4342058300971985\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 120 with loss: 0.4771404266357422 and average loss:0.43667858839035034\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 125 with loss: 0.4223710000514984 and average loss:0.4362967014312744\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 130 with loss: 0.3336106538772583 and average loss:0.4339204728603363\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 135 with loss: 0.5386983752250671 and average loss:0.4371069073677063\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 140 with loss: 0.45812904834747314 and average loss:0.4370761215686798\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 145 with loss: 0.43132683634757996 and average loss:0.4384700655937195\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 150 with loss: 0.44110071659088135 and average loss:0.4373166859149933\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 155 with loss: 0.3795248568058014 and average loss:0.43754979968070984\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 160 with loss: 0.3509853780269623 and average loss:0.43731895089149475\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 165 with loss: 0.5974357724189758 and average loss:0.44010379910469055\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 170 with loss: 0.4314081072807312 and average loss:0.4398551285266876\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 175 with loss: 0.5958174467086792 and average loss:0.4408721327781677\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 180 with loss: 0.4151187539100647 and average loss:0.43902942538261414\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 185 with loss: 0.44356098771095276 and average loss:0.43983641266822815\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 190 with loss: 0.38392192125320435 and average loss:0.43982434272766113\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 195 with loss: 0.45485633611679077 and average loss:0.43997669219970703\n",
            "Epoch 8: LR=0.000200\n",
            "epoch: 7, batch: 200 with loss: 0.09337268769741058 and average loss:0.44024857878685\n",
            "Epoch 8: LR=0.000200\n",
            "Validation loss: 0.46010249327210817\n",
            "epoch: 8, batch: 0 with loss: 0.3374335467815399 and average loss:0.3374335467815399\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 5 with loss: 0.30528122186660767 and average loss:0.3853999376296997\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 10 with loss: 0.2084786742925644 and average loss:0.38602128624916077\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 15 with loss: 0.33490267395973206 and average loss:0.37626752257347107\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 20 with loss: 0.42667463421821594 and average loss:0.3845057785511017\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 25 with loss: 0.6358197331428528 and average loss:0.3960164785385132\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 30 with loss: 0.45581260323524475 and average loss:0.3973010778427124\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 35 with loss: 0.4645601809024811 and average loss:0.3938828408718109\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 40 with loss: 0.4666938781738281 and average loss:0.396681547164917\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 45 with loss: 0.5472184419631958 and average loss:0.40962862968444824\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 50 with loss: 0.43798452615737915 and average loss:0.40366193652153015\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 55 with loss: 0.4574572741985321 and average loss:0.4189888834953308\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 60 with loss: 0.3520253598690033 and average loss:0.4137289822101593\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 65 with loss: 0.4808696210384369 and average loss:0.4135412573814392\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 70 with loss: 0.3879093825817108 and average loss:0.4129514694213867\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 75 with loss: 0.406371533870697 and average loss:0.41539323329925537\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 80 with loss: 0.4839470088481903 and average loss:0.41628462076187134\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 85 with loss: 0.5531067848205566 and average loss:0.4164181649684906\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 90 with loss: 0.3235640823841095 and average loss:0.4125364124774933\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 95 with loss: 0.42534828186035156 and average loss:0.41657423973083496\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 100 with loss: 0.26490330696105957 and average loss:0.41672471165657043\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 105 with loss: 0.4760861098766327 and average loss:0.41856902837753296\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 110 with loss: 0.4175425171852112 and average loss:0.4214101731777191\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 115 with loss: 0.3521847128868103 and average loss:0.42057836055755615\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 120 with loss: 0.44562992453575134 and average loss:0.42360109090805054\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 125 with loss: 0.3763721287250519 and average loss:0.42538726329803467\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 130 with loss: 0.3233715295791626 and average loss:0.42662855982780457\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 135 with loss: 0.3199857175350189 and average loss:0.427255779504776\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 140 with loss: 0.3503819406032562 and average loss:0.42756956815719604\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 145 with loss: 0.45593535900115967 and average loss:0.4275793433189392\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 150 with loss: 0.42975619435310364 and average loss:0.4259626269340515\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 155 with loss: 0.4258641004562378 and average loss:0.4258076250553131\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 160 with loss: 0.304755836725235 and average loss:0.42584607005119324\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 165 with loss: 0.37591925263404846 and average loss:0.4257626533508301\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 170 with loss: 0.3263852894306183 and average loss:0.4250882863998413\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 175 with loss: 0.41980883479118347 and average loss:0.422806978225708\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 180 with loss: 0.42581191658973694 and average loss:0.4247921407222748\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 185 with loss: 0.3548126816749573 and average loss:0.42421141266822815\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 190 with loss: 0.21248653531074524 and average loss:0.4213389754295349\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 195 with loss: 0.533686637878418 and average loss:0.4218282103538513\n",
            "Epoch 9: LR=0.000200\n",
            "epoch: 8, batch: 200 with loss: 0.28300929069519043 and average loss:0.41990646719932556\n",
            "Epoch 9: LR=0.000200\n",
            "Validation loss: 0.41377346013106553\n",
            "epoch: 9, batch: 0 with loss: 0.5714207887649536 and average loss:0.5714207887649536\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 5 with loss: 0.4255596101284027 and average loss:0.37221571803092957\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 10 with loss: 0.32641521096229553 and average loss:0.38156670331954956\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 15 with loss: 0.6275387406349182 and average loss:0.4133477210998535\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 20 with loss: 0.37074801325798035 and average loss:0.4205218255519867\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 25 with loss: 0.39261487126350403 and average loss:0.42473769187927246\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 30 with loss: 0.4454268217086792 and average loss:0.42336180806159973\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 35 with loss: 0.40519973635673523 and average loss:0.4315713346004486\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 40 with loss: 0.41318151354789734 and average loss:0.4278663992881775\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 45 with loss: 0.4894665479660034 and average loss:0.427518755197525\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 50 with loss: 0.4438571631908417 and average loss:0.4308008849620819\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 55 with loss: 0.3963334262371063 and average loss:0.43542057275772095\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 60 with loss: 0.3871386647224426 and average loss:0.43672797083854675\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 65 with loss: 0.4131253659725189 and average loss:0.4323820173740387\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 70 with loss: 0.45600998401641846 and average loss:0.4318124055862427\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 75 with loss: 0.20699183642864227 and average loss:0.4278455078601837\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 80 with loss: 0.526605486869812 and average loss:0.42946138978004456\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 85 with loss: 0.3313571512699127 and average loss:0.4266965091228485\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 90 with loss: 0.40190589427948 and average loss:0.4248477816581726\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 95 with loss: 0.5179182291030884 and average loss:0.42506858706474304\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 100 with loss: 0.44637686014175415 and average loss:0.42534440755844116\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 105 with loss: 0.442423939704895 and average loss:0.4250599443912506\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 110 with loss: 0.3207886517047882 and average loss:0.42530733346939087\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 115 with loss: 0.3735087215900421 and average loss:0.42152467370033264\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 120 with loss: 0.3401835560798645 and average loss:0.4188368320465088\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 125 with loss: 0.614550769329071 and average loss:0.4178452491760254\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 130 with loss: 0.3829047977924347 and average loss:0.41778090596199036\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 135 with loss: 0.38142430782318115 and average loss:0.41508233547210693\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 140 with loss: 0.22389939427375793 and average loss:0.4160984456539154\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 145 with loss: 0.39962756633758545 and average loss:0.41584110260009766\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 150 with loss: 0.5147419571876526 and average loss:0.4175700545310974\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 155 with loss: 0.49509552121162415 and average loss:0.41924476623535156\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 160 with loss: 0.38870489597320557 and average loss:0.4177635610103607\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 165 with loss: 0.5026744604110718 and average loss:0.4184319078922272\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 170 with loss: 0.3417281210422516 and average loss:0.41822105646133423\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 175 with loss: 0.4353088140487671 and average loss:0.4174899756908417\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 180 with loss: 0.4214836657047272 and average loss:0.4177207350730896\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 185 with loss: 0.3078213036060333 and average loss:0.4165189862251282\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 190 with loss: 0.3750600814819336 and average loss:0.41634896397590637\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 195 with loss: 0.29159122705459595 and average loss:0.41672301292419434\n",
            "Epoch 10: LR=0.000200\n",
            "epoch: 9, batch: 200 with loss: 0.46319085359573364 and average loss:0.41716787219047546\n",
            "Epoch 10: LR=0.000200\n",
            "Validation loss: 0.4235334370066138\n",
            "epoch: 10, batch: 0 with loss: 0.5165775418281555 and average loss:0.5165775418281555\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 5 with loss: 0.4255729615688324 and average loss:0.47062820196151733\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 10 with loss: 0.4177236557006836 and average loss:0.4337378442287445\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 15 with loss: 0.4702678918838501 and average loss:0.41997069120407104\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 20 with loss: 0.12868629395961761 and average loss:0.392180860042572\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 25 with loss: 0.49191761016845703 and average loss:0.39891666173934937\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 30 with loss: 0.3906647861003876 and average loss:0.4010465741157532\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 35 with loss: 0.30966344475746155 and average loss:0.39981281757354736\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 40 with loss: 0.5859489440917969 and average loss:0.4029756188392639\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 45 with loss: 0.38600674271583557 and average loss:0.40491873025894165\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 50 with loss: 0.4474089741706848 and average loss:0.40256160497665405\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 55 with loss: 0.49144431948661804 and average loss:0.4096316397190094\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 60 with loss: 0.5339784622192383 and average loss:0.40610620379447937\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 65 with loss: 0.32631197571754456 and average loss:0.4071314036846161\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 70 with loss: 0.41640645265579224 and average loss:0.403258740901947\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 75 with loss: 0.3195224702358246 and average loss:0.3966246545314789\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 80 with loss: 0.4242269992828369 and average loss:0.3945932388305664\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 85 with loss: 0.4148358404636383 and average loss:0.39663299918174744\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 90 with loss: 0.27994853258132935 and average loss:0.39129123091697693\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 95 with loss: 0.39014941453933716 and average loss:0.3952901363372803\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 100 with loss: 0.47629672288894653 and average loss:0.3992680013179779\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 105 with loss: 0.4000230133533478 and average loss:0.39759117364883423\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 110 with loss: 0.3427332937717438 and average loss:0.3971077501773834\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 115 with loss: 0.4950145184993744 and average loss:0.39852914214134216\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 120 with loss: 0.39526546001434326 and average loss:0.3980989158153534\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 125 with loss: 0.3724168539047241 and average loss:0.3989483118057251\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 130 with loss: 0.3899945914745331 and average loss:0.3995437026023865\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 135 with loss: 0.6392329335212708 and average loss:0.4019305109977722\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 140 with loss: 0.3478039503097534 and average loss:0.3991067111492157\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 145 with loss: 0.31122198700904846 and average loss:0.3970896899700165\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 150 with loss: 0.36733800172805786 and average loss:0.3985859751701355\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 155 with loss: 0.5667176246643066 and average loss:0.4000985026359558\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 160 with loss: 0.33779454231262207 and average loss:0.4000451862812042\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 165 with loss: 0.44967734813690186 and average loss:0.4020792841911316\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 170 with loss: 0.4368419647216797 and average loss:0.4029987156391144\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 175 with loss: 0.39722102880477905 and average loss:0.4027334749698639\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 180 with loss: 0.27885761857032776 and average loss:0.4031551778316498\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 185 with loss: 0.7081993818283081 and average loss:0.4038101136684418\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 190 with loss: 0.48524561524391174 and average loss:0.4063122868537903\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 195 with loss: 0.3729960322380066 and average loss:0.40562742948532104\n",
            "Epoch 11: LR=0.000200\n",
            "epoch: 10, batch: 200 with loss: 0.41771259903907776 and average loss:0.40680280327796936\n",
            "Epoch 11: LR=0.000200\n",
            "Validation loss: 0.41878318260697756\n",
            "epoch: 11, batch: 0 with loss: 0.5963984131813049 and average loss:0.5963984131813049\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 5 with loss: 0.4292997717857361 and average loss:0.41312190890312195\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 10 with loss: 0.30150285363197327 and average loss:0.3819271922111511\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 15 with loss: 0.5171176791191101 and average loss:0.39083248376846313\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 20 with loss: 0.5775409936904907 and average loss:0.3950560986995697\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 25 with loss: 0.28507816791534424 and average loss:0.39587831497192383\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 30 with loss: 0.36849865317344666 and average loss:0.3909325897693634\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 35 with loss: 0.4371642470359802 and average loss:0.4027039706707001\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 40 with loss: 0.47433555126190186 and average loss:0.3995603024959564\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 45 with loss: 0.40060704946517944 and average loss:0.3930055499076843\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 50 with loss: 0.4033178389072418 and average loss:0.38558319211006165\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 55 with loss: 0.40779533982276917 and average loss:0.3859465420246124\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 60 with loss: 0.40079382061958313 and average loss:0.386330783367157\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 65 with loss: 0.3370005786418915 and average loss:0.384253591299057\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 70 with loss: 0.36449357867240906 and average loss:0.3889828324317932\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 75 with loss: 0.42938968539237976 and average loss:0.38924920558929443\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 80 with loss: 0.1933203786611557 and average loss:0.38808009028434753\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 85 with loss: 0.33543434739112854 and average loss:0.38546574115753174\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 90 with loss: 0.3311721384525299 and average loss:0.3844534158706665\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 95 with loss: 0.6504020690917969 and average loss:0.3898286819458008\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 100 with loss: 0.31991782784461975 and average loss:0.3880954384803772\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 105 with loss: 0.3513476550579071 and average loss:0.3872540593147278\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 110 with loss: 0.24286016821861267 and average loss:0.3848576247692108\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 115 with loss: 0.35704684257507324 and average loss:0.38352257013320923\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 120 with loss: 0.25959473848342896 and average loss:0.3818392753601074\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 125 with loss: 0.3607337176799774 and average loss:0.380523681640625\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 130 with loss: 0.3569749891757965 and average loss:0.3775290250778198\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 135 with loss: 0.4566665589809418 and average loss:0.3787039816379547\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 140 with loss: 0.4140465259552002 and average loss:0.3787153363227844\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 145 with loss: 0.36300981044769287 and average loss:0.3783867657184601\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 150 with loss: 0.33191728591918945 and average loss:0.3807092010974884\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 155 with loss: 0.5048984885215759 and average loss:0.38208845257759094\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 160 with loss: 0.5294922590255737 and average loss:0.38246679306030273\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 165 with loss: 0.3315052092075348 and average loss:0.3833823800086975\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 170 with loss: 0.4702436029911041 and average loss:0.38410913944244385\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 175 with loss: 0.39021435379981995 and average loss:0.38410553336143494\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 180 with loss: 0.2364773005247116 and average loss:0.382906049489975\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 185 with loss: 0.44157785177230835 and average loss:0.3841160535812378\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 190 with loss: 0.26864397525787354 and average loss:0.3851017355918884\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 195 with loss: 0.5469476580619812 and average loss:0.3866232931613922\n",
            "Epoch 12: LR=0.000200\n",
            "epoch: 11, batch: 200 with loss: 0.4935031533241272 and average loss:0.3859700858592987\n",
            "Epoch 12: LR=0.000200\n",
            "Validation loss: 0.39991769574436487\n",
            "epoch: 12, batch: 0 with loss: 0.31544288992881775 and average loss:0.31544288992881775\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 5 with loss: 0.3704051673412323 and average loss:0.33507126569747925\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 10 with loss: 0.26139745116233826 and average loss:0.32957273721694946\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 15 with loss: 0.3779338300228119 and average loss:0.34308964014053345\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 20 with loss: 0.31634455919265747 and average loss:0.3522595465183258\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 25 with loss: 0.34709227085113525 and average loss:0.36135026812553406\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 30 with loss: 0.30218666791915894 and average loss:0.35622134804725647\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 35 with loss: 0.47654077410697937 and average loss:0.35674941539764404\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 40 with loss: 0.36467286944389343 and average loss:0.35912927985191345\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 45 with loss: 0.414563924074173 and average loss:0.36197996139526367\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 50 with loss: 0.16329450905323029 and average loss:0.3582375943660736\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 55 with loss: 0.14915215969085693 and average loss:0.3604363799095154\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 60 with loss: 0.6524977087974548 and average loss:0.3667999804019928\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 65 with loss: 0.33792492747306824 and average loss:0.366266131401062\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 70 with loss: 0.4295333921909332 and average loss:0.36704495549201965\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 75 with loss: 0.4949902296066284 and average loss:0.3667779266834259\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 80 with loss: 0.46379554271698 and average loss:0.36765456199645996\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 85 with loss: 0.29291605949401855 and average loss:0.3668787181377411\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 90 with loss: 0.36405473947525024 and average loss:0.36761730909347534\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 95 with loss: 0.2177969217300415 and average loss:0.36652040481567383\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 100 with loss: 0.30418506264686584 and average loss:0.3637690842151642\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 105 with loss: 0.26363834738731384 and average loss:0.3640698790550232\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 110 with loss: 0.3726418912410736 and average loss:0.3643398582935333\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 115 with loss: 0.31789785623550415 and average loss:0.36642834544181824\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 120 with loss: 0.26613131165504456 and average loss:0.3647266626358032\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 125 with loss: 0.3262418210506439 and average loss:0.36241814494132996\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 130 with loss: 0.43770262598991394 and average loss:0.36334553360939026\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 135 with loss: 0.6562530398368835 and average loss:0.3638293743133545\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 140 with loss: 0.35022687911987305 and average loss:0.3643689453601837\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 145 with loss: 0.3927447497844696 and average loss:0.3656134605407715\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 150 with loss: 0.41819313168525696 and average loss:0.3673242926597595\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 155 with loss: 0.4765201807022095 and average loss:0.36757004261016846\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 160 with loss: 0.3462987244129181 and average loss:0.3690331280231476\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 165 with loss: 0.41159823536872864 and average loss:0.3681332468986511\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 170 with loss: 0.2604823112487793 and average loss:0.3668118417263031\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 175 with loss: 0.41506442427635193 and average loss:0.3661559522151947\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 180 with loss: 0.3994382619857788 and average loss:0.36593949794769287\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 185 with loss: 0.45635998249053955 and average loss:0.36646148562431335\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 190 with loss: 0.2028994858264923 and average loss:0.36741670966148376\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 195 with loss: 0.5701577067375183 and average loss:0.36863914132118225\n",
            "Epoch 13: LR=0.000200\n",
            "epoch: 12, batch: 200 with loss: 0.1799471229314804 and average loss:0.3678981363773346\n",
            "Epoch 13: LR=0.000200\n",
            "Validation loss: 0.41834590452558856\n",
            "epoch: 13, batch: 0 with loss: 0.3351677656173706 and average loss:0.3351677656173706\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 5 with loss: 0.3881491422653198 and average loss:0.34682196378707886\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 10 with loss: 0.31062084436416626 and average loss:0.33470335602760315\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 15 with loss: 0.37664127349853516 and average loss:0.3303952217102051\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 20 with loss: 0.3129117488861084 and average loss:0.3376506268978119\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 25 with loss: 0.3234041929244995 and average loss:0.3346921503543854\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 30 with loss: 0.13058604300022125 and average loss:0.32900407910346985\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 35 with loss: 0.33921244740486145 and average loss:0.3447171747684479\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 40 with loss: 0.1840890347957611 and average loss:0.34235674142837524\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 45 with loss: 0.42841047048568726 and average loss:0.3590559959411621\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 50 with loss: 0.3500872254371643 and average loss:0.36078619956970215\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 55 with loss: 0.3803218603134155 and average loss:0.3649239242076874\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 60 with loss: 0.3260250687599182 and average loss:0.37039420008659363\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 65 with loss: 0.22689758241176605 and average loss:0.3667902648448944\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 70 with loss: 0.41140812635421753 and average loss:0.3766991198062897\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 75 with loss: 0.42145273089408875 and average loss:0.3761361539363861\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 80 with loss: 0.40309658646583557 and average loss:0.377019464969635\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 85 with loss: 0.3704273998737335 and average loss:0.37809452414512634\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 90 with loss: 0.25985753536224365 and average loss:0.3786637485027313\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 95 with loss: 0.615624189376831 and average loss:0.38063710927963257\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 100 with loss: 0.35205045342445374 and average loss:0.38540369272232056\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 105 with loss: 0.40171948075294495 and average loss:0.3861231505870819\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 110 with loss: 0.2513786852359772 and average loss:0.38445553183555603\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 115 with loss: 0.4199269711971283 and average loss:0.3846275806427002\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 120 with loss: 0.47644683718681335 and average loss:0.385672003030777\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 125 with loss: 0.3166333734989166 and average loss:0.3825768232345581\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 130 with loss: 0.45447787642478943 and average loss:0.3841255307197571\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 135 with loss: 0.34371545910835266 and average loss:0.3842184543609619\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 140 with loss: 0.23072318732738495 and average loss:0.38325071334838867\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 145 with loss: 0.292971134185791 and average loss:0.38127216696739197\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 150 with loss: 0.23843418061733246 and average loss:0.3823074400424957\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 155 with loss: 0.43450868129730225 and average loss:0.383139967918396\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 160 with loss: 0.48182106018066406 and average loss:0.38007891178131104\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 165 with loss: 0.30590760707855225 and average loss:0.37972185015678406\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 170 with loss: 0.5336511731147766 and average loss:0.3804435431957245\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 175 with loss: 0.2572231590747833 and average loss:0.37984418869018555\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 180 with loss: 0.3165588676929474 and average loss:0.3778281509876251\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 185 with loss: 0.27218636870384216 and average loss:0.3777983486652374\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 190 with loss: 0.3934812843799591 and average loss:0.3780452609062195\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 195 with loss: 0.23788771033287048 and average loss:0.37672972679138184\n",
            "Epoch 14: LR=0.000200\n",
            "epoch: 13, batch: 200 with loss: 0.13595795631408691 and average loss:0.374810129404068\n",
            "Epoch 14: LR=0.000200\n",
            "Validation loss: 0.40237360812869727\n",
            "epoch: 14, batch: 0 with loss: 0.2941952347755432 and average loss:0.2941952347755432\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 5 with loss: 0.4302619993686676 and average loss:0.4198322892189026\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 10 with loss: 0.22561432421207428 and average loss:0.3606496751308441\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 15 with loss: 0.2176872193813324 and average loss:0.3269853889942169\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 20 with loss: 0.3603591024875641 and average loss:0.3264780342578888\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 25 with loss: 0.159329354763031 and average loss:0.3099384903907776\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 30 with loss: 0.3557240068912506 and average loss:0.32091253995895386\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 35 with loss: 0.300686776638031 and average loss:0.32599738240242004\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 40 with loss: 0.26237040758132935 and average loss:0.3284614384174347\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 45 with loss: 0.48835086822509766 and average loss:0.3429065942764282\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 50 with loss: 0.4598298966884613 and average loss:0.3480261266231537\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 55 with loss: 0.3426530659198761 and average loss:0.3509097397327423\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 60 with loss: 0.2704663872718811 and average loss:0.34890544414520264\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 65 with loss: 0.30950531363487244 and average loss:0.35037705302238464\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 70 with loss: 0.21064606308937073 and average loss:0.35042330622673035\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 75 with loss: 0.3232277035713196 and average loss:0.3477809727191925\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 80 with loss: 0.3419360816478729 and average loss:0.34899988770484924\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 85 with loss: 0.39878296852111816 and average loss:0.34924137592315674\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 90 with loss: 0.3149055540561676 and average loss:0.3521464467048645\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 95 with loss: 0.3207116425037384 and average loss:0.35286492109298706\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 100 with loss: 0.425066202878952 and average loss:0.3519355058670044\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 105 with loss: 0.41492271423339844 and average loss:0.35090357065200806\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 110 with loss: 0.45041006803512573 and average loss:0.35403960943222046\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 115 with loss: 0.21346637606620789 and average loss:0.3537806570529938\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 120 with loss: 0.27445346117019653 and average loss:0.3517513871192932\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 125 with loss: 0.2808075547218323 and average loss:0.3553852140903473\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 130 with loss: 0.27964678406715393 and average loss:0.3545522689819336\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 135 with loss: 0.38642263412475586 and average loss:0.3545774817466736\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 140 with loss: 0.2559509873390198 and average loss:0.3559669852256775\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 145 with loss: 0.46478110551834106 and average loss:0.35788217186927795\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 150 with loss: 0.43567630648612976 and average loss:0.36096951365470886\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 155 with loss: 0.32624948024749756 and average loss:0.3585648238658905\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 160 with loss: 0.39696523547172546 and average loss:0.3600154221057892\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 165 with loss: 0.3807006776332855 and average loss:0.3596770465373993\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 170 with loss: 0.5072215795516968 and average loss:0.35929858684539795\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 175 with loss: 0.36513879895210266 and average loss:0.3591115176677704\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 180 with loss: 0.4642445147037506 and average loss:0.3612727224826813\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 185 with loss: 0.31110745668411255 and average loss:0.35986244678497314\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 190 with loss: 0.2631283402442932 and average loss:0.3611436188220978\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 195 with loss: 0.33342963457107544 and average loss:0.36075595021247864\n",
            "Epoch 15: LR=0.000200\n",
            "epoch: 14, batch: 200 with loss: 0.4843320846557617 and average loss:0.36087483167648315\n",
            "Epoch 15: LR=0.000200\n",
            "Validation loss: 0.3703385752205755\n",
            "epoch: 15, batch: 0 with loss: 0.1583099216222763 and average loss:0.1583099216222763\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 5 with loss: 0.41937583684921265 and average loss:0.39015862345695496\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 10 with loss: 0.37105897068977356 and average loss:0.41224607825279236\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 15 with loss: 0.1992635577917099 and average loss:0.3901723325252533\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 20 with loss: 0.41361743211746216 and average loss:0.39151033759117126\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 25 with loss: 0.2817924916744232 and average loss:0.37588363885879517\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 30 with loss: 0.506344199180603 and average loss:0.3782230019569397\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 35 with loss: 0.31294792890548706 and average loss:0.3748745322227478\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 40 with loss: 0.22519978880882263 and average loss:0.3703027665615082\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 45 with loss: 0.3197445273399353 and average loss:0.36340296268463135\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 50 with loss: 0.2144950032234192 and average loss:0.36314836144447327\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 55 with loss: 0.20274047553539276 and average loss:0.3592991530895233\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 60 with loss: 0.3961631953716278 and average loss:0.3582896888256073\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 65 with loss: 0.4015713930130005 and average loss:0.3586662709712982\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 70 with loss: 0.2758331894874573 and average loss:0.3581613600254059\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 75 with loss: 0.2207954078912735 and average loss:0.3508244752883911\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 80 with loss: 0.17783528566360474 and average loss:0.34947875142097473\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 85 with loss: 0.4242282807826996 and average loss:0.3500346541404724\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 90 with loss: 0.29723525047302246 and average loss:0.34958845376968384\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 95 with loss: 0.45947378873825073 and average loss:0.35314518213272095\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 100 with loss: 0.3978380262851715 and average loss:0.35622191429138184\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 105 with loss: 0.2805924713611603 and average loss:0.35407134890556335\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 110 with loss: 0.5980677008628845 and average loss:0.35799744725227356\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 115 with loss: 0.27036231756210327 and average loss:0.354877769947052\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 120 with loss: 0.28010687232017517 and average loss:0.35147324204444885\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 125 with loss: 0.3641641139984131 and average loss:0.3530389964580536\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 130 with loss: 0.316360741853714 and average loss:0.35204315185546875\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 135 with loss: 0.23862792551517487 and average loss:0.3502756953239441\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 140 with loss: 0.3069551885128021 and average loss:0.34886714816093445\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 145 with loss: 0.2596556842327118 and average loss:0.3500339686870575\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 150 with loss: 0.496480256319046 and average loss:0.35175713896751404\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 155 with loss: 0.25515127182006836 and average loss:0.35025379061698914\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 160 with loss: 0.3474290668964386 and average loss:0.35018742084503174\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 165 with loss: 0.21991309523582458 and average loss:0.35030001401901245\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 170 with loss: 0.515129804611206 and average loss:0.35270586609840393\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 175 with loss: 0.3992922902107239 and average loss:0.35263359546661377\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 180 with loss: 0.37782803177833557 and average loss:0.3524070978164673\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 185 with loss: 0.49464651942253113 and average loss:0.3524877429008484\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 190 with loss: 0.41655153036117554 and average loss:0.35384824872016907\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 195 with loss: 0.3302850127220154 and average loss:0.3532186448574066\n",
            "Epoch 16: LR=0.000200\n",
            "epoch: 15, batch: 200 with loss: 0.2706463634967804 and average loss:0.3543201684951782\n",
            "Epoch 16: LR=0.000200\n",
            "Validation loss: 0.4155658907165714\n",
            "epoch: 16, batch: 0 with loss: 0.13964855670928955 and average loss:0.13964855670928955\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 5 with loss: 0.6213425397872925 and average loss:0.34416770935058594\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 10 with loss: 0.21894587576389313 and average loss:0.35067716240882874\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 15 with loss: 0.34351062774658203 and average loss:0.360906720161438\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 20 with loss: 0.38393494486808777 and average loss:0.3701038062572479\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 25 with loss: 0.335646390914917 and average loss:0.3567092716693878\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 30 with loss: 0.3218778967857361 and average loss:0.35265639424324036\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 35 with loss: 0.34832972288131714 and average loss:0.3542446196079254\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 40 with loss: 0.2163764238357544 and average loss:0.35081151127815247\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 45 with loss: 0.3525327742099762 and average loss:0.35184621810913086\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 50 with loss: 0.4702233672142029 and average loss:0.35466936230659485\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 55 with loss: 0.26211017370224 and average loss:0.35311025381088257\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 60 with loss: 0.3433620035648346 and average loss:0.34947842359542847\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 65 with loss: 0.22338010370731354 and average loss:0.34533920884132385\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 70 with loss: 0.3117710053920746 and average loss:0.3484247624874115\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 75 with loss: 0.3623432517051697 and average loss:0.34722885489463806\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 80 with loss: 0.3242843747138977 and average loss:0.35185495018959045\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 85 with loss: 0.4217855930328369 and average loss:0.3485862612724304\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 90 with loss: 0.28449052572250366 and average loss:0.3452335596084595\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 95 with loss: 0.3473608195781708 and average loss:0.34295064210891724\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 100 with loss: 0.3424064517021179 and average loss:0.3407273590564728\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 105 with loss: 0.44036146998405457 and average loss:0.34478774666786194\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 110 with loss: 0.21454377472400665 and average loss:0.3395197093486786\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 115 with loss: 0.350816935300827 and average loss:0.34211981296539307\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 120 with loss: 0.20638348162174225 and average loss:0.34101954102516174\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 125 with loss: 0.4858531653881073 and average loss:0.34556150436401367\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 130 with loss: 0.3047696352005005 and average loss:0.3436949849128723\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 135 with loss: 0.3618514835834503 and average loss:0.34271901845932007\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 140 with loss: 0.5505913496017456 and average loss:0.345533162355423\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 145 with loss: 0.3334464132785797 and average loss:0.34675490856170654\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 150 with loss: 0.32552188634872437 and average loss:0.3474595844745636\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 155 with loss: 0.330486923456192 and average loss:0.34746846556663513\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 160 with loss: 0.4373783767223358 and average loss:0.3472025990486145\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 165 with loss: 0.3410082757472992 and average loss:0.3488326668739319\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 170 with loss: 0.34517335891723633 and average loss:0.3493274748325348\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 175 with loss: 0.2011997252702713 and average loss:0.3488631248474121\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 180 with loss: 0.15724243223667145 and average loss:0.3481850028038025\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 185 with loss: 0.46825671195983887 and average loss:0.3486928641796112\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 190 with loss: 0.23753944039344788 and average loss:0.34576332569122314\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 195 with loss: 0.3132072687149048 and average loss:0.3462621569633484\n",
            "Epoch 17: LR=0.000200\n",
            "epoch: 16, batch: 200 with loss: 0.12878209352493286 and average loss:0.3444114625453949\n",
            "Epoch 17: LR=0.000200\n",
            "Validation loss: 0.5172076423962911\n",
            "epoch: 17, batch: 0 with loss: 0.4734136760234833 and average loss:0.4734136760234833\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 5 with loss: 0.21591871976852417 and average loss:0.3723662793636322\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 10 with loss: 0.3472324013710022 and average loss:0.34386035799980164\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 15 with loss: 0.4011203348636627 and average loss:0.3370809853076935\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 20 with loss: 0.4644373953342438 and average loss:0.3362088203430176\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 25 with loss: 0.330493688583374 and average loss:0.3268132209777832\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 30 with loss: 0.22779229283332825 and average loss:0.3245503604412079\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 35 with loss: 0.4620569348335266 and average loss:0.33566561341285706\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 40 with loss: 0.18483185768127441 and average loss:0.3313714861869812\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 45 with loss: 0.2776564955711365 and average loss:0.330768346786499\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 50 with loss: 0.16121678054332733 and average loss:0.3238803446292877\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 55 with loss: 0.3632110357284546 and average loss:0.3331991732120514\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 60 with loss: 0.41224828362464905 and average loss:0.3379424512386322\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 65 with loss: 0.30620861053466797 and average loss:0.3354700803756714\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 70 with loss: 0.28721264004707336 and average loss:0.3392545282840729\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 75 with loss: 0.4328658878803253 and average loss:0.34023183584213257\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 80 with loss: 0.3231875002384186 and average loss:0.3378186523914337\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 85 with loss: 0.20482762157917023 and average loss:0.3343815505504608\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 90 with loss: 0.2627529203891754 and average loss:0.33304157853126526\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 95 with loss: 0.24407003819942474 and average loss:0.3328385651111603\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 100 with loss: 0.4849559962749481 and average loss:0.33441415429115295\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 105 with loss: 0.4086441993713379 and average loss:0.33167698979377747\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 110 with loss: 0.3939058780670166 and average loss:0.3349093496799469\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 115 with loss: 0.3142555356025696 and average loss:0.333960622549057\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 120 with loss: 0.29640763998031616 and average loss:0.3339397609233856\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 125 with loss: 0.2819273769855499 and average loss:0.33278897404670715\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 130 with loss: 0.3588923215866089 and average loss:0.3338746726512909\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 135 with loss: 0.33599427342414856 and average loss:0.334455281496048\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 140 with loss: 0.15086951851844788 and average loss:0.3359731435775757\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 145 with loss: 0.25670361518859863 and average loss:0.33609944581985474\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 150 with loss: 0.14989212155342102 and average loss:0.3332764506340027\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 155 with loss: 0.33885276317596436 and average loss:0.3334695100784302\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 160 with loss: 0.2705684006214142 and average loss:0.33371567726135254\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 165 with loss: 0.3599579334259033 and average loss:0.333626389503479\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 170 with loss: 0.33843737840652466 and average loss:0.33206045627593994\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 175 with loss: 0.3240678310394287 and average loss:0.3324434757232666\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 180 with loss: 0.2666745185852051 and average loss:0.3335166573524475\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 185 with loss: 0.43305104970932007 and average loss:0.33344539999961853\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 190 with loss: 0.4123707115650177 and average loss:0.3321787416934967\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 195 with loss: 0.22090186178684235 and average loss:0.3305577337741852\n",
            "Epoch 18: LR=0.000200\n",
            "epoch: 17, batch: 200 with loss: 0.5357043743133545 and average loss:0.33321574330329895\n",
            "Epoch 18: LR=0.000200\n",
            "Validation loss: 0.4358423270431219\n",
            "epoch: 18, batch: 0 with loss: 0.3292141854763031 and average loss:0.3292141854763031\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 5 with loss: 0.3603493571281433 and average loss:0.34492403268814087\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 10 with loss: 0.6161053776741028 and average loss:0.37000545859336853\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 15 with loss: 0.34165695309638977 and average loss:0.34696975350379944\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 20 with loss: 0.28589704632759094 and average loss:0.3511584997177124\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 25 with loss: 0.28100645542144775 and average loss:0.3452659845352173\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 30 with loss: 0.22779318690299988 and average loss:0.3397831916809082\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 35 with loss: 0.3048435151576996 and average loss:0.336662232875824\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 40 with loss: 0.33682647347450256 and average loss:0.33390408754348755\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 45 with loss: 0.2978481352329254 and average loss:0.32822808623313904\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 50 with loss: 0.2002910077571869 and average loss:0.3242665231227875\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 55 with loss: 0.4905899167060852 and average loss:0.3261665999889374\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 60 with loss: 0.3282688856124878 and average loss:0.3294064998626709\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 65 with loss: 0.22490884363651276 and average loss:0.32990649342536926\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 70 with loss: 0.2859141528606415 and average loss:0.32689711451530457\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 75 with loss: 0.4199221730232239 and average loss:0.3259688913822174\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 80 with loss: 0.2858341932296753 and average loss:0.3263569474220276\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 85 with loss: 0.3233473300933838 and average loss:0.32714536786079407\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 90 with loss: 0.5109390616416931 and average loss:0.33185455203056335\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 95 with loss: 0.29236742854118347 and average loss:0.33303210139274597\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 100 with loss: 0.28673896193504333 and average loss:0.3340030014514923\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 105 with loss: 0.38553479313850403 and average loss:0.33348554372787476\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 110 with loss: 0.3075389862060547 and average loss:0.33284932374954224\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 115 with loss: 0.27522727847099304 and average loss:0.33054739236831665\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 120 with loss: 0.29953646659851074 and average loss:0.33097293972969055\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 125 with loss: 0.2375563234090805 and average loss:0.33002761006355286\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 130 with loss: 0.5363050699234009 and average loss:0.33397072553634644\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 135 with loss: 0.31361547112464905 and average loss:0.33444833755493164\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 140 with loss: 0.2999041974544525 and average loss:0.33445093035697937\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 145 with loss: 0.36893296241760254 and average loss:0.33651602268218994\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 150 with loss: 0.3138115704059601 and average loss:0.3357700705528259\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 155 with loss: 0.24912160634994507 and average loss:0.33568036556243896\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 160 with loss: 0.337252140045166 and average loss:0.3349316120147705\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 165 with loss: 0.33518657088279724 and average loss:0.33526158332824707\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 170 with loss: 0.6272314786911011 and average loss:0.3371626138687134\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 175 with loss: 0.390529602766037 and average loss:0.3376038074493408\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 180 with loss: 0.429638147354126 and average loss:0.33891016244888306\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 185 with loss: 0.1861608326435089 and average loss:0.3367936611175537\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 190 with loss: 0.507475733757019 and average loss:0.34042254090309143\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 195 with loss: 0.3536955714225769 and average loss:0.3397675156593323\n",
            "Epoch 19: LR=0.000200\n",
            "epoch: 18, batch: 200 with loss: 0.36143946647644043 and average loss:0.3397310972213745\n",
            "Epoch 19: LR=0.000200\n",
            "Validation loss: 0.3676678024086298\n",
            "epoch: 19, batch: 0 with loss: 0.2598368525505066 and average loss:0.2598368525505066\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 5 with loss: 0.3187342882156372 and average loss:0.288040429353714\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 10 with loss: 0.44562143087387085 and average loss:0.3122332692146301\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 15 with loss: 0.3508356213569641 and average loss:0.2947230041027069\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 20 with loss: 0.2192138135433197 and average loss:0.28415581583976746\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 25 with loss: 0.5164433717727661 and average loss:0.3089101314544678\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 30 with loss: 0.3656024932861328 and average loss:0.3062974214553833\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 35 with loss: 0.3531232178211212 and average loss:0.30664023756980896\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 40 with loss: 0.35535264015197754 and average loss:0.30551058053970337\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 45 with loss: 0.44442951679229736 and average loss:0.30726853013038635\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 50 with loss: 0.21888960897922516 and average loss:0.2996096611022949\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 55 with loss: 0.3303188681602478 and average loss:0.3034718930721283\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 60 with loss: 0.2769508957862854 and average loss:0.3016098439693451\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 65 with loss: 0.22094807028770447 and average loss:0.296801894903183\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 70 with loss: 0.4303229749202728 and average loss:0.3033125102519989\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 75 with loss: 0.5217867493629456 and average loss:0.30797451734542847\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 80 with loss: 0.4630615711212158 and average loss:0.30655360221862793\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 85 with loss: 0.2365255206823349 and average loss:0.30614927411079407\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 90 with loss: 0.4067884683609009 and average loss:0.309660941362381\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 95 with loss: 0.2237756848335266 and average loss:0.3105992078781128\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 100 with loss: 0.22267572581768036 and average loss:0.3087630271911621\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 105 with loss: 0.4029822051525116 and average loss:0.3102351129055023\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 110 with loss: 0.47077053785324097 and average loss:0.312929630279541\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 115 with loss: 0.1498633772134781 and average loss:0.311543732881546\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 120 with loss: 0.42362403869628906 and average loss:0.31690195202827454\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 125 with loss: 0.3312966525554657 and average loss:0.3194049298763275\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 130 with loss: 0.42661571502685547 and average loss:0.32059767842292786\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 135 with loss: 0.18051114678382874 and average loss:0.321449875831604\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 140 with loss: 0.6176202893257141 and average loss:0.3239566683769226\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 145 with loss: 0.2436278611421585 and average loss:0.3238641023635864\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 150 with loss: 0.43233659863471985 and average loss:0.3249483108520508\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 155 with loss: 0.2787798345088959 and average loss:0.32588428258895874\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 160 with loss: 0.2767346501350403 and average loss:0.32497718930244446\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 165 with loss: 0.5506794452667236 and average loss:0.32727915048599243\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 170 with loss: 0.3793349266052246 and average loss:0.3291184604167938\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 175 with loss: 0.2674607038497925 and average loss:0.3270980715751648\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 180 with loss: 0.2648125886917114 and average loss:0.3277660310268402\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 185 with loss: 0.40728241205215454 and average loss:0.3276608884334564\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 190 with loss: 0.3370790183544159 and average loss:0.327970951795578\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 195 with loss: 0.42258429527282715 and average loss:0.3287929892539978\n",
            "Epoch 20: LR=0.000200\n",
            "epoch: 19, batch: 200 with loss: 0.39311665296554565 and average loss:0.32853981852531433\n",
            "Epoch 20: LR=0.000200\n",
            "Validation loss: 0.36043066925862255\n",
            "epoch: 20, batch: 0 with loss: 0.22938531637191772 and average loss:0.22938531637191772\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 5 with loss: 0.20786187052726746 and average loss:0.2792617678642273\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 10 with loss: 0.30979031324386597 and average loss:0.291710764169693\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 15 with loss: 0.2686835527420044 and average loss:0.31721797585487366\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 20 with loss: 0.28625330328941345 and average loss:0.3108832836151123\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 25 with loss: 0.3182392716407776 and average loss:0.3130433261394501\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 30 with loss: 0.3333567976951599 and average loss:0.3145095705986023\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 35 with loss: 0.3389180600643158 and average loss:0.30863797664642334\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 40 with loss: 0.33309629559516907 and average loss:0.31240400671958923\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 45 with loss: 0.4247061610221863 and average loss:0.3180310130119324\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 50 with loss: 0.26903820037841797 and average loss:0.3151654303073883\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 55 with loss: 0.13686402142047882 and average loss:0.31005537509918213\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 60 with loss: 0.22751674056053162 and average loss:0.30742934346199036\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 65 with loss: 0.3432195782661438 and average loss:0.3073384463787079\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 70 with loss: 0.30755990743637085 and average loss:0.3079562783241272\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 75 with loss: 0.3658766448497772 and average loss:0.30585598945617676\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 80 with loss: 0.2531215250492096 and average loss:0.3024975657463074\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 85 with loss: 0.29196563363075256 and average loss:0.30152732133865356\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 90 with loss: 0.25944802165031433 and average loss:0.30538973212242126\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 95 with loss: 0.4533675014972687 and average loss:0.30459317564964294\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 100 with loss: 0.361802339553833 and average loss:0.3078651428222656\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 105 with loss: 0.6077940464019775 and average loss:0.3098759353160858\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 110 with loss: 0.26703953742980957 and average loss:0.3088987469673157\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 115 with loss: 0.32155072689056396 and average loss:0.3079855740070343\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 120 with loss: 0.35523471236228943 and average loss:0.3088270425796509\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 125 with loss: 0.3699159622192383 and average loss:0.3065594732761383\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 130 with loss: 0.23800043761730194 and average loss:0.30667418241500854\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 135 with loss: 0.2651144564151764 and average loss:0.3056052029132843\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 140 with loss: 0.4572252631187439 and average loss:0.30745694041252136\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 145 with loss: 0.221100315451622 and average loss:0.3070717751979828\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 150 with loss: 0.14784875512123108 and average loss:0.30667003989219666\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 155 with loss: 0.15664005279541016 and average loss:0.3075771927833557\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 160 with loss: 0.28650134801864624 and average loss:0.308046817779541\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 165 with loss: 0.3110024333000183 and average loss:0.309404581785202\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 170 with loss: 0.21140658855438232 and average loss:0.31015658378601074\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 175 with loss: 0.23777854442596436 and average loss:0.3084738850593567\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 180 with loss: 0.40570712089538574 and average loss:0.30843493342399597\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 185 with loss: 0.4025416076183319 and average loss:0.30808672308921814\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 190 with loss: 0.2859022915363312 and average loss:0.30741891264915466\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 195 with loss: 0.24565164744853973 and average loss:0.3058145344257355\n",
            "Epoch 21: LR=0.000200\n",
            "epoch: 20, batch: 200 with loss: 0.09836052358150482 and average loss:0.3054437041282654\n",
            "Epoch 21: LR=0.000200\n",
            "Validation loss: 0.36711432068955663\n",
            "epoch: 21, batch: 0 with loss: 0.30524712800979614 and average loss:0.30524712800979614\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 5 with loss: 0.46831613779067993 and average loss:0.34009048342704773\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 10 with loss: 0.3085557818412781 and average loss:0.3235987722873688\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 15 with loss: 0.3011159896850586 and average loss:0.30212485790252686\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 20 with loss: 0.4550367593765259 and average loss:0.31044119596481323\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 25 with loss: 0.279671847820282 and average loss:0.29588621854782104\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 30 with loss: 0.47658398747444153 and average loss:0.29782089591026306\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 35 with loss: 0.3152211010456085 and average loss:0.2995014786720276\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 40 with loss: 0.35453709959983826 and average loss:0.2978743314743042\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 45 with loss: 0.30303043127059937 and average loss:0.3048354983329773\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 50 with loss: 0.30169177055358887 and average loss:0.30756402015686035\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 55 with loss: 0.21619197726249695 and average loss:0.31076931953430176\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 60 with loss: 0.3187471032142639 and average loss:0.31662583351135254\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 65 with loss: 0.4288974404335022 and average loss:0.31809544563293457\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 70 with loss: 0.29127925634384155 and average loss:0.31545940041542053\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 75 with loss: 0.26340097188949585 and average loss:0.31268075108528137\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 80 with loss: 0.29742351174354553 and average loss:0.313137412071228\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 85 with loss: 0.2336030751466751 and average loss:0.31031420826911926\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 90 with loss: 0.4459501802921295 and average loss:0.31556323170661926\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 95 with loss: 0.36517566442489624 and average loss:0.31600022315979004\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 100 with loss: 0.34867915511131287 and average loss:0.3144911527633667\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 105 with loss: 0.20355577766895294 and average loss:0.3149365782737732\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 110 with loss: 0.22685600817203522 and average loss:0.31232115626335144\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 115 with loss: 0.1697799265384674 and average loss:0.3114197552204132\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 120 with loss: 0.2697997987270355 and average loss:0.31119033694267273\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 125 with loss: 0.3843502998352051 and average loss:0.3100113868713379\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 130 with loss: 0.553510844707489 and average loss:0.3123590648174286\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 135 with loss: 0.20930132269859314 and average loss:0.3134273290634155\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 140 with loss: 0.2294899821281433 and average loss:0.31341081857681274\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 145 with loss: 0.32735103368759155 and average loss:0.31450334191322327\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 150 with loss: 0.352035790681839 and average loss:0.31342455744743347\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 155 with loss: 0.38186562061309814 and average loss:0.3144839406013489\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 160 with loss: 0.23929738998413086 and average loss:0.31464120745658875\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 165 with loss: 0.2473331093788147 and average loss:0.31534355878829956\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 170 with loss: 0.3385974168777466 and average loss:0.31363409757614136\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 175 with loss: 0.18187160789966583 and average loss:0.31302690505981445\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 180 with loss: 0.18115389347076416 and average loss:0.3132714331150055\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 185 with loss: 0.30357658863067627 and average loss:0.3128175735473633\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 190 with loss: 0.32038578391075134 and average loss:0.3138003647327423\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 195 with loss: 0.27670818567276 and average loss:0.31474509835243225\n",
            "Epoch 22: LR=0.000200\n",
            "epoch: 21, batch: 200 with loss: 0.4244229197502136 and average loss:0.31445857882499695\n",
            "Epoch 22: LR=0.000200\n",
            "Validation loss: 0.38036645715143164\n",
            "epoch: 22, batch: 0 with loss: 0.3970109224319458 and average loss:0.3970109224319458\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 5 with loss: 0.3057171106338501 and average loss:0.3195886015892029\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 10 with loss: 0.37893015146255493 and average loss:0.32257992029190063\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 15 with loss: 0.3373333215713501 and average loss:0.3047337234020233\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 20 with loss: 0.23027364909648895 and average loss:0.3154853582382202\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 25 with loss: 0.18676042556762695 and average loss:0.31436610221862793\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 30 with loss: 0.2129804641008377 and average loss:0.30775895714759827\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 35 with loss: 0.3440121114253998 and average loss:0.31361332535743713\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 40 with loss: 0.26668262481689453 and average loss:0.31687963008880615\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 45 with loss: 0.12148400396108627 and average loss:0.3112126886844635\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 50 with loss: 0.15891090035438538 and average loss:0.3067948818206787\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 55 with loss: 0.3243330121040344 and average loss:0.307618111371994\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 60 with loss: 0.44442862272262573 and average loss:0.30607640743255615\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 65 with loss: 0.3089970648288727 and average loss:0.3027627170085907\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 70 with loss: 0.5859000086784363 and average loss:0.3082582950592041\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 75 with loss: 0.2591547966003418 and average loss:0.3062736988067627\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 80 with loss: 0.32015886902809143 and average loss:0.3066542148590088\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 85 with loss: 0.1589754968881607 and average loss:0.30499351024627686\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 90 with loss: 0.38894256949424744 and average loss:0.3072783946990967\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 95 with loss: 0.18659354746341705 and average loss:0.30473464727401733\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 100 with loss: 0.36351725459098816 and average loss:0.30767378211021423\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 105 with loss: 0.42383551597595215 and average loss:0.30831795930862427\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 110 with loss: 0.38009896874427795 and average loss:0.3089819550514221\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 115 with loss: 0.2730640769004822 and average loss:0.30728474259376526\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 120 with loss: 0.16241031885147095 and average loss:0.30861806869506836\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 125 with loss: 0.25225207209587097 and average loss:0.3079194724559784\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 130 with loss: 0.38761287927627563 and average loss:0.30713504552841187\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 135 with loss: 0.21593520045280457 and average loss:0.30695250630378723\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 140 with loss: 0.11684001982212067 and average loss:0.3060775697231293\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 145 with loss: 0.594531774520874 and average loss:0.30525216460227966\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 150 with loss: 0.2572445869445801 and average loss:0.30600762367248535\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 155 with loss: 0.3142590820789337 and average loss:0.30542874336242676\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 160 with loss: 0.32790815830230713 and average loss:0.30519911646842957\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 165 with loss: 0.40727800130844116 and average loss:0.30561965703964233\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 170 with loss: 0.23031874001026154 and average loss:0.30413317680358887\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 175 with loss: 0.43254655599594116 and average loss:0.3042632043361664\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 180 with loss: 0.5436281561851501 and average loss:0.30522310733795166\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 185 with loss: 0.39803868532180786 and average loss:0.3052941560745239\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 190 with loss: 0.3759227395057678 and average loss:0.3042305111885071\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 195 with loss: 0.2358861118555069 and average loss:0.3013562858104706\n",
            "Epoch 23: LR=0.000200\n",
            "epoch: 22, batch: 200 with loss: 0.0841580256819725 and average loss:0.3010672628879547\n",
            "Epoch 23: LR=0.000200\n",
            "Validation loss: 0.39892906856303123\n",
            "epoch: 23, batch: 0 with loss: 0.3032873868942261 and average loss:0.3032873868942261\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 5 with loss: 0.22544440627098083 and average loss:0.3391531705856323\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 10 with loss: 0.2811257243156433 and average loss:0.32497692108154297\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 15 with loss: 0.18779972195625305 and average loss:0.3209097683429718\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 20 with loss: 0.21312156319618225 and average loss:0.3213961124420166\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 25 with loss: 0.4139452874660492 and average loss:0.31788790225982666\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 30 with loss: 0.10117287933826447 and average loss:0.3100859820842743\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 35 with loss: 0.2578015923500061 and average loss:0.3062403202056885\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 40 with loss: 0.15594927966594696 and average loss:0.29902273416519165\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 45 with loss: 0.34475621581077576 and average loss:0.29335176944732666\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 50 with loss: 0.48465999960899353 and average loss:0.29853004217147827\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 55 with loss: 0.23343387246131897 and average loss:0.30209118127822876\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 60 with loss: 0.21507138013839722 and average loss:0.29607412219047546\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 65 with loss: 0.3238389492034912 and average loss:0.29873451590538025\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 70 with loss: 0.35092228651046753 and average loss:0.29865047335624695\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 75 with loss: 0.146255761384964 and average loss:0.29953858256340027\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 80 with loss: 0.23025581240653992 and average loss:0.3019265830516815\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 85 with loss: 0.1661226600408554 and average loss:0.3018471300601959\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 90 with loss: 0.2309104949235916 and average loss:0.29859423637390137\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 95 with loss: 0.4683837890625 and average loss:0.29977044463157654\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 100 with loss: 0.31343239545822144 and average loss:0.29934898018836975\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 105 with loss: 0.34061509370803833 and average loss:0.2973938584327698\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 110 with loss: 0.33834612369537354 and average loss:0.2972297966480255\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 115 with loss: 0.3532206416130066 and average loss:0.29441288113594055\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 120 with loss: 0.1878264993429184 and average loss:0.2935641407966614\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 125 with loss: 0.17880435287952423 and average loss:0.2983492314815521\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 130 with loss: 0.179363414645195 and average loss:0.29915377497673035\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 135 with loss: 0.20924723148345947 and average loss:0.2985526919364929\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 140 with loss: 0.19966422021389008 and average loss:0.29824602603912354\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 145 with loss: 0.27417799830436707 and average loss:0.29789844155311584\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 150 with loss: 0.44005730748176575 and average loss:0.29917412996292114\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 155 with loss: 0.34285905957221985 and average loss:0.29855668544769287\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 160 with loss: 0.2383064329624176 and average loss:0.2988321781158447\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 165 with loss: 0.30990228056907654 and average loss:0.29990828037261963\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 170 with loss: 0.2762891352176666 and average loss:0.2997089922428131\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 175 with loss: 0.32992422580718994 and average loss:0.29930543899536133\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 180 with loss: 0.29056382179260254 and average loss:0.29907622933387756\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 185 with loss: 0.2133537381887436 and average loss:0.3000966012477875\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 190 with loss: 0.3424774706363678 and average loss:0.30276381969451904\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 195 with loss: 0.29695719480514526 and average loss:0.3038315176963806\n",
            "Epoch 24: LR=0.000200\n",
            "epoch: 23, batch: 200 with loss: 0.5660172700881958 and average loss:0.3052186965942383\n",
            "Epoch 24: LR=0.000200\n",
            "Validation loss: 0.3756678498843137\n",
            "epoch: 24, batch: 0 with loss: 0.355133980512619 and average loss:0.355133980512619\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 5 with loss: 0.32020729780197144 and average loss:0.35419294238090515\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 10 with loss: 0.33810004591941833 and average loss:0.37979477643966675\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 15 with loss: 0.34128060936927795 and average loss:0.35592296719551086\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 20 with loss: 0.2301083207130432 and average loss:0.34177520871162415\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 25 with loss: 0.2909807562828064 and average loss:0.33844104409217834\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 30 with loss: 0.23189058899879456 and average loss:0.33625999093055725\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 35 with loss: 0.396089106798172 and average loss:0.32741209864616394\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 40 with loss: 0.21721810102462769 and average loss:0.32521986961364746\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 45 with loss: 0.22249329090118408 and average loss:0.3194505572319031\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 50 with loss: 0.2259429544210434 and average loss:0.31512370705604553\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 55 with loss: 0.29584649205207825 and average loss:0.3116135001182556\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 60 with loss: 0.3228396475315094 and average loss:0.31554991006851196\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 65 with loss: 0.36920562386512756 and average loss:0.310665100812912\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 70 with loss: 0.41656991839408875 and average loss:0.3124762773513794\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 75 with loss: 0.4461125135421753 and average loss:0.30953437089920044\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 80 with loss: 0.5045375227928162 and average loss:0.3098907470703125\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 85 with loss: 0.1306702345609665 and average loss:0.30700746178627014\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 90 with loss: 0.18291907012462616 and average loss:0.3014090657234192\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 95 with loss: 0.10549107193946838 and average loss:0.29897817969322205\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 100 with loss: 0.27342289686203003 and average loss:0.2967143654823303\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 105 with loss: 0.13240453600883484 and average loss:0.29205122590065\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 110 with loss: 0.4088318943977356 and average loss:0.29176318645477295\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 115 with loss: 0.28477904200553894 and average loss:0.28988078236579895\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 120 with loss: 0.3498620390892029 and average loss:0.28848254680633545\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 125 with loss: 0.23941269516944885 and average loss:0.28871044516563416\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 130 with loss: 0.1940670609474182 and average loss:0.2887789309024811\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 135 with loss: 0.32466986775398254 and average loss:0.2868236005306244\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 140 with loss: 0.3183330297470093 and average loss:0.2864936590194702\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 145 with loss: 0.3267425298690796 and average loss:0.28564679622650146\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 150 with loss: 0.15469616651535034 and average loss:0.2847820222377777\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 155 with loss: 0.2820872962474823 and average loss:0.28426167368888855\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 160 with loss: 0.20350155234336853 and average loss:0.28274354338645935\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 165 with loss: 0.2829646170139313 and average loss:0.28193825483322144\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 170 with loss: 0.23184283077716827 and average loss:0.28234991431236267\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 175 with loss: 0.23328503966331482 and average loss:0.2809065282344818\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 180 with loss: 0.3447570204734802 and average loss:0.2811499834060669\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 185 with loss: 0.2814928889274597 and average loss:0.2787303328514099\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 190 with loss: 0.15163396298885345 and average loss:0.2775976061820984\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 195 with loss: 0.17556194961071014 and average loss:0.27637240290641785\n",
            "Epoch 25: LR=0.000100\n",
            "epoch: 24, batch: 200 with loss: 0.014524023979902267 and average loss:0.27466055750846863\n",
            "Epoch 25: LR=0.000100\n",
            "Validation loss: 0.36859987500835867\n",
            "epoch: 25, batch: 0 with loss: 0.21436072885990143 and average loss:0.21436072885990143\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 5 with loss: 0.28031298518180847 and average loss:0.29951006174087524\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 10 with loss: 0.4101425111293793 and average loss:0.28718480467796326\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 15 with loss: 0.19562366604804993 and average loss:0.2695316970348358\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 20 with loss: 0.18145397305488586 and average loss:0.2669677436351776\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 25 with loss: 0.20390547811985016 and average loss:0.26822930574417114\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 30 with loss: 0.4634597897529602 and average loss:0.2744452953338623\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 35 with loss: 0.3374071717262268 and average loss:0.2719506025314331\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 40 with loss: 0.4881983697414398 and average loss:0.27029135823249817\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 45 with loss: 0.3220689296722412 and average loss:0.27396294474601746\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 50 with loss: 0.20253582298755646 and average loss:0.2650138735771179\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 55 with loss: 0.39538440108299255 and average loss:0.2631584405899048\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 60 with loss: 0.373661071062088 and average loss:0.2643435597419739\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 65 with loss: 0.3652092218399048 and average loss:0.2616614103317261\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 70 with loss: 0.17369496822357178 and average loss:0.2605791687965393\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 75 with loss: 0.1694989800453186 and average loss:0.2597525417804718\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 80 with loss: 0.29198309779167175 and average loss:0.26029902696609497\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 85 with loss: 0.2235255241394043 and average loss:0.2589140832424164\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 90 with loss: 0.4197092354297638 and average loss:0.2590804994106293\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 95 with loss: 0.27500638365745544 and average loss:0.2574268579483032\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 100 with loss: 0.4572821855545044 and average loss:0.26074808835983276\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 105 with loss: 0.24234552681446075 and average loss:0.2584279775619507\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 110 with loss: 0.18512780964374542 and average loss:0.25820788741111755\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 115 with loss: 0.2404760718345642 and average loss:0.254673570394516\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 120 with loss: 0.16400578618049622 and average loss:0.2523270547389984\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 125 with loss: 0.34217569231987 and average loss:0.25101524591445923\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 130 with loss: 0.27198272943496704 and average loss:0.2518853545188904\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 135 with loss: 0.308531790971756 and average loss:0.2507626414299011\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 140 with loss: 0.29509127140045166 and average loss:0.25182849168777466\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 145 with loss: 0.3306986689567566 and average loss:0.2535781264305115\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 150 with loss: 0.1879119873046875 and average loss:0.2534020245075226\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 155 with loss: 0.13786481320858002 and average loss:0.2544208765029907\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 160 with loss: 0.16529317200183868 and average loss:0.2537561058998108\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 165 with loss: 0.16643215715885162 and average loss:0.25635483860969543\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 170 with loss: 0.24221032857894897 and average loss:0.2574899196624756\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 175 with loss: 0.30960893630981445 and average loss:0.256389856338501\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 180 with loss: 0.15277890861034393 and average loss:0.25670138001441956\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 185 with loss: 0.26784050464630127 and average loss:0.256535142660141\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 190 with loss: 0.14868474006652832 and average loss:0.25610262155532837\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 195 with loss: 0.23715245723724365 and average loss:0.25435835123062134\n",
            "Epoch 26: LR=0.000100\n",
            "epoch: 25, batch: 200 with loss: 0.789875864982605 and average loss:0.25825735926628113\n",
            "Epoch 26: LR=0.000100\n",
            "Validation loss: 0.33504637842084845\n",
            "epoch: 26, batch: 0 with loss: 0.18083836138248444 and average loss:0.18083836138248444\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 5 with loss: 0.3310806155204773 and average loss:0.2626238465309143\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 10 with loss: 0.1781456619501114 and average loss:0.24789592623710632\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 15 with loss: 0.3048643469810486 and average loss:0.25175076723098755\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 20 with loss: 0.15078121423721313 and average loss:0.24085672199726105\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 25 with loss: 0.26111119985580444 and average loss:0.24082690477371216\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 30 with loss: 0.24439875781536102 and average loss:0.2546482980251312\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 35 with loss: 0.24691040813922882 and average loss:0.2600422501564026\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 40 with loss: 0.20583276450634003 and average loss:0.2536872923374176\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 45 with loss: 0.2566547989845276 and average loss:0.2565922439098358\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 50 with loss: 0.33192145824432373 and average loss:0.2544727623462677\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 55 with loss: 0.15253664553165436 and average loss:0.2505256235599518\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 60 with loss: 0.3452652394771576 and average loss:0.25041934847831726\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 65 with loss: 0.20705536007881165 and average loss:0.26151737570762634\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 70 with loss: 0.26126688718795776 and average loss:0.26063433289527893\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 75 with loss: 0.23366756737232208 and average loss:0.25778287649154663\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 80 with loss: 0.28233587741851807 and average loss:0.25810977816581726\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 85 with loss: 0.23776394128799438 and average loss:0.26084479689598083\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 90 with loss: 0.2646021842956543 and average loss:0.258563369512558\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 95 with loss: 0.27256423234939575 and average loss:0.2597079575061798\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 100 with loss: 0.26247477531433105 and average loss:0.26095813512802124\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 105 with loss: 0.15987764298915863 and average loss:0.25968122482299805\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 110 with loss: 0.21753789484500885 and average loss:0.2596595883369446\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 115 with loss: 0.4137343466281891 and average loss:0.2594648003578186\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 120 with loss: 0.3419586718082428 and average loss:0.26083827018737793\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 125 with loss: 0.23099493980407715 and average loss:0.2596854865550995\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 130 with loss: 0.1344708353281021 and average loss:0.2593947649002075\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 135 with loss: 0.2605946660041809 and average loss:0.2602868974208832\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 140 with loss: 0.15633368492126465 and average loss:0.25899437069892883\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 145 with loss: 0.22776037454605103 and average loss:0.2587474584579468\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 150 with loss: 0.2773614823818207 and average loss:0.25798720121383667\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 155 with loss: 0.27428337931632996 and average loss:0.25743505358695984\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 160 with loss: 0.2126048058271408 and average loss:0.2558043301105499\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 165 with loss: 0.15085013210773468 and average loss:0.25489526987075806\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 170 with loss: 0.2941444516181946 and average loss:0.25439321994781494\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 175 with loss: 0.4049117863178253 and average loss:0.255667120218277\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 180 with loss: 0.23704718053340912 and average loss:0.25409629940986633\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 185 with loss: 0.34747183322906494 and average loss:0.2545801103115082\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 190 with loss: 0.29875704646110535 and average loss:0.25267916917800903\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 195 with loss: 0.31539174914360046 and average loss:0.2531593143939972\n",
            "Epoch 27: LR=0.000100\n",
            "epoch: 26, batch: 200 with loss: 0.06278273463249207 and average loss:0.2516123652458191\n",
            "Epoch 27: LR=0.000100\n",
            "Validation loss: 0.37589899465149523\n",
            "epoch: 27, batch: 0 with loss: 0.3083442151546478 and average loss:0.3083442151546478\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 5 with loss: 0.30759647488594055 and average loss:0.27727144956588745\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 10 with loss: 0.2212398797273636 and average loss:0.2833336293697357\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 15 with loss: 0.2648443877696991 and average loss:0.25749653577804565\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 20 with loss: 0.1726008653640747 and average loss:0.2531040906906128\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 25 with loss: 0.2685195505619049 and average loss:0.24686244130134583\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 30 with loss: 0.45015445351600647 and average loss:0.25894638895988464\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 35 with loss: 0.17274148762226105 and average loss:0.24415554106235504\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 40 with loss: 0.22427089512348175 and average loss:0.240594744682312\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 45 with loss: 0.17357176542282104 and average loss:0.2369609773159027\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 50 with loss: 0.38162750005722046 and average loss:0.24138228595256805\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 55 with loss: 0.22749295830726624 and average loss:0.24250903725624084\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 60 with loss: 0.2984545826911926 and average loss:0.23935198783874512\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 65 with loss: 0.13856609165668488 and average loss:0.2390245944261551\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 70 with loss: 0.250395804643631 and average loss:0.24214032292366028\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 75 with loss: 0.2309357225894928 and average loss:0.24322567880153656\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 80 with loss: 0.2199365347623825 and average loss:0.23931089043617249\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 85 with loss: 0.2051139771938324 and average loss:0.23813007771968842\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 90 with loss: 0.16150327026844025 and average loss:0.23627938330173492\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 95 with loss: 0.22142232954502106 and average loss:0.23708203434944153\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 100 with loss: 0.18752916157245636 and average loss:0.23867468535900116\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 105 with loss: 0.1182471439242363 and average loss:0.2405187487602234\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 110 with loss: 0.0879589393734932 and average loss:0.23736783862113953\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 115 with loss: 0.3508806824684143 and average loss:0.23957763612270355\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 120 with loss: 0.24942408502101898 and average loss:0.23915274441242218\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 125 with loss: 0.2915797531604767 and average loss:0.2388898879289627\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 130 with loss: 0.1830565631389618 and average loss:0.23732683062553406\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 135 with loss: 0.2161969095468521 and average loss:0.2360364943742752\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 140 with loss: 0.46902182698249817 and average loss:0.23777537047863007\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 145 with loss: 0.07730832695960999 and average loss:0.23642085492610931\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 150 with loss: 0.17894059419631958 and average loss:0.23589666187763214\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 155 with loss: 0.1673094630241394 and average loss:0.2377542406320572\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 160 with loss: 0.33779969811439514 and average loss:0.2378789782524109\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 165 with loss: 0.3074240982532501 and average loss:0.2390793114900589\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 170 with loss: 0.23664511740207672 and average loss:0.23942595720291138\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 175 with loss: 0.12436418980360031 and average loss:0.24134676158428192\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 180 with loss: 0.23211818933486938 and average loss:0.24098257720470428\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 185 with loss: 0.34027278423309326 and average loss:0.24305827915668488\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 190 with loss: 0.13015660643577576 and average loss:0.24392598867416382\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 195 with loss: 0.19567793607711792 and average loss:0.24446463584899902\n",
            "Epoch 28: LR=0.000100\n",
            "epoch: 27, batch: 200 with loss: 0.30415821075439453 and average loss:0.24496163427829742\n",
            "Epoch 28: LR=0.000100\n",
            "Validation loss: 0.3545001642961128\n",
            "epoch: 28, batch: 0 with loss: 0.3852301239967346 and average loss:0.3852301239967346\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 5 with loss: 0.22333544492721558 and average loss:0.25290533900260925\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 10 with loss: 0.21826869249343872 and average loss:0.25243502855300903\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 15 with loss: 0.3475898802280426 and average loss:0.23585529625415802\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 20 with loss: 0.2743583619594574 and average loss:0.2487221509218216\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 25 with loss: 0.25046977400779724 and average loss:0.23622167110443115\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 30 with loss: 0.1696779578924179 and average loss:0.24136888980865479\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 35 with loss: 0.23573078215122223 and average loss:0.23303432762622833\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 40 with loss: 0.30394694209098816 and average loss:0.23688152432441711\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 45 with loss: 0.20461800694465637 and average loss:0.23477351665496826\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 50 with loss: 0.31256037950515747 and average loss:0.2307259440422058\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 55 with loss: 0.33944639563560486 and average loss:0.2350170761346817\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 60 with loss: 0.19673015177249908 and average loss:0.23465591669082642\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 65 with loss: 0.14236760139465332 and average loss:0.23347456753253937\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 70 with loss: 0.16397424042224884 and average loss:0.23674564063549042\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 75 with loss: 0.12643009424209595 and average loss:0.23524966835975647\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 80 with loss: 0.15391895174980164 and average loss:0.2320316880941391\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 85 with loss: 0.21164464950561523 and average loss:0.22909855842590332\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 90 with loss: 0.11599603295326233 and average loss:0.23052352666854858\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 95 with loss: 0.33292722702026367 and average loss:0.23376822471618652\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 100 with loss: 0.3222466707229614 and average loss:0.23591218888759613\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 105 with loss: 0.3324663043022156 and average loss:0.23662981390953064\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 110 with loss: 0.3042210340499878 and average loss:0.2385798841714859\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 115 with loss: 0.18340051174163818 and average loss:0.23784446716308594\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 120 with loss: 0.22229403257369995 and average loss:0.2392328977584839\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 125 with loss: 0.19938218593597412 and average loss:0.23926439881324768\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 130 with loss: 0.4044685959815979 and average loss:0.23956173658370972\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 135 with loss: 0.1703079640865326 and average loss:0.23825877904891968\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 140 with loss: 0.21234148740768433 and average loss:0.24107052385807037\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 145 with loss: 0.24368205666542053 and average loss:0.24013707041740417\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 150 with loss: 0.20851407945156097 and average loss:0.23948277533054352\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 155 with loss: 0.286648154258728 and average loss:0.23943637311458588\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 160 with loss: 0.37134888768196106 and average loss:0.23849445581436157\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 165 with loss: 0.22045160830020905 and average loss:0.23789745569229126\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 170 with loss: 0.28365904092788696 and average loss:0.23983222246170044\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 175 with loss: 0.12867744266986847 and average loss:0.24333007633686066\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 180 with loss: 0.2932267487049103 and average loss:0.24306128919124603\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 185 with loss: 0.252816379070282 and average loss:0.2427159994840622\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 190 with loss: 0.1837504357099533 and average loss:0.24311639368534088\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 195 with loss: 0.22234714031219482 and average loss:0.24211649596691132\n",
            "Epoch 29: LR=0.000100\n",
            "epoch: 28, batch: 200 with loss: 0.006251094862818718 and average loss:0.24232164025306702\n",
            "Epoch 29: LR=0.000100\n",
            "Validation loss: 0.3492941019289634\n",
            "epoch: 29, batch: 0 with loss: 0.12101443111896515 and average loss:0.12101443111896515\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 5 with loss: 0.2569425106048584 and average loss:0.28088241815567017\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 10 with loss: 0.2327721118927002 and average loss:0.2405935674905777\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 15 with loss: 0.24268761277198792 and average loss:0.2615264058113098\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 20 with loss: 0.3163506090641022 and average loss:0.24767088890075684\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 25 with loss: 0.27818191051483154 and average loss:0.25699684023857117\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 30 with loss: 0.2529996335506439 and average loss:0.26891857385635376\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 35 with loss: 0.32973456382751465 and average loss:0.26755231618881226\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 40 with loss: 0.16945905983448029 and average loss:0.2584693729877472\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 45 with loss: 0.26342079043388367 and average loss:0.25320911407470703\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 50 with loss: 0.2083309441804886 and average loss:0.24690544605255127\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 55 with loss: 0.11829366534948349 and average loss:0.24085433781147003\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 60 with loss: 0.16689586639404297 and average loss:0.2387499213218689\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 65 with loss: 0.33861568570137024 and average loss:0.24501046538352966\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 70 with loss: 0.2842576503753662 and average loss:0.24135008454322815\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 75 with loss: 0.3717769682407379 and average loss:0.24025683104991913\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 80 with loss: 0.137272909283638 and average loss:0.2364414781332016\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 85 with loss: 0.2123667299747467 and average loss:0.23944781720638275\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 90 with loss: 0.2388417273759842 and average loss:0.23897984623908997\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 95 with loss: 0.3432164192199707 and average loss:0.2388184368610382\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 100 with loss: 0.1974973827600479 and average loss:0.23750197887420654\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 105 with loss: 0.25583499670028687 and average loss:0.23900428414344788\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 110 with loss: 0.4312150478363037 and average loss:0.23903237283229828\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 115 with loss: 0.3957288861274719 and average loss:0.24052047729492188\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 120 with loss: 0.25214582681655884 and average loss:0.23877353966236115\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 125 with loss: 0.2551780939102173 and average loss:0.237547367811203\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 130 with loss: 0.21129406988620758 and average loss:0.2383420467376709\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 135 with loss: 0.13580331206321716 and average loss:0.23835451900959015\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 140 with loss: 0.15880535542964935 and average loss:0.23588550090789795\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 145 with loss: 0.35118141770362854 and average loss:0.23637761175632477\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 150 with loss: 0.38573455810546875 and average loss:0.23906134068965912\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 155 with loss: 0.2547270357608795 and average loss:0.2388865202665329\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 160 with loss: 0.1940227597951889 and average loss:0.23663823306560516\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 165 with loss: 0.11627563089132309 and average loss:0.23721739649772644\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 170 with loss: 0.16556420922279358 and average loss:0.236399307847023\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 175 with loss: 0.36334460973739624 and average loss:0.23503415286540985\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 180 with loss: 0.09609150141477585 and average loss:0.23716521263122559\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 185 with loss: 0.24346698820590973 and average loss:0.23822082579135895\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 190 with loss: 0.25295817852020264 and average loss:0.23724816739559174\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 195 with loss: 0.2909117043018341 and average loss:0.23690249025821686\n",
            "Epoch 30: LR=0.000100\n",
            "epoch: 29, batch: 200 with loss: 0.08402715623378754 and average loss:0.23619073629379272\n",
            "Epoch 30: LR=0.000100\n",
            "Validation loss: 0.363355323380115\n",
            "epoch: 30, batch: 0 with loss: 0.20415785908699036 and average loss:0.20415785908699036\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 5 with loss: 0.39761385321617126 and average loss:0.30432844161987305\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 10 with loss: 0.30285337567329407 and average loss:0.2759144902229309\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 15 with loss: 0.158741757273674 and average loss:0.2609831392765045\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 20 with loss: 0.19160065054893494 and average loss:0.2471572607755661\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 25 with loss: 0.22783897817134857 and average loss:0.23768697679042816\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 30 with loss: 0.21143358945846558 and average loss:0.23490259051322937\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 35 with loss: 0.2995377480983734 and average loss:0.23514696955680847\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 40 with loss: 0.37847983837127686 and average loss:0.23586112260818481\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 45 with loss: 0.11755945533514023 and average loss:0.22790394723415375\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 50 with loss: 0.16394244134426117 and average loss:0.22777149081230164\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 55 with loss: 0.19420155882835388 and average loss:0.2273600846529007\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 60 with loss: 0.16149860620498657 and average loss:0.2254689484834671\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 65 with loss: 0.3290785849094391 and average loss:0.2270573079586029\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 70 with loss: 0.22672365605831146 and average loss:0.22449377179145813\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 75 with loss: 0.16958500444889069 and average loss:0.2289833277463913\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 80 with loss: 0.17833775281906128 and average loss:0.22535912692546844\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 85 with loss: 0.3955026865005493 and average loss:0.22832724452018738\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 90 with loss: 0.1183524802327156 and average loss:0.22467945516109467\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 95 with loss: 0.13139107823371887 and average loss:0.2212154120206833\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 100 with loss: 0.2661823034286499 and average loss:0.22291120886802673\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 105 with loss: 0.1805497109889984 and average loss:0.22020582854747772\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 110 with loss: 0.39516234397888184 and average loss:0.22041556239128113\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 115 with loss: 0.3540091812610626 and average loss:0.22162115573883057\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 120 with loss: 0.15248161554336548 and average loss:0.2216043323278427\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 125 with loss: 0.26603442430496216 and average loss:0.220337375998497\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 130 with loss: 0.2093501091003418 and average loss:0.22239404916763306\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 135 with loss: 0.11557549238204956 and average loss:0.22149868309497833\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 140 with loss: 0.15423840284347534 and average loss:0.2232152372598648\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 145 with loss: 0.14602623879909515 and average loss:0.22160613536834717\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 150 with loss: 0.1240970715880394 and average loss:0.22119297087192535\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 155 with loss: 0.18513472378253937 and average loss:0.22208522260189056\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 160 with loss: 0.23361796140670776 and average loss:0.22135449945926666\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 165 with loss: 0.15701451897621155 and average loss:0.22203031182289124\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 170 with loss: 0.26401031017303467 and average loss:0.22243879735469818\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 175 with loss: 0.39981377124786377 and average loss:0.22302325069904327\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 180 with loss: 0.3442704677581787 and average loss:0.22352245450019836\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 185 with loss: 0.32634028792381287 and average loss:0.22375836968421936\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 190 with loss: 0.23173902928829193 and average loss:0.22481194138526917\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 195 with loss: 0.23535211384296417 and average loss:0.22328664362430573\n",
            "Epoch 31: LR=0.000050\n",
            "epoch: 30, batch: 200 with loss: 0.017977846786379814 and average loss:0.22295361757278442\n",
            "Epoch 31: LR=0.000050\n",
            "Validation loss: 0.31506789738641067\n",
            "epoch: 31, batch: 0 with loss: 0.26964500546455383 and average loss:0.26964500546455383\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 5 with loss: 0.11505639553070068 and average loss:0.17780785262584686\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 10 with loss: 0.09259266406297684 and average loss:0.19096243381500244\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 15 with loss: 0.3218953311443329 and average loss:0.2005564272403717\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 20 with loss: 0.18572059273719788 and average loss:0.20772109925746918\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 25 with loss: 0.18137772381305695 and average loss:0.19938308000564575\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 30 with loss: 0.15505319833755493 and average loss:0.2010844349861145\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 35 with loss: 0.2250511348247528 and average loss:0.1911303997039795\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 40 with loss: 0.3313889801502228 and average loss:0.19450059533119202\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 45 with loss: 0.18645018339157104 and average loss:0.20414996147155762\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 50 with loss: 0.05994774028658867 and average loss:0.20299272239208221\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 55 with loss: 0.2369873821735382 and average loss:0.2037932425737381\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 60 with loss: 0.15422964096069336 and average loss:0.20246857404708862\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 65 with loss: 0.25298309326171875 and average loss:0.20225799083709717\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 70 with loss: 0.12611690163612366 and average loss:0.2030334323644638\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 75 with loss: 0.2111644446849823 and average loss:0.2053222954273224\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 80 with loss: 0.31665509939193726 and average loss:0.20730960369110107\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 85 with loss: 0.1959855854511261 and average loss:0.20427089929580688\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 90 with loss: 0.10299801081418991 and average loss:0.20531149208545685\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 95 with loss: 0.22488458454608917 and average loss:0.20534607768058777\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 100 with loss: 0.04951170086860657 and average loss:0.20148220658302307\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 105 with loss: 0.1552126556634903 and average loss:0.1993597000837326\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 110 with loss: 0.14819121360778809 and average loss:0.20131923258304596\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 115 with loss: 0.28531748056411743 and average loss:0.20313900709152222\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 120 with loss: 0.3580290377140045 and average loss:0.20493271946907043\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 125 with loss: 0.28855738043785095 and average loss:0.20975913107395172\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 130 with loss: 0.4842540919780731 and average loss:0.20968097448349\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 135 with loss: 0.08960733562707901 and average loss:0.20794670283794403\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 140 with loss: 0.1558900624513626 and average loss:0.2070949226617813\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 145 with loss: 0.0823076069355011 and average loss:0.205719456076622\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 150 with loss: 0.277590811252594 and average loss:0.2061980962753296\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 155 with loss: 0.10788266360759735 and average loss:0.20861408114433289\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 160 with loss: 0.27248236536979675 and average loss:0.21123363077640533\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 165 with loss: 0.1923903226852417 and average loss:0.2112809121608734\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 170 with loss: 0.319374680519104 and average loss:0.2113802582025528\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 175 with loss: 0.2985440790653229 and average loss:0.21268734335899353\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 180 with loss: 0.27435627579689026 and average loss:0.21264120936393738\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 185 with loss: 0.13162215054035187 and average loss:0.21289129555225372\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 190 with loss: 0.1402704268693924 and average loss:0.21207080781459808\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 195 with loss: 0.16253256797790527 and average loss:0.21311035752296448\n",
            "Epoch 32: LR=0.000050\n",
            "epoch: 31, batch: 200 with loss: 0.3159288465976715 and average loss:0.214975968003273\n",
            "Epoch 32: LR=0.000050\n",
            "Validation loss: 0.33802113053845423\n",
            "epoch: 32, batch: 0 with loss: 0.1501552164554596 and average loss:0.1501552164554596\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 5 with loss: 0.2669427990913391 and average loss:0.187015101313591\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 10 with loss: 0.30550694465637207 and average loss:0.21740682423114777\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 15 with loss: 0.22053048014640808 and average loss:0.20592878758907318\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 20 with loss: 0.2501654028892517 and average loss:0.21379075944423676\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 25 with loss: 0.1703299880027771 and average loss:0.21988467872142792\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 30 with loss: 0.1620432734489441 and average loss:0.21002992987632751\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 35 with loss: 0.24429936707019806 and average loss:0.20606105029582977\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 40 with loss: 0.25310423970222473 and average loss:0.20654836297035217\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 45 with loss: 0.21547706425189972 and average loss:0.20537921786308289\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 50 with loss: 0.3661612272262573 and average loss:0.2085571587085724\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 55 with loss: 0.13252754509449005 and average loss:0.20972630381584167\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 60 with loss: 0.19664186239242554 and average loss:0.20555157959461212\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 65 with loss: 0.19372649490833282 and average loss:0.20277859270572662\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 70 with loss: 0.12641103565692902 and average loss:0.20029428601264954\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 75 with loss: 0.20422253012657166 and average loss:0.19958731532096863\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 80 with loss: 0.137028768658638 and average loss:0.1971094161272049\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 85 with loss: 0.3996600806713104 and average loss:0.19563880562782288\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 90 with loss: 0.41308051347732544 and average loss:0.19766029715538025\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 95 with loss: 0.10589121282100677 and average loss:0.19878548383712769\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 100 with loss: 0.45715630054473877 and average loss:0.2036582976579666\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 105 with loss: 0.18916165828704834 and average loss:0.2019772082567215\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 110 with loss: 0.12892670929431915 and average loss:0.2039000391960144\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 115 with loss: 0.27378883957862854 and average loss:0.20256762206554413\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 120 with loss: 0.15483373403549194 and average loss:0.20319470763206482\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 125 with loss: 0.2882586121559143 and average loss:0.20584233105182648\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 130 with loss: 0.3609694242477417 and average loss:0.20911142230033875\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 135 with loss: 0.2953405976295471 and average loss:0.2086213082075119\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 140 with loss: 0.32331711053848267 and average loss:0.2116614133119583\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 145 with loss: 0.13905119895935059 and average loss:0.211735799908638\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 150 with loss: 0.1563781350851059 and average loss:0.21160873770713806\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 155 with loss: 0.2720908224582672 and average loss:0.21132540702819824\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 160 with loss: 0.1044362485408783 and average loss:0.2108558714389801\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 165 with loss: 0.2111983597278595 and average loss:0.20984257757663727\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 170 with loss: 0.35473400354385376 and average loss:0.20946529507637024\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 175 with loss: 0.2550184726715088 and average loss:0.21062219142913818\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 180 with loss: 0.11533743143081665 and average loss:0.20985014736652374\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 185 with loss: 0.15571218729019165 and average loss:0.20907452702522278\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 190 with loss: 0.16133779287338257 and average loss:0.20841474831104279\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 195 with loss: 0.11950923502445221 and average loss:0.20727936923503876\n",
            "Epoch 33: LR=0.000050\n",
            "epoch: 32, batch: 200 with loss: 0.2663014531135559 and average loss:0.20894983410835266\n",
            "Epoch 33: LR=0.000050\n",
            "Validation loss: 0.3478955537372944\n",
            "epoch: 33, batch: 0 with loss: 0.2597619891166687 and average loss:0.2597619891166687\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 5 with loss: 0.12946824729442596 and average loss:0.19377177953720093\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 10 with loss: 0.14081352949142456 and average loss:0.18546371161937714\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 15 with loss: 0.35297533869743347 and average loss:0.1973370462656021\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 20 with loss: 0.32478222250938416 and average loss:0.19744643568992615\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 25 with loss: 0.09648703038692474 and average loss:0.19540376961231232\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 30 with loss: 0.18711821734905243 and average loss:0.1883036345243454\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 35 with loss: 0.36324918270111084 and average loss:0.19097211956977844\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 40 with loss: 0.36136969923973083 and average loss:0.1983286589384079\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 45 with loss: 0.15959639847278595 and average loss:0.19333599507808685\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 50 with loss: 0.27927204966545105 and average loss:0.19616210460662842\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 55 with loss: 0.1586991250514984 and average loss:0.1981925368309021\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 60 with loss: 0.1430380493402481 and average loss:0.19723282754421234\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 65 with loss: 0.2857823073863983 and average loss:0.20495223999023438\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 70 with loss: 0.17872360348701477 and average loss:0.20381878316402435\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 75 with loss: 0.15357625484466553 and average loss:0.20433522760868073\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 80 with loss: 0.15259473025798798 and average loss:0.20200887322425842\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 85 with loss: 0.15709395706653595 and average loss:0.20110034942626953\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 90 with loss: 0.2754523456096649 and average loss:0.20095261931419373\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 95 with loss: 0.2461986243724823 and average loss:0.20033910870552063\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 100 with loss: 0.3898710012435913 and average loss:0.20217594504356384\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 105 with loss: 0.30916470289230347 and average loss:0.20447400212287903\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 110 with loss: 0.20968465507030487 and average loss:0.20868945121765137\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 115 with loss: 0.0992819145321846 and average loss:0.20856021344661713\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 120 with loss: 0.11944583803415298 and average loss:0.2064160406589508\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 125 with loss: 0.30988460779190063 and average loss:0.2064492106437683\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 130 with loss: 0.10538451373577118 and average loss:0.20684213936328888\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 135 with loss: 0.15677206218242645 and average loss:0.20631685853004456\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 140 with loss: 0.22954711318016052 and average loss:0.20611050724983215\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 145 with loss: 0.22148793935775757 and average loss:0.2072760909795761\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 150 with loss: 0.23941993713378906 and average loss:0.20772400498390198\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 155 with loss: 0.0673106461763382 and average loss:0.2090950310230255\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 160 with loss: 0.19452913105487823 and average loss:0.20887228846549988\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 165 with loss: 0.11806830018758774 and average loss:0.20901964604854584\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 170 with loss: 0.2550208866596222 and average loss:0.20978586375713348\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 175 with loss: 0.18319016695022583 and average loss:0.21006056666374207\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 180 with loss: 0.15220986306667328 and average loss:0.20942476391792297\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 185 with loss: 0.36711105704307556 and average loss:0.2092987596988678\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 190 with loss: 0.1464167982339859 and average loss:0.20916272699832916\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 195 with loss: 0.19873790442943573 and average loss:0.20825877785682678\n",
            "Epoch 34: LR=0.000050\n",
            "epoch: 33, batch: 200 with loss: 0.21790853142738342 and average loss:0.20774517953395844\n",
            "Epoch 34: LR=0.000050\n",
            "Validation loss: 0.32557330002971724\n",
            "epoch: 34, batch: 0 with loss: 0.15114203095436096 and average loss:0.15114203095436096\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 5 with loss: 0.2148243635892868 and average loss:0.21856236457824707\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 10 with loss: 0.17607799172401428 and average loss:0.19780564308166504\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 15 with loss: 0.1508028507232666 and average loss:0.20844294130802155\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 20 with loss: 0.0952388122677803 and average loss:0.22394435107707977\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 25 with loss: 0.320833683013916 and average loss:0.22807861864566803\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 30 with loss: 0.08043595403432846 and average loss:0.22049178183078766\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 35 with loss: 0.15038055181503296 and average loss:0.2103903889656067\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 40 with loss: 0.34731191396713257 and average loss:0.21274486184120178\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 45 with loss: 0.16550153493881226 and average loss:0.21679973602294922\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 50 with loss: 0.12429963052272797 and average loss:0.21637415885925293\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 55 with loss: 0.10807463526725769 and average loss:0.21035023033618927\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 60 with loss: 0.16923275589942932 and average loss:0.2087189257144928\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 65 with loss: 0.2070857584476471 and average loss:0.2112385332584381\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 70 with loss: 0.2675760090351105 and average loss:0.20977963507175446\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 75 with loss: 0.08829917758703232 and average loss:0.20527391135692596\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 80 with loss: 0.23583601415157318 and average loss:0.20456500351428986\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 85 with loss: 0.39035841822624207 and average loss:0.20824265480041504\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 90 with loss: 0.3562638461589813 and average loss:0.21074259281158447\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 95 with loss: 0.286122590303421 and average loss:0.21026913821697235\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 100 with loss: 0.3444930613040924 and average loss:0.2112751305103302\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 105 with loss: 0.3166397511959076 and average loss:0.21036143600940704\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 110 with loss: 0.311660498380661 and average loss:0.21097081899642944\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 115 with loss: 0.15280234813690186 and average loss:0.21405959129333496\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 120 with loss: 0.1451863944530487 and average loss:0.21573397517204285\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 125 with loss: 0.24740272760391235 and average loss:0.21541376411914825\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 130 with loss: 0.20252059400081635 and average loss:0.21519255638122559\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 135 with loss: 0.22378739714622498 and average loss:0.2150897979736328\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 140 with loss: 0.23817361891269684 and average loss:0.21433639526367188\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 145 with loss: 0.14048556983470917 and average loss:0.21273009479045868\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 150 with loss: 0.17691026628017426 and average loss:0.21243058145046234\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 155 with loss: 0.18723514676094055 and average loss:0.21387019753456116\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 160 with loss: 0.14366354048252106 and average loss:0.21274825930595398\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 165 with loss: 0.24577821791172028 and average loss:0.2136656641960144\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 170 with loss: 0.22411097586154938 and average loss:0.21416471898555756\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 175 with loss: 0.16878582537174225 and average loss:0.21352709829807281\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 180 with loss: 0.07948322594165802 and average loss:0.21471403539180756\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 185 with loss: 0.1658402532339096 and average loss:0.21426627039909363\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 190 with loss: 0.1635495126247406 and average loss:0.21273580193519592\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 195 with loss: 0.24104326963424683 and average loss:0.2127131223678589\n",
            "Epoch 35: LR=0.000050\n",
            "epoch: 34, batch: 200 with loss: 0.0024732465390115976 and average loss:0.2119978815317154\n",
            "Epoch 35: LR=0.000050\n",
            "Validation loss: 0.3594176763412999\n",
            "epoch: 35, batch: 0 with loss: 0.09601721167564392 and average loss:0.09601721167564392\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 5 with loss: 0.08046852052211761 and average loss:0.148023784160614\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 10 with loss: 0.22303515672683716 and average loss:0.18855829536914825\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 15 with loss: 0.11296426504850388 and average loss:0.18860262632369995\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 20 with loss: 0.17661185562610626 and average loss:0.1764095574617386\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 25 with loss: 0.12203776836395264 and average loss:0.17443780601024628\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 30 with loss: 0.10402389615774155 and average loss:0.1733112335205078\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 35 with loss: 0.1884688138961792 and average loss:0.17375141382217407\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 40 with loss: 0.21179668605327606 and average loss:0.1806575208902359\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 45 with loss: 0.15094362199306488 and average loss:0.18346351385116577\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 50 with loss: 0.13697953522205353 and average loss:0.18707451224327087\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 55 with loss: 0.1307697892189026 and average loss:0.18363767862319946\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 60 with loss: 0.07724935561418533 and average loss:0.18037500977516174\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 65 with loss: 0.2546808123588562 and average loss:0.1777946949005127\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 70 with loss: 0.10363934189081192 and average loss:0.17743809521198273\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 75 with loss: 0.3236970603466034 and average loss:0.1819026917219162\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 80 with loss: 0.24806414544582367 and average loss:0.18430112302303314\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 85 with loss: 0.2614628076553345 and average loss:0.18713600933551788\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 90 with loss: 0.4185333251953125 and average loss:0.18738137185573578\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 95 with loss: 0.18430106341838837 and average loss:0.18728631734848022\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 100 with loss: 0.20480503141880035 and average loss:0.18607977032661438\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 105 with loss: 0.1846611350774765 and average loss:0.18519330024719238\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 110 with loss: 0.3601767122745514 and average loss:0.1884855479001999\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 115 with loss: 0.20845453441143036 and average loss:0.18924976885318756\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 120 with loss: 0.25941669940948486 and average loss:0.19146956503391266\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 125 with loss: 0.36551931500434875 and average loss:0.19212540984153748\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 130 with loss: 0.21583373844623566 and average loss:0.19100576639175415\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 135 with loss: 0.2606014609336853 and average loss:0.19183464348316193\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 140 with loss: 0.2568496763706207 and average loss:0.19294410943984985\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 145 with loss: 0.27209019660949707 and average loss:0.19287703931331635\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 150 with loss: 0.25242483615875244 and average loss:0.19451384246349335\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 155 with loss: 0.2473098635673523 and average loss:0.195346862077713\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 160 with loss: 0.2150733917951584 and average loss:0.19477570056915283\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 165 with loss: 0.21635448932647705 and average loss:0.1946943998336792\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 170 with loss: 0.22010402381420135 and average loss:0.19500203430652618\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 175 with loss: 0.08984703570604324 and average loss:0.19477155804634094\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 180 with loss: 0.2011856883764267 and average loss:0.19450977444648743\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 185 with loss: 0.051311150193214417 and average loss:0.19300329685211182\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 190 with loss: 0.3258688449859619 and average loss:0.19339169561862946\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 195 with loss: 0.07046416401863098 and average loss:0.19403694570064545\n",
            "Epoch 36: LR=0.000025\n",
            "epoch: 35, batch: 200 with loss: 0.21294863522052765 and average loss:0.19439300894737244\n",
            "Epoch 36: LR=0.000025\n",
            "Validation loss: 0.3253553506790423\n",
            "epoch: 36, batch: 0 with loss: 0.2648089826107025 and average loss:0.2648089826107025\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 5 with loss: 0.25729620456695557 and average loss:0.2131672501564026\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 10 with loss: 0.1954394429922104 and average loss:0.1956201046705246\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 15 with loss: 0.1210855022072792 and average loss:0.1894041895866394\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 20 with loss: 0.15566787123680115 and average loss:0.19663970172405243\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 25 with loss: 0.1858440339565277 and average loss:0.18886421620845795\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 30 with loss: 0.20494940876960754 and average loss:0.19486980140209198\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 35 with loss: 0.218826025724411 and average loss:0.19345825910568237\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 40 with loss: 0.23686307668685913 and average loss:0.1920820027589798\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 45 with loss: 0.20668035745620728 and average loss:0.1902925670146942\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 50 with loss: 0.19111551344394684 and average loss:0.18907339870929718\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 55 with loss: 0.27264857292175293 and average loss:0.19045701622962952\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 60 with loss: 0.19429071247577667 and average loss:0.18730387091636658\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 65 with loss: 0.2154604196548462 and average loss:0.18686172366142273\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 70 with loss: 0.19965361058712006 and average loss:0.18910452723503113\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 75 with loss: 0.13594886660575867 and average loss:0.18658389151096344\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 80 with loss: 0.4453815221786499 and average loss:0.19208884239196777\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 85 with loss: 0.08849292248487473 and average loss:0.1888730525970459\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 90 with loss: 0.2217443436384201 and average loss:0.19150716066360474\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 95 with loss: 0.15887883305549622 and average loss:0.1931265890598297\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 100 with loss: 0.10197005420923233 and average loss:0.19467030465602875\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 105 with loss: 0.11523360759019852 and average loss:0.1910592019557953\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 110 with loss: 0.22710618376731873 and average loss:0.19000835716724396\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 115 with loss: 0.1591891199350357 and average loss:0.19007053971290588\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 120 with loss: 0.41159945726394653 and average loss:0.192013680934906\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 125 with loss: 0.18167440593242645 and average loss:0.19272901117801666\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 130 with loss: 0.08109544217586517 and average loss:0.1895703822374344\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 135 with loss: 0.13785116374492645 and average loss:0.18975402414798737\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 140 with loss: 0.08452092111110687 and average loss:0.18764708936214447\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 145 with loss: 0.31398552656173706 and average loss:0.18674935400485992\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 150 with loss: 0.2710130214691162 and average loss:0.18786656856536865\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 155 with loss: 0.08473095297813416 and average loss:0.18596087396144867\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 160 with loss: 0.08158312737941742 and average loss:0.18429096043109894\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 165 with loss: 0.17659136652946472 and average loss:0.18372298777103424\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 170 with loss: 0.11378484964370728 and average loss:0.1842268854379654\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 175 with loss: 0.10958512872457504 and average loss:0.1863255500793457\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 180 with loss: 0.08197658509016037 and average loss:0.18564671277999878\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 185 with loss: 0.22725296020507812 and average loss:0.18533895909786224\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 190 with loss: 0.1615176796913147 and average loss:0.1866377592086792\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 195 with loss: 0.3280669152736664 and average loss:0.18708591163158417\n",
            "Epoch 37: LR=0.000025\n",
            "epoch: 36, batch: 200 with loss: 0.031222596764564514 and average loss:0.18630975484848022\n",
            "Epoch 37: LR=0.000025\n",
            "Validation loss: 0.3732090130740521\n",
            "epoch: 37, batch: 0 with loss: 0.07987786084413528 and average loss:0.07987786084413528\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 5 with loss: 0.13586275279521942 and average loss:0.1277262568473816\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 10 with loss: 0.09645003825426102 and average loss:0.12341107428073883\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 15 with loss: 0.12876924872398376 and average loss:0.1340390145778656\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 20 with loss: 0.0921211764216423 and average loss:0.1418338119983673\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 25 with loss: 0.11335964500904083 and average loss:0.14084358513355255\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 30 with loss: 0.4656592011451721 and average loss:0.1452116221189499\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 35 with loss: 0.32447394728660583 and average loss:0.15917570888996124\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 40 with loss: 0.18831905722618103 and average loss:0.16313226521015167\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 45 with loss: 0.2018187791109085 and average loss:0.1620255410671234\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 50 with loss: 0.20638492703437805 and average loss:0.16335761547088623\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 55 with loss: 0.10482359677553177 and average loss:0.1635456085205078\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 60 with loss: 0.1307801455259323 and average loss:0.1716550886631012\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 65 with loss: 0.3096238076686859 and average loss:0.17251840233802795\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 70 with loss: 0.1552286595106125 and average loss:0.17687691748142242\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 75 with loss: 0.20549356937408447 and average loss:0.1800558716058731\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 80 with loss: 0.12712770700454712 and average loss:0.17846259474754333\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 85 with loss: 0.12736071646213531 and average loss:0.17928341031074524\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 90 with loss: 0.131386861205101 and average loss:0.1795782893896103\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 95 with loss: 0.16473719477653503 and average loss:0.17973151803016663\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 100 with loss: 0.1198197677731514 and average loss:0.18285080790519714\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 105 with loss: 0.3146458566188812 and average loss:0.1845666915178299\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 110 with loss: 0.17332911491394043 and average loss:0.18579989671707153\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 115 with loss: 0.22301700711250305 and average loss:0.1868697851896286\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 120 with loss: 0.07486065477132797 and average loss:0.18581828474998474\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 125 with loss: 0.3092195391654968 and average loss:0.18745869398117065\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 130 with loss: 0.0858321487903595 and average loss:0.18585436046123505\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 135 with loss: 0.19937720894813538 and average loss:0.18555310368537903\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 140 with loss: 0.08844299614429474 and average loss:0.1846655309200287\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 145 with loss: 0.1172989010810852 and average loss:0.18278448283672333\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 150 with loss: 0.34617599844932556 and average loss:0.18355655670166016\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 155 with loss: 0.11590242385864258 and average loss:0.18487834930419922\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 160 with loss: 0.11379162967205048 and average loss:0.18472985923290253\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 165 with loss: 0.17018237709999084 and average loss:0.1843637079000473\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 170 with loss: 0.12922018766403198 and average loss:0.18471361696720123\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 175 with loss: 0.293800413608551 and average loss:0.18419353663921356\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 180 with loss: 0.14117531478405 and average loss:0.18609115481376648\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 185 with loss: 0.2607024908065796 and average loss:0.1865876466035843\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 190 with loss: 0.10239559412002563 and average loss:0.18645846843719482\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 195 with loss: 0.36857348680496216 and average loss:0.1861855387687683\n",
            "Epoch 38: LR=0.000025\n",
            "epoch: 37, batch: 200 with loss: 0.15826131403446198 and average loss:0.18696781992912292\n",
            "Epoch 38: LR=0.000025\n",
            "Validation loss: 0.35456715115145143\n",
            "epoch: 38, batch: 0 with loss: 0.2319277822971344 and average loss:0.2319277822971344\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 5 with loss: 0.1704099178314209 and average loss:0.18038451671600342\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 10 with loss: 0.2853545844554901 and average loss:0.19807256758213043\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 15 with loss: 0.32753631472587585 and average loss:0.20068395137786865\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 20 with loss: 0.2347537726163864 and average loss:0.19588874280452728\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 25 with loss: 0.2679758667945862 and average loss:0.19347520172595978\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 30 with loss: 0.23135751485824585 and average loss:0.19431999325752258\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 35 with loss: 0.07355351746082306 and average loss:0.1915060132741928\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 40 with loss: 0.3459916412830353 and average loss:0.19427786767482758\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 45 with loss: 0.1562415361404419 and average loss:0.19400165975093842\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 50 with loss: 0.1670711636543274 and average loss:0.18729066848754883\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 55 with loss: 0.09529811888933182 and average loss:0.18542835116386414\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 60 with loss: 0.09549620747566223 and average loss:0.1831332892179489\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 65 with loss: 0.3326379954814911 and average loss:0.18700844049453735\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 70 with loss: 0.20839712023735046 and average loss:0.18813511729240417\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 75 with loss: 0.15736809372901917 and average loss:0.19008366763591766\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 80 with loss: 0.19101126492023468 and average loss:0.1902654618024826\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 85 with loss: 0.2011488676071167 and average loss:0.1907881796360016\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 90 with loss: 0.12107396125793457 and average loss:0.19330133497714996\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 95 with loss: 0.3487955927848816 and average loss:0.1942686140537262\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 100 with loss: 0.23238807916641235 and average loss:0.19317474961280823\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 105 with loss: 0.1412242352962494 and average loss:0.19084516167640686\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 110 with loss: 0.07557665556669235 and average loss:0.1901259422302246\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 115 with loss: 0.18859878182411194 and average loss:0.19376802444458008\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 120 with loss: 0.15940134227275848 and average loss:0.19217436015605927\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 125 with loss: 0.2049904763698578 and average loss:0.19050946831703186\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 130 with loss: 0.15059515833854675 and average loss:0.18926982581615448\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 135 with loss: 0.24178750813007355 and average loss:0.1880490481853485\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 140 with loss: 0.05942152068018913 and average loss:0.1871306300163269\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 145 with loss: 0.10074617713689804 and average loss:0.1848338097333908\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 150 with loss: 0.24881906807422638 and average loss:0.1847047507762909\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 155 with loss: 0.18348918855190277 and average loss:0.18513761460781097\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 160 with loss: 0.15006525814533234 and average loss:0.18499554693698883\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 165 with loss: 0.19445544481277466 and average loss:0.18445485830307007\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 170 with loss: 0.17894382774829865 and average loss:0.18448682129383087\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 175 with loss: 0.08245737850666046 and average loss:0.18398727476596832\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 180 with loss: 0.14767521619796753 and average loss:0.18485228717327118\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 185 with loss: 0.1821703016757965 and average loss:0.1850624829530716\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 190 with loss: 0.15509125590324402 and average loss:0.18527936935424805\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 195 with loss: 0.15880967676639557 and average loss:0.185724139213562\n",
            "Epoch 39: LR=0.000025\n",
            "epoch: 38, batch: 200 with loss: 0.01591050624847412 and average loss:0.18537044525146484\n",
            "Epoch 39: LR=0.000025\n",
            "Validation loss: 0.3227711037970057\n",
            "epoch: 39, batch: 0 with loss: 0.18412144482135773 and average loss:0.18412144482135773\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 5 with loss: 0.18678566813468933 and average loss:0.19407783448696136\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 10 with loss: 0.06830762326717377 and average loss:0.15182650089263916\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 15 with loss: 0.13823994994163513 and average loss:0.15684759616851807\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 20 with loss: 0.1074354276061058 and average loss:0.15646152198314667\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 25 with loss: 0.09472385048866272 and average loss:0.16523602604866028\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 30 with loss: 0.15552547574043274 and average loss:0.15998177230358124\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 35 with loss: 0.2568856477737427 and average loss:0.1606181114912033\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 40 with loss: 0.24847106635570526 and average loss:0.1595100611448288\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 45 with loss: 0.1504271924495697 and average loss:0.1603941172361374\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 50 with loss: 0.21676161885261536 and average loss:0.1679133176803589\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 55 with loss: 0.16529634594917297 and average loss:0.17085425555706024\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 60 with loss: 0.2893531024456024 and average loss:0.1704014092683792\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 65 with loss: 0.14633534848690033 and average loss:0.16799508035182953\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 70 with loss: 0.16058358550071716 and average loss:0.16808131337165833\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 75 with loss: 0.12499222904443741 and average loss:0.16817380487918854\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 80 with loss: 0.15152201056480408 and average loss:0.16902703046798706\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 85 with loss: 0.12639844417572021 and average loss:0.1666252464056015\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 90 with loss: 0.4098222851753235 and average loss:0.171225905418396\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 95 with loss: 0.1792157143354416 and average loss:0.17037886381149292\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 100 with loss: 0.3476417064666748 and average loss:0.17298150062561035\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 105 with loss: 0.19773219525814056 and average loss:0.1719857156276703\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 110 with loss: 0.26429763436317444 and average loss:0.17216241359710693\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 115 with loss: 0.08703788369894028 and average loss:0.170250803232193\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 120 with loss: 0.29859381914138794 and average loss:0.17384567856788635\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 125 with loss: 0.2088802456855774 and average loss:0.1730714738368988\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 130 with loss: 0.054412148892879486 and average loss:0.1719609797000885\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 135 with loss: 0.11760026961565018 and average loss:0.17486393451690674\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 140 with loss: 0.19930033385753632 and average loss:0.17694838345050812\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 145 with loss: 0.21894653141498566 and average loss:0.177584707736969\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 150 with loss: 0.21479153633117676 and average loss:0.1789136379957199\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 155 with loss: 0.19187627732753754 and average loss:0.1789693683385849\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 160 with loss: 0.18148012459278107 and average loss:0.17790798842906952\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 165 with loss: 0.1817498654127121 and average loss:0.1767750382423401\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 170 with loss: 0.048617638647556305 and average loss:0.1763014942407608\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 175 with loss: 0.12750685214996338 and average loss:0.17730019986629486\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 180 with loss: 0.13694532215595245 and average loss:0.17649658024311066\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 185 with loss: 0.24779413640499115 and average loss:0.1763293594121933\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 190 with loss: 0.18211376667022705 and average loss:0.1779286414384842\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 195 with loss: 0.17628657817840576 and average loss:0.17896011471748352\n",
            "Epoch 40: LR=0.000013\n",
            "epoch: 39, batch: 200 with loss: 0.035958316177129745 and average loss:0.17906372249126434\n",
            "Epoch 40: LR=0.000013\n",
            "Validation loss: 0.3364498293867298\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "  total_loss = 0\n",
        "  model.train()\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "        total_loss += loss\n",
        "        average_loss = total_loss / (batch_idx + 1)\n",
        "        if batch_idx % 5 == 0:\n",
        "          print(f'epoch: {epoch}, batch: {batch_idx} with loss: {loss} and average loss:{average_loss}')\n",
        "          print(f\"Epoch {epoch+1}: LR={optimizer.param_groups[0]['lr']:.6f}\")\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "  val_loss = validate(model, val_loader, criterion)\n",
        "  print(f'Validation loss: {val_loss}')\n",
        "  scheduler.step(val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReH9xISzPy-C",
        "outputId": "0171f30c-0eed-44a1-9940-0b84e149d22c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 6064 / 6404 with accuracy 94.69\n",
            "Got 1779 / 2023 with accuracy 87.94\n"
          ]
        }
      ],
      "source": [
        "def check_accuracy(loader,model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()   #evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)   #max(1) finds the highest value and predictions to store the indices of the value (like from 0-9 then just need the highest value to find the number, the value itself is not needed)\n",
        "            num_correct += (predictions==y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
        "        model.train()\n",
        "\n",
        "check_accuracy(train_loader, model)\n",
        "check_accuracy(test_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = 'dog_cat.pt'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "GbtGgqMzYrbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = CNN()\n",
        "new_model.load_state_dict(torch.load(PATH))\n",
        "new_model.to(device)\n",
        "new_model.eval()\n",
        "check_accuracy(test_loader, new_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrVEDiFGjk5w",
        "outputId": "c027477c-911d-4756-85b5-4146db92f292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 1773 / 2023 with accuracy 87.64\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}